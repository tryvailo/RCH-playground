#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–∏–Ω–≥ test2.md —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ—Ä—Å–∏–∏ 3.1 (NON-CQC fields only)
"""

import os
import sys
import json
import openai
from typing import Dict

# Try to load .env file if python-dotenv is available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-2024-08-06')

def load_prompt_and_schema():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ JSON Schema –≤–µ—Ä—Å–∏–∏ 3.1"""
    prompt_file = "input/autumna/AUTUMNA_PARSING_PROMPT_v3_1_OPTIMIZED_NON_CQC.md"
    schema_file = "input/autumna/response_format_v3_1_optimized_non_cqc.json"
    
    if not os.path.exists(prompt_file):
        raise FileNotFoundError(f"–ü—Ä–æ–º–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompt_file}")
    if not os.path.exists(schema_file):
        raise FileNotFoundError(f"JSON Schema –Ω–µ –Ω–∞–π–¥–µ–Ω: {schema_file}")
    
    with open(prompt_file, 'r', encoding='utf-8') as f:
        system_prompt = f.read()
    
    with open(schema_file, 'r', encoding='utf-8') as f:
        response_format = json.load(f)
    
    return system_prompt, response_format

def parse_markdown_file(markdown_file: str):
    """–ü–∞—Ä—Å–∏–Ω–≥ Markdown —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ OpenAI API"""
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å API –∫–ª—é—á
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export OPENAI_API_KEY=sk-...")
    
    client = openai.OpenAI(api_key=api_key)
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ schema
    print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –∏ JSON Schema v3.1...")
    system_prompt, response_format = load_prompt_and_schema()
    
    # –ü—Ä–æ—á–∏—Ç–∞—Ç—å Markdown —Ñ–∞–π–ª
    print(f"üìÑ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {markdown_file}")
    with open(markdown_file, 'r', encoding='utf-8') as f:
        markdown_content = f.read()
    
    original_size = len(markdown_content)
    print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {original_size} —Å–∏–º–≤–æ–ª–æ–≤ (~{original_size // 4} —Ç–æ–∫–µ–Ω–æ–≤)")
    
    # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ OpenAI
    print("\nü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ OpenAI API...")
    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Parse this markdown page:\n\n{markdown_content}"}
            ],
            response_format={"type": "json_schema", "json_schema": response_format['json_schema']},
            temperature=0
        )
        
        parsed_data = json.loads(response.choices[0].message.content)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        usage = response.usage
        print(f"\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤:")
        print(f"   –ü—Ä–æ–º–ø—Ç: {usage.prompt_tokens}")
        print(f"   –û—Ç–≤–µ—Ç: {usage.completion_tokens}")
        print(f"   –í—Å–µ–≥–æ: {usage.total_tokens}")
        
        return {
            'success': True,
            'data': parsed_data,
            'usage': {
                'prompt_tokens': usage.prompt_tokens,
                'completion_tokens': usage.completion_tokens,
                'total_tokens': usage.total_tokens
            },
            'raw_response': response.model_dump_json()
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI API: {e}")
        import traceback
        traceback.print_exc()
        return {
            'success': False,
            'error': str(e),
            'data': None
        }

def print_summary(parsed_data: Dict):
    """–í—ã–≤–µ—Å—Ç–∏ –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    print("\n" + "="*80)
    print("üìã –ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*80)
    
    # Identity
    identity = parsed_data.get('identity', {})
    print(f"\nüè† –ò–î–ï–ù–¢–ò–§–ò–ö–ê–¶–ò–Ø:")
    print(f"   –ù–∞–∑–≤–∞–Ω–∏–µ: {identity.get('name', 'N/A')}")
    print(f"   CQC Location ID: {identity.get('cqc_location_id', 'N/A')}")
    print(f"   Provider: {identity.get('provider_name', 'N/A')}")
    print(f"   Brand: {identity.get('brand_name', 'N/A')}")
    
    # Location
    location = parsed_data.get('location', {})
    print(f"\nüìç –õ–û–ö–ê–¶–ò–Ø:")
    print(f"   –ì–æ—Ä–æ–¥: {location.get('city', 'N/A')}")
    print(f"   –ü–æ—á—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å: {location.get('postcode', 'N/A')}")
    print(f"   –†–µ–≥–∏–æ–Ω: {location.get('region', 'N/A')}")
    print(f"   Local Authority: {location.get('local_authority', 'N/A')}")
    
    # Pricing
    pricing = parsed_data.get('pricing', {})
    print(f"\nüí∞ –¶–ï–ù–û–û–ë–†–ê–ó–û–í–ê–ù–ò–ï:")
    if pricing.get('fee_residential_from'):
        print(f"   Residential: ¬£{pricing.get('fee_residential_from')} - ¬£{pricing.get('fee_residential_to', 'N/A')}")
    if pricing.get('fee_nursing_from'):
        print(f"   Nursing: ¬£{pricing.get('fee_nursing_from')} - ¬£{pricing.get('fee_nursing_to', 'N/A')}")
    if pricing.get('fee_dementia_from'):
        print(f"   Dementia: ¬£{pricing.get('fee_dementia_from')} - ¬£{pricing.get('fee_dementia_to', 'N/A')}")
    
    # Funding
    funding = parsed_data.get('funding', {})
    print(f"\nüí≥ –§–ò–ù–ê–ù–°–ò–†–û–í–ê–ù–ò–ï:")
    print(f"   Self-funding: {funding.get('accepts_self_funding', 'N/A')}")
    print(f"   Local Authority: {funding.get('accepts_local_authority', 'N/A')}")
    print(f"   NHS CHC: {funding.get('accepts_nhs_chc', 'N/A')}")
    
    # Availability
    capacity = parsed_data.get('capacity', {})
    print(f"\nüõèÔ∏è  –î–û–°–¢–£–ü–ù–û–°–¢–¨:")
    print(f"   –í—Å–µ–≥–æ –∫—Ä–æ–≤–∞—Ç–µ–π: {capacity.get('beds_total', 'N/A')}")
    print(f"   –î–æ—Å—Ç—É–ø–Ω–æ: {capacity.get('beds_available', 'N/A')}")
    print(f"   –ï—Å—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å: {capacity.get('has_availability', 'N/A')}")
    print(f"   –°—Ç–∞—Ç—É—Å: {capacity.get('availability_status', 'N/A')}")
    
    # Medical Specialisms
    medical = parsed_data.get('medical_specialisms', {})
    conditions = medical.get('conditions_list', [])
    print(f"\nüè• –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ï –°–ü–ï–¶–ò–ê–õ–ò–ó–ê–¶–ò–ò:")
    print(f"   –í—Å–µ–≥–æ —É—Å–ª–æ–≤–∏–π: {len(conditions)}")
    if conditions:
        print(f"   –ü—Ä–∏–º–µ—Ä—ã: {', '.join(conditions[:5])}")
        if len(conditions) > 5:
            print(f"   ... –∏ –µ—â–µ {len(conditions) - 5}")
    
    # Dietary Options
    dietary = parsed_data.get('dietary_options', {})
    special_diets = dietary.get('special_diets', {})
    print(f"\nüçΩÔ∏è  –î–ò–ï–¢–ò–ß–ï–°–ö–ò–ï –û–ü–¶–ò–ò:")
    available_diets = [k for k, v in special_diets.items() if v and k != 'other']
    print(f"   –î–æ—Å—Ç—É–ø–Ω–æ –¥–∏–µ—Ç: {len(available_diets)}")
    if available_diets:
        print(f"   –ü—Ä–∏–º–µ—Ä—ã: {', '.join(available_diets[:5])}")
    
    # Extraction Metadata
    extraction_meta = parsed_data.get('extraction_metadata', {})
    print(f"\nüìä –ú–ï–¢–ê–î–ê–ù–ù–´–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø:")
    print(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {extraction_meta.get('extraction_confidence', 'N/A')}")
    print(f"   –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {extraction_meta.get('data_quality_score', 'N/A')}/100")
    print(f"   –î–æ–º –∑–∞–∫—Ä—ã—Ç: {extraction_meta.get('is_dormant', False)}")
    
    # Schema version
    source_meta = parsed_data.get('source_metadata', {})
    print(f"\nüìå –í–ï–†–°–ò–Ø –°–•–ï–ú–´:")
    print(f"   Schema version: {source_meta.get('schema_version', 'N/A')}")
    print(f"   Source: {source_meta.get('source', 'N/A')}")

def main():
    markdown_file = "input/autumna/Data-MD/html 2/test2.md"
    
    if not os.path.exists(markdown_file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {markdown_file}")
        sys.exit(1)
    
    # –ü–∞—Ä—Å–∏–Ω–≥
    print("="*80)
    print("üß™ –ü–ê–†–°–ò–ù–ì TEST2.MD –° –í–ï–†–°–ò–ï–ô 3.1 (NON-CQC FIELDS ONLY)")
    print("="*80)
    
    result = parse_markdown_file(markdown_file)
    
    if not result['success']:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.get('error')}")
        sys.exit(1)
    
    parsed_data = result['data']
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_file = "input/autumna/Data-MD/html 2/test2-v31-parsed-result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(parsed_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
    
    # –í—ã–≤–µ—Å—Ç–∏ —Å–≤–æ–¥–∫—É
    print_summary(parsed_data)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*80)
    print("üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*80)
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –î–∞")
    print(f"   üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {result['usage']['total_tokens']}")
    print(f"      - –ü—Ä–æ–º–ø—Ç: {result['usage']['prompt_tokens']}")
    print(f"      - –û—Ç–≤–µ—Ç: {result['usage']['completion_tokens']}")
    print(f"   üí∞ –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${result['usage']['total_tokens'] * 0.00001:.4f}")

if __name__ == '__main__':
    main()

