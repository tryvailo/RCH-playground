#!/usr/bin/env python3
"""
–§–ê–ó–ê 2: –ü–∞—Ä—Å–∏–Ω–≥ Markdown —á–µ—Ä–µ–∑ OpenAI LLM
================================================================
–¶–µ–ª—å: –ò–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Markdown —Å –ø–æ–º–æ—â—å—é ChatGPT

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python phase2_parse_llm.py --prompt-version v2.4 --batch-size 25

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - OpenAI API –∫–ª—é—á –≤ .env
    - PostgreSQL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
    - Markdown —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≤ staging —Ç–∞–±–ª–∏—Ü—É (–§–∞–∑–∞ 1)
"""

import os
import sys
import json
import time
import argparse
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv
from openai import OpenAI
from typing import Dict, Optional, List
import logging

load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENAI_MODEL = "gpt-4o-2024-08-06"
DEFAULT_BATCH_SIZE = 25
MAX_RETRIES = 3
RETRY_DELAY = 5


def get_db_connection():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
    return psycopg2.connect(
        host=os.getenv('DB_HOST', 'localhost'),
        port=os.getenv('DB_PORT', '5432'),
        database=os.getenv('DB_NAME', 'care_homes_db'),
        user=os.getenv('DB_USER', 'postgres'),
        password=os.getenv('DB_PASSWORD', '')
    )


def load_prompt_and_schema(prompt_version: str) -> tuple:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ JSON Schema –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏"""
    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–µ—Ä—Å–∏–π –ø—Ä–æ–º–ø—Ç–æ–≤
    if prompt_version == 'v2.6' or prompt_version == 'v2.6-final':
        # v2.6 FINAL - –∏—Å–ø–æ–ª—å–∑—É–µ–º Markdown –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        prompt_file = f"input/autumna/autumna_markdown_prompt_v26.md"
        schema_file = f"input/autumna/response_format_v26_final.json"
    elif prompt_version == 'v2.6-html':
        # v2.6 HTML –≤–µ—Ä—Å–∏—è
        prompt_file = f"input/autumna/autumna_html_prompt_v26.md"
        schema_file = f"input/autumna/response_format_v26_final.json"
    elif prompt_version == 'v2.5' or prompt_version == 'v2.5-optimized':
        prompt_file = f"input/autumna/AUTUMNA_PARSING_PROMPT_v2_5_OPTIMIZED.md"
        schema_file = f"input/autumna/response_format_v2_4.json"
    else:
        prompt_file = f"input/autumna/AUTUMNA_PARSING_PROMPT_v2_4.md"
        schema_file = f"input/autumna/response_format_v2_4.json"
    
    if not os.path.exists(prompt_file):
        raise FileNotFoundError(f"–ü—Ä–æ–º–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompt_file}")
    if not os.path.exists(schema_file):
        raise FileNotFoundError(f"JSON Schema –Ω–µ –Ω–∞–π–¥–µ–Ω: {schema_file}")
    
    with open(prompt_file, 'r', encoding='utf-8') as f:
        system_prompt = f.read()
    
    with open(schema_file, 'r', encoding='utf-8') as f:
        response_format = json.load(f)
    
    return system_prompt, response_format


def parse_markdown_with_openai(client: OpenAI, markdown_content: str, system_prompt: str, response_format: dict) -> Dict:
    """–ü–∞—Ä—Å–∏–Ω–≥ Markdown —á–µ—Ä–µ–∑ OpenAI API"""
    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Parse this markdown page:\n\n{markdown_content}"}
            ],
            response_format={"type": "json_schema", "json_schema": response_format['json_schema']},
            temperature=0
        )
        
        parsed_data = json.loads(response.choices[0].message.content)
        return {
            'success': True,
            'data': parsed_data,
            'raw_response': response.model_dump_json()
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ OpenAI API: {e}")
        return {
            'success': False,
            'error': str(e),
            'data': None
        }


def extract_metadata(parsed_data: Dict) -> Dict:
    """–ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ parsed JSON"""
    extraction_meta = parsed_data.get('extraction_metadata', {})
    
    return {
        'extraction_confidence': extraction_meta.get('extraction_confidence', 'medium'),
        'data_quality_score': extraction_meta.get('data_quality_score'),
        'is_dormant': extraction_meta.get('is_dormant', False),
        'critical_fields_missing': extraction_meta.get('critical_fields_missing', []),
        'data_quality_notes': extraction_meta.get('data_quality_notes')
    }


def save_parsing_result(conn, staging_id: int, parsed_data: Dict, metadata: Dict, prompt_version: str):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ staging —Ç–∞–±–ª–∏—Ü—É"""
    cursor = conn.cursor()
    
    try:
        cursor.execute("""
            UPDATE autumna_staging
            SET 
                parsed_json = %(parsed_json)s::jsonb,
                extraction_confidence = %(confidence)s,
                data_quality_score = %(quality_score)s,
                is_dormant = %(is_dormant)s,
                llm_model = %(model)s,
                llm_prompt_version = %(prompt_version)s,
                parsing_errors = %(errors)s::jsonb,
                needs_reparse = FALSE,
                updated_at = CURRENT_TIMESTAMP
            WHERE id = %(staging_id)s
        """, {
            'staging_id': staging_id,
            'parsed_json': json.dumps(parsed_data),
            'confidence': metadata['extraction_confidence'],
            'quality_score': metadata['data_quality_score'],
            'is_dormant': metadata['is_dormant'],
            'model': OPENAI_MODEL,
            'prompt_version': prompt_version,
            'errors': json.dumps({
                'critical_fields_missing': metadata['critical_fields_missing'],
                'data_quality_notes': metadata['data_quality_notes']
            })
        })
        
        conn.commit()
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è ID {staging_id}: {e}")
        conn.rollback()
        return False
    finally:
        cursor.close()


def get_unparsed_records(conn, batch_size: int, needs_reparse: bool = False) -> List[Dict]:
    """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–ø–∏—Å–∏ –±–µ–∑ parsed_json"""
    cursor = conn.cursor(cursor_factory=RealDictCursor)
    
    if needs_reparse:
        cursor.execute("""
            SELECT id, markdown_content, source_url, cqc_location_id
            FROM autumna_staging
            WHERE markdown_content IS NOT NULL
              AND needs_reparse = TRUE
            ORDER BY created_at ASC
            LIMIT %(batch_size)s
        """, {'batch_size': batch_size})
    else:
        cursor.execute("""
            SELECT id, markdown_content, source_url, cqc_location_id
            FROM autumna_staging
            WHERE markdown_content IS NOT NULL
              AND parsed_json IS NULL
            ORDER BY created_at ASC
            LIMIT %(batch_size)s
        """, {'batch_size': batch_size})
    
    records = cursor.fetchall()
    cursor.close()
    return [dict(record) for record in records]


def process_batch(conn, client: OpenAI, records: List[Dict], system_prompt: str, response_format: dict, prompt_version: str):
    """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á –∑–∞–ø–∏—Å–µ–π"""
    success_count = 0
    error_count = 0
    
    for record in records:
        staging_id = record['id']
        url = record['source_url']
        markdown_content = record['markdown_content']
        
        logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥: {url} (ID: {staging_id})")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ OpenAI
        result = parse_markdown_with_openai(client, markdown_content, system_prompt, response_format)
        
        if not result['success']:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {url}: {result.get('error')}")
            error_count += 1
            continue
        
        parsed_data = result['data']
        
        # –ò–∑–≤–ª–µ—á—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = extract_metadata(parsed_data)
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if save_parsing_result(conn, staging_id, parsed_data, metadata, prompt_version):
            logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {url} (quality: {metadata['data_quality_score']})")
            success_count += 1
        else:
            error_count += 1
        
        # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (rate limiting)
        time.sleep(0.5)
    
    return success_count, error_count


def main():
    parser = argparse.ArgumentParser(description='–§–∞–∑–∞ 2: –ü–∞—Ä—Å–∏–Ω–≥ Markdown —á–µ—Ä–µ–∑ OpenAI LLM')
    parser.add_argument('--prompt-version', default='v2.6', help='–í–µ—Ä—Å–∏—è –ø—Ä–æ–º–ø—Ç–∞ (v2.4, v2.5, v2.5-optimized, v2.6, v2.6-final, v2.6-html)')
    parser.add_argument('--batch-size', type=int, default=DEFAULT_BATCH_SIZE, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--reparse', action='store_true', help='–ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø–∏—Å–∏ —Å needs_reparse=TRUE')
    parser.add_argument('--dry-run', action='store_true', help='–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å OpenAI API –∫–ª—é—á
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.error("‚ùå OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        sys.exit(1)
    
    client = OpenAI(api_key=api_key)
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ schema
    logger.info(f"üìö –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –∏ JSON Schema (–≤–µ—Ä—Å–∏—è: {args.prompt_version})...")
    try:
        system_prompt, response_format = load_prompt_and_schema(args.prompt_version)
        logger.info("   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        sys.exit(1)
    
    # –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î
    logger.info("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î...")
    conn = get_db_connection()
    logger.info("   ‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ")
    
    total_success = 0
    total_errors = 0
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
    while True:
        # –ü–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –±–∞—Ç—á
        records = get_unparsed_records(conn, args.batch_size, needs_reparse=args.reparse)
        
        if not records:
            logger.info("‚úÖ –í—Å–µ –∑–∞–ø–∏—Å–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã!")
            break
        
        logger.info(f"\nüì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –∏–∑ {len(records)} –∑–∞–ø–∏—Å–µ–π...")
        
        if args.dry_run:
            logger.info("üß™ DRY RUN - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
            for record in records:
                logger.info(f"  - {record['source_url']}")
            break
        
        # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –±–∞—Ç—á
        success, errors = process_batch(
            conn, client, records, system_prompt, response_format, args.prompt_version
        )
        
        total_success += success
        total_errors += errors
        
        logger.info(f"üìä –ë–∞—Ç—á –∑–∞–≤–µ—Ä—à–µ–Ω: ‚úÖ {success}, ‚ùå {errors}")
        
        # –ï—Å–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –Ω–µ –≤—Å–µ –∑–∞–ø–∏—Å–∏, –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å
        if len(records) < args.batch_size:
            break
    
    conn.close()
    
    logger.info(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –í—Å–µ–≥–æ: ‚úÖ {total_success}, ‚ùå {total_errors}")


if __name__ == '__main__':
    main()

