#!/usr/bin/env python3
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–¥–Ω–æ–≥–æ Markdown —Ñ–∞–π–ª–∞
================================================================
–¶–µ–ª—å: –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ test1-md.md (—Ç—Ä–µ–±—É–µ—Ç OPENAI_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è)
"""

import os
import sys
import json
import openai
from typing import Dict

# Try to load .env file if python-dotenv is available
try:
    from dotenv import load_dotenv
    # Load environment variables from .env file
    load_dotenv()
except ImportError:
    # python-dotenv not installed, will use environment variables directly
    pass

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-2024-08-06')

def clean_markdown(markdown_content: str):
    """
    –£–¥–∞–ª–∏—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –∏–∑ Markdown –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
    
    –£–¥–∞–ª—è–µ—Ç:
    - Cookies —Å–µ–∫—Ü–∏—é
    - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥—Ä—É–≥–∏—Ö –¥–æ–º–æ–≤
    - –§—É—Ç–µ—Ä Autumna
    
    Returns:
        (cleaned_content, removed_chars_count)
    """
    lines = markdown_content.split('\n')
    cleaned_lines = []
    removed_chars = 0
    
    skip_sections = [
        'Cookies on the Autumna Website',
        'Other Care Homes in the area',
        'The UK\'s largest & most detailed directory',
        'BESbswy'  # –ö–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    ]
    
    skip = False
    skip_start_line = None
    
    for i, line in enumerate(lines):
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—á–∞–ª–æ —Å–µ–∫—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞
        should_skip = False
        for section in skip_sections:
            if section in line:
                should_skip = True
                skip_start_line = i
                break
        
        if should_skip:
            skip = True
            continue
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω–µ—Ü —Å–µ–∫—Ü–∏–∏ –ø—Ä–æ–ø—É—Å–∫–∞ (–Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É—Ä–æ–≤–Ω—è 1-2 –∏–ª–∏ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞)
        if skip:
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —É—Ä–æ–≤–Ω—è 1-2 (–Ω–µ 3+), –ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—å –ø—Ä–æ–ø—É—Å–∫
            if line.startswith('#') and not line.startswith('###'):
                skip = False
            # –ï—Å–ª–∏ —ç—Ç–æ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞
            elif i == len(lines) - 1:
                skip = False
        
        if not skip:
            cleaned_lines.append(line)
        else:
            removed_chars += len(line) + 1  # +1 –¥–ª—è —Å–∏–º–≤–æ–ª–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏
    
    cleaned_content = '\n'.join(cleaned_lines)
    return cleaned_content, removed_chars

def load_prompt_and_schema():
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ JSON Schema"""
    prompt_file = "input/autumna/autumna_markdown_prompt_v26.md"
    schema_file = "input/autumna/response_format_v26_final.json"
    
    if not os.path.exists(prompt_file):
        raise FileNotFoundError(f"–ü—Ä–æ–º–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompt_file}")
    if not os.path.exists(schema_file):
        raise FileNotFoundError(f"JSON Schema –Ω–µ –Ω–∞–π–¥–µ–Ω: {schema_file}")
    
    with open(prompt_file, 'r', encoding='utf-8') as f:
        system_prompt = f.read()
    
    with open(schema_file, 'r', encoding='utf-8') as f:
        response_format = json.load(f)
    
    return system_prompt, response_format

def parse_markdown_file(markdown_file: str):
    """–ü–∞—Ä—Å–∏–Ω–≥ Markdown —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ OpenAI API"""
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å API –∫–ª—é—á
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: export OPENAI_API_KEY=sk-...")
    
    client = openai.OpenAI(api_key=api_key)
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏ schema
    print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –∏ JSON Schema...")
    system_prompt, response_format = load_prompt_and_schema()
    
    # –ü—Ä–æ—á–∏—Ç–∞—Ç—å Markdown —Ñ–∞–π–ª
    print(f"üìÑ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {markdown_file}")
    with open(markdown_file, 'r', encoding='utf-8') as f:
        markdown_content = f.read()
    
    original_size = len(markdown_content)
    print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {original_size} —Å–∏–º–≤–æ–ª–æ–≤ (~{original_size // 4} —Ç–æ–∫–µ–Ω–æ–≤)")
    
    # –û—á–∏—Å—Ç–∏—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏
    markdown_content, removed_chars = clean_markdown(markdown_content)
    cleaned_size = len(markdown_content)
    savings_percent = (removed_chars / original_size * 100) if original_size > 0 else 0
    
    print(f"   –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {cleaned_size} —Å–∏–º–≤–æ–ª–æ–≤ (~{cleaned_size // 4} —Ç–æ–∫–µ–Ω–æ–≤)")
    print(f"   –£–¥–∞–ª–µ–Ω–æ: {removed_chars} —Å–∏–º–≤–æ–ª–æ–≤ ({savings_percent:.1f}%)")
    
    # –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ OpenAI
    print("\nü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ OpenAI API...")
    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Parse this markdown page:\n\n{markdown_content}"}
            ],
            response_format={"type": "json_schema", "json_schema": response_format['json_schema']},
            temperature=0
        )
        
        parsed_data = json.loads(response.choices[0].message.content)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        usage = response.usage
        print(f"\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤:")
        print(f"   –ü—Ä–æ–º–ø—Ç: {usage.prompt_tokens}")
        print(f"   –û—Ç–≤–µ—Ç: {usage.completion_tokens}")
        print(f"   –í—Å–µ–≥–æ: {usage.total_tokens}")
        
        return {
            'success': True,
            'data': parsed_data,
            'usage': {
                'prompt_tokens': usage.prompt_tokens,
                'completion_tokens': usage.completion_tokens,
                'total_tokens': usage.total_tokens
            },
            'raw_response': response.model_dump_json()
        }
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ OpenAI API: {e}")
        return {
            'success': False,
            'error': str(e),
            'data': None
        }

def extract_expected_data():
    """–ò–∑–≤–ª–µ—á—å –æ–∂–∏–¥–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Markdown —Ñ–∞–π–ª–∞ (—Ä—É—á–Ω–æ–π –∞–Ω–∞–ª–∏–∑)"""
    return {
        'name': 'Ladydale Care Home',
        'cqc_location_id': '1-145996910',  # –ò–∑ URL –≤ —Å—Ç—Ä–æ–∫–µ 78: https://www.cqc.org.uk/location/1-145996910/reports
        'city': 'Leek',
        'postcode': 'ST13 5LF',
        'address': '9 Fynney Street, Leek, Staffordshire',
        'care_residential': True,
        'care_respite': True,
        'care_dementia': False,
        'care_nursing': False,
        'provider_name': 'Pearlcare',
        'local_authority': 'Staffordshire',
        'cqc_rating_overall': 'Good',
        'beds_total': 54,
        'year_registered': 2011  # "26th January 2011"
    }

def compare_results(parsed_data: Dict, expected_data: Dict):
    """–°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –æ–∂–∏–¥–∞–µ–º—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    print("\n" + "="*80)
    print("üîç –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*80)
    
    errors = []
    warnings = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    identity = parsed_data.get('identity', {})
    location = parsed_data.get('location', {})
    
    # 1. name
    parsed_name = identity.get('name')
    expected_name = expected_data.get('name')
    if parsed_name != expected_name:
        errors.append(f"name: –æ–∂–∏–¥–∞–ª–æ—Å—å '{expected_name}', –ø–æ–ª—É—á–µ–Ω–æ '{parsed_name}'")
    else:
        print(f"‚úÖ name: {parsed_name}")
    
    # 2. cqc_location_id
    parsed_cqc_id = identity.get('cqc_location_id')
    expected_cqc_id = expected_data.get('cqc_location_id')
    if parsed_cqc_id != expected_cqc_id:
        errors.append(f"cqc_location_id: –æ–∂–∏–¥–∞–ª–æ—Å—å '{expected_cqc_id}', –ø–æ–ª—É—á–µ–Ω–æ '{parsed_cqc_id}'")
    else:
        print(f"‚úÖ cqc_location_id: {parsed_cqc_id}")
    
    # 3. city
    parsed_city = location.get('city')
    expected_city = expected_data.get('city')
    if parsed_city != expected_city:
        errors.append(f"city: –æ–∂–∏–¥–∞–ª–æ—Å—å '{expected_city}', –ø–æ–ª—É—á–µ–Ω–æ '{parsed_city}'")
    else:
        print(f"‚úÖ city: {parsed_city}")
    
    # 4. postcode
    parsed_postcode = location.get('postcode')
    expected_postcode = expected_data.get('postcode')
    if parsed_postcode != expected_postcode:
        errors.append(f"postcode: –æ–∂–∏–¥–∞–ª–æ—Å—å '{expected_postcode}', –ø–æ–ª—É—á–µ–Ω–æ '{parsed_postcode}'")
    else:
        print(f"‚úÖ postcode: {parsed_postcode}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π
    care_services = parsed_data.get('care_services', {})
    
    # care_residential
    parsed_residential = care_services.get('care_residential')
    expected_residential = expected_data.get('care_residential')
    if parsed_residential != expected_residential:
        warnings.append(f"care_residential: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_residential}, –ø–æ–ª—É—á–µ–Ω–æ {parsed_residential}")
    else:
        print(f"‚úÖ care_residential: {parsed_residential}")
    
    # care_respite
    parsed_respite = care_services.get('care_respite')
    expected_respite = expected_data.get('care_respite')
    if parsed_respite != expected_respite:
        warnings.append(f"care_respite: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_respite}, –ø–æ–ª—É—á–µ–Ω–æ {parsed_respite}")
    else:
        print(f"‚úÖ care_respite: {parsed_respite}")
    
    # care_dementia
    parsed_dementia = care_services.get('care_dementia')
    expected_dementia = expected_data.get('care_dementia')
    if parsed_dementia != expected_dementia:
        warnings.append(f"care_dementia: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_dementia}, –ø–æ–ª—É—á–µ–Ω–æ {parsed_dementia}")
    else:
        print(f"‚úÖ care_dementia: {parsed_dementia}")
    
    # provider_name
    parsed_provider = identity.get('provider_name')
    expected_provider = expected_data.get('provider_name')
    if parsed_provider != expected_provider:
        warnings.append(f"provider_name: –æ–∂–∏–¥–∞–ª–æ—Å—å '{expected_provider}', –ø–æ–ª—É—á–µ–Ω–æ '{parsed_provider}'")
    else:
        print(f"‚úÖ provider_name: {parsed_provider}")
    
    # local_authority
    parsed_la = location.get('local_authority')
    expected_la = expected_data.get('local_authority')
    if parsed_la != expected_la:
        warnings.append(f"local_authority: –æ–∂–∏–¥–∞–ª–æ—Å—å '{expected_la}', –ø–æ–ª—É—á–µ–Ω–æ '{parsed_la}'")
    else:
        print(f"‚úÖ local_authority: {parsed_la}")
    
    # beds_total
    capacity = parsed_data.get('capacity', {})
    parsed_beds = capacity.get('beds_total')
    expected_beds = expected_data.get('beds_total')
    if parsed_beds != expected_beds:
        warnings.append(f"beds_total: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_beds}, –ø–æ–ª—É—á–µ–Ω–æ {parsed_beds}")
    else:
        print(f"‚úÖ beds_total: {parsed_beds}")
    
    # year_registered
    parsed_year_reg = capacity.get('year_registered')
    expected_year_reg = expected_data.get('year_registered')
    if parsed_year_reg != expected_year_reg:
        warnings.append(f"year_registered: –æ–∂–∏–¥–∞–ª–æ—Å—å {expected_year_reg}, –ø–æ–ª—É—á–µ–Ω–æ {parsed_year_reg}")
    else:
        print(f"‚úÖ year_registered: {parsed_year_reg}")
    
    # CQC rating
    cqc_ratings = parsed_data.get('cqc_ratings', {})
    parsed_rating = cqc_ratings.get('cqc_rating_overall') or cqc_ratings.get('overall_rating')
    expected_rating = expected_data.get('cqc_rating_overall')
    if parsed_rating != expected_rating:
        warnings.append(f"cqc_rating_overall: –æ–∂–∏–¥–∞–ª–æ—Å—å '{expected_rating}', –ø–æ–ª—É—á–µ–Ω–æ '{parsed_rating}'")
    else:
        print(f"‚úÖ cqc_rating_overall: {parsed_rating}")
    
    print("\n" + "="*80)
    if errors:
        print(f"‚ùå –û–®–ò–ë–ö–ò ({len(errors)}):")
        for error in errors:
            print(f"   - {error}")
    else:
        print("‚úÖ –í—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –∏–∑–≤–ª–µ—á–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
    
    if warnings:
        print(f"\n‚ö†Ô∏è  –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø ({len(warnings)}):")
        for warning in warnings:
            print(f"   - {warning}")
    else:
        print("\n‚úÖ –í—Å–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ –ø–æ–ª—è –∏–∑–≤–ª–µ—á–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
    
    return {
        'errors': errors,
        'warnings': warnings,
        'total_errors': len(errors),
        'total_warnings': len(warnings)
    }

def main():
    markdown_file = "input/autumna/Data-MD/html 1 /test1-md.md"
    
    if not os.path.exists(markdown_file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {markdown_file}")
        sys.exit(1)
    
    # –ü–∞—Ä—Å–∏–Ω–≥
    print("="*80)
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–ê–†–°–ò–ù–ì–ê MARKDOWN –§–ê–ô–õ–ê")
    print("="*80)
    
    result = parse_markdown_file(markdown_file)
    
    if not result['success']:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.get('error')}")
        sys.exit(1)
    
    parsed_data = result['data']
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_file = "input/autumna/Data-MD/html 1 /test1-parsed-result.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(parsed_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
    
    # –ò–∑–≤–ª–µ—á—å –æ–∂–∏–¥–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ
    expected_data = extract_expected_data()
    
    # –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    comparison = compare_results(parsed_data, expected_data)
    
    # –í—ã–≤–µ—Å—Ç–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    extraction_meta = parsed_data.get('extraction_metadata', {})
    print("\n" + "="*80)
    print("üìä –ú–ï–¢–ê–î–ê–ù–ù–´–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø")
    print("="*80)
    print(f"   extraction_confidence: {extraction_meta.get('extraction_confidence', 'N/A')}")
    print(f"   data_quality_score: {extraction_meta.get('data_quality_score', 'N/A')}")
    print(f"   is_dormant: {extraction_meta.get('is_dormant', False)}")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "="*80)
    print("üìà –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*80)
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥: –î–∞")
    print(f"   ‚ùå –û—à–∏–±–∫–∏: {comparison['total_errors']}")
    print(f"   ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {comparison['total_warnings']}")
    print(f"   üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {result['usage']['total_tokens']}")
    print(f"      - –ü—Ä–æ–º–ø—Ç: {result['usage']['prompt_tokens']}")
    print(f"      - –û—Ç–≤–µ—Ç: {result['usage']['completion_tokens']}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
    markdown_size = len(open(markdown_file, 'r', encoding='utf-8').read())
    print(f"\nüìè –†–ê–ó–ú–ï–† –î–ê–ù–ù–´–•:")
    print(f"   Markdown —Ñ–∞–π–ª: {markdown_size} —Å–∏–º–≤–æ–ª–æ–≤ (~{markdown_size // 4} —Ç–æ–∫–µ–Ω–æ–≤)")
    print(f"   –ü—Ä–æ–º–ø—Ç: ~6,400 —Ç–æ–∫–µ–Ω–æ–≤")
    print(f"   –í—Å–µ–≥–æ –≤ –∑–∞–ø—Ä–æ—Å–µ: ~{result['usage']['prompt_tokens']} —Ç–æ–∫–µ–Ω–æ–≤")
    print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {result['usage']['prompt_tokens'] / 128000 * 100:.2f}%")

if __name__ == '__main__':
    main()
