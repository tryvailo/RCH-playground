#!/usr/bin/env python3
"""
–§–ê–ó–ê 1: –ó–∞–≥—Ä—É–∑–∫–∞ Markdown –∏–∑ Firecrawl –≤ staging —Ç–∞–±–ª–∏—Ü—É
================================================================
–¶–µ–ª—å: –°–æ—Ö—Ä–∞–Ω–∏—Ç—å Markdown –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü Autumna –≤ staging —Ç–∞–±–ª–∏—Ü—É –æ–¥–∏–Ω —Ä–∞–∑

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python phase1_load_html.py --urls urls.txt --api-key FIRECRAWL_API_KEY
    python phase1_load_html.py --urls urls.txt --api-key FIRECRAWL_API_KEY --api-version v2.5 --use-cache

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
    - –°–ø–∏—Å–æ–∫ URL –≤ —Ñ–∞–π–ª–µ (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É)
    - Firecrawl API –∫–ª—é—á
    - PostgreSQL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤ .env

Firecrawl API v2.5:
    - üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π semantic index (40% –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–±—Å–ª—É–∂–∏–≤–∞—é—Ç—Å—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ)
    - üéØ –ö–∞—Å—Ç–æ–º–Ω—ã–π browser stack –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    - üì¶ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ "as of now" –∏–ª–∏ "as of last known good copy" —á–µ—Ä–µ–∑ useCache
    - ‚ö° –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–∞ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
"""

import os
import sys
import json
import time
import argparse
import psycopg2
from psycopg2.extras import execute_values
from dotenv import load_dotenv
import requests
from typing import List, Dict, Optional
import re

load_dotenv()

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
FIRECRAWL_BASE_URL = "https://api.firecrawl.dev"
BATCH_SIZE = 50  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è Firecrawl
RETRY_DELAY = 5  # –°–µ–∫—É–Ω–¥ –º–µ–∂–¥—É –ø–æ–ø—ã—Ç–∫–∞–º–∏


def extract_cqc_id_from_url(url: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ—á—å CQC Location ID –∏–∑ URL Autumna"""
    match = re.search(r'/1-(\d{10})', url)
    if match:
        return f"1-{match.group(1)}"
    return None


def get_db_connection():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
    return psycopg2.connect(
        host=os.getenv('DB_HOST', 'localhost'),
        port=os.getenv('DB_PORT', '5432'),
        database=os.getenv('DB_NAME', 'care_homes_db'),
        user=os.getenv('DB_USER', 'postgres'),
        password=os.getenv('DB_PASSWORD', '')
    )


def scrape_urls_with_firecrawl(
    urls: List[str], 
    api_key: str, 
    api_version: str = "v2.5",
    use_cache: bool = False
) -> List[Dict]:
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å URLs –≤ Firecrawl API –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
    
    Args:
        urls: –°–ø–∏—Å–æ–∫ URL –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
        api_key: Firecrawl API –∫–ª—é—á
        api_version: –í–µ—Ä—Å–∏—è API ("v1", "v2", "v2.5"). –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é v2.5
        use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å semantic index cache (v2.5+) –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏ —ç–∫–æ–Ω–æ–º–∏–∏
    
    Returns:
        List[Dict] —Å –∫–ª—é—á–∞–º–∏: url, markdown_content, metadata, status
    """
    results = []
    
    # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å endpoint –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏
    if api_version.startswith("v2"):
        # v2.5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π endpoint —Å semantic index
        endpoint = f"{FIRECRAWL_BASE_URL}/v2/scrape/batch"
    else:
        # v1 - —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        endpoint = f"{FIRECRAWL_BASE_URL}/v1/scrape/batch"
    
    print(f"üåê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Firecrawl API {api_version} (endpoint: {endpoint})")
    if use_cache and api_version.startswith("v2"):
        print("   ‚úÖ Semantic index cache –≤–∫–ª—é—á–µ–Ω (—ç–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å—Ä–µ–¥—Å—Ç–≤)")
    
    # Firecrawl –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç batch –∑–∞–ø—Ä–æ—Å—ã
    for i in range(0, len(urls), BATCH_SIZE):
        batch = urls[i:i+BATCH_SIZE]
        print(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//BATCH_SIZE + 1}/{(len(urls)-1)//BATCH_SIZE + 1} ({len(batch)} URLs)...")
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å payload –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–µ—Ä—Å–∏–∏ API
            payload = {
                "urls": batch,
                "format": "markdown"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º markdown –≤–º–µ—Å—Ç–æ HTML –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
            }
            
            # v2.5+ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏
            if api_version.startswith("v2"):
                payload.update({
                    "useCache": use_cache,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å semantic index –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
                    # –î—Ä—É–≥–∏–µ –æ–ø—Ü–∏–∏ v2.5 –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∑–¥–µ—Å—å:
                    # "timeout": 60000,  # —Ç–∞–π–º–∞—É—Ç –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
                    # "waitFor": 0,  # –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                })
            
            # –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤ Firecrawl
            response = requests.post(
                endpoint,
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json=payload,
                timeout=300  # 5 –º–∏–Ω—É—Ç –Ω–∞ –±–∞—Ç—á
            )
            
            if response.status_code != 200:
                print(f"‚ùå –û—à–∏–±–∫–∞ Firecrawl: {response.status_code} - {response.text}")
                # –î–æ–±–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ—à–∏–±–∫–æ–π
                for url in batch:
                    results.append({
                        'url': url,
                        'markdown_content': None,
                        'metadata': {
                            'status': 'error', 
                            'error': response.text,
                            'api_version': api_version
                        },
                        'status': 'error'
                    })
                continue
            
            data = response.json()
            
            # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤ v2.5)
            results_data = data.get('data', data.get('results', []))
            
            for item in results_data:
                # v2.5 –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: markdown > content > html (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                content = item.get('markdown') or item.get('content') or item.get('html', '')
                
                results.append({
                    'url': item.get('url', ''),
                    'markdown_content': content,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ markdown_content
                    'metadata': {
                        'status': item.get('status', 'success'),
                        'scraped_at': item.get('metadata', {}).get('timestamp', time.strftime('%Y-%m-%dT%H:%M:%SZ')),
                        'title': item.get('metadata', {}).get('title', ''),
                        'api_version': api_version,
                        'from_cache': item.get('metadata', {}).get('fromCache', False) if use_cache else None,
                        'source': item.get('metadata', {}).get('source', 'unknown'),  # v2.5 –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫
                    },
                    'status': item.get('status', 'success')
                })
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ (–º–µ–Ω—å—à–µ –¥–ª—è v2.5 —Å cache)
            if i + BATCH_SIZE < len(urls):
                delay = 1 if (use_cache and api_version.startswith("v2")) else 2
                time.sleep(delay)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–∞—Ç—á–∞: {e}")
            # –î–æ–±–∞–≤–∏—Ç—å –ø—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ—à–∏–±–∫–æ–π
            for url in batch:
                results.append({
                    'url': url,
                    'markdown_content': None,
                    'metadata': {
                        'status': 'error', 
                        'error': str(e),
                        'api_version': api_version
                    },
                    'status': 'error'
                })
    
    return results


def save_to_staging(conn, results: List[Dict]):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ staging —Ç–∞–±–ª–∏—Ü—É"""
    cursor = conn.cursor()
    
    success_count = 0
    error_count = 0
    skipped_count = 0
    
    for result in results:
        url = result['url']
        cqc_id = extract_cqc_id_from_url(url)
        markdown_content = result.get('markdown_content')
        metadata = result.get('metadata', {})
        status = result.get('status', 'unknown')
        
        if not markdown_content:
            print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç Markdown): {url}")
            error_count += 1
            continue
        
        try:
            cursor.execute("""
                INSERT INTO autumna_staging (
                    source_url,
                    cqc_location_id,
                    scraped_at,
                    markdown_content,
                    firecrawl_metadata
                ) VALUES (
                    %(url)s,
                    %(cqc_id)s,
                    CURRENT_TIMESTAMP,
                    %(markdown_content)s,
                    %(metadata)s::jsonb
                )
                ON CONFLICT (source_url) DO UPDATE
                SET 
                    markdown_content = EXCLUDED.markdown_content,
                    firecrawl_metadata = EXCLUDED.firecrawl_metadata,
                    scraped_at = EXCLUDED.scraped_at,
                    updated_at = CURRENT_TIMESTAMP
                RETURNING id
            """, {
                'url': url,
                'cqc_id': cqc_id,
                'markdown_content': markdown_content,
                'metadata': json.dumps(metadata)
            })
            
            staging_id = cursor.fetchone()[0]
            success_count += 1
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {url} (ID: {staging_id})")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {url}: {e}")
            error_count += 1
    
    conn.commit()
    cursor.close()
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {success_count}")
    print(f"   ‚ùå –û—à–∏–±–∫–∏: {error_count}")
    print(f"   ‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
    
    return success_count, error_count


def load_urls_from_file(filepath: str) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ URL –∏–∑ —Ñ–∞–π–ª–∞"""
    urls = []
    with open(filepath, 'r') as f:
        for line in f:
            url = line.strip()
            if url and url.startswith('http'):
                urls.append(url)
    return urls


def main():
    parser = argparse.ArgumentParser(
        description='–§–∞–∑–∞ 1: –ó–∞–≥—Ä—É–∑–∫–∞ Markdown –∏–∑ Firecrawl –≤ staging',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å v2.5 —Å semantic index cache (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
  python phase1_load_html.py --urls urls.txt --api-key $FIRECRAWL_API_KEY --api-version v2.5 --use-cache
  
  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å v2.5 –±–µ–∑ cache (—Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ)
  python phase1_load_html.py --urls urls.txt --api-key $FIRECRAWL_API_KEY --api-version v2.5
  
  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é v1 (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
  python phase1_load_html.py --urls urls.txt --api-key $FIRECRAWL_API_KEY --api-version v1
        """
    )
    parser.add_argument('--urls', required=True, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º URL')
    parser.add_argument('--api-key', required=True, help='Firecrawl API –∫–ª—é—á')
    parser.add_argument('--api-version', default='v2.5', choices=['v1', 'v2', 'v2.5'], 
                       help='–í–µ—Ä—Å–∏—è Firecrawl API (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: v2.5)')
    parser.add_argument('--use-cache', action='store_true', 
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å semantic index cache (v2.5+). –£—Å–∫–æ—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –∏ —ç–∫–æ–Ω–æ–º–∏—Ç —Å—Ä–µ–¥—Å—Ç–≤–∞')
    parser.add_argument('--dry-run', action='store_true', help='–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∑–∏—Ç—å URLs
    print(f"üìã –ó–∞–≥—Ä—É–∑–∫–∞ URLs –∏–∑ {args.urls}...")
    urls = load_urls_from_file(args.urls)
    print(f"   –ù–∞–π–¥–µ–Ω–æ {len(urls)} URLs")
    
    if args.dry_run:
        print("üß™ DRY RUN - URLs –Ω–µ –±—É–¥—É—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ Firecrawl")
        print(f"\n–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
        print(f"  - API –≤–µ—Ä—Å–∏—è: {args.api_version}")
        print(f"  - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å cache: {args.use_cache}")
        print("\n–ü–µ—Ä–≤—ã–µ 5 URLs:")
        for url in urls[:5]:
            print(f"  - {url}")
        return
    
    # –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –ë–î
    print("\nüîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î...")
    conn = get_db_connection()
    print("   ‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ")
    
    # –°–∫—Ä–∞–ø–∏—Ç—å —á–µ—Ä–µ–∑ Firecrawl
    print("\nüöÄ –ó–∞–ø—É—Å–∫ Firecrawl —Å–∫—Ä–∞–ø–∏–Ω–≥–∞...")
    results = scrape_urls_with_firecrawl(
        urls, 
        args.api_key, 
        api_version=args.api_version,
        use_cache=args.use_cache
    )
    
    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ staging
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ staging —Ç–∞–±–ª–∏—Ü—É...")
    success, errors = save_to_staging(conn, results)
    
    conn.close()
    
    print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ! –£—Å–ø–µ—à–Ω–æ: {success}, –û—à–∏–±–æ–∫: {errors}")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ cache (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è)
    if args.use_cache:
        cache_hits = sum(1 for r in results if r.get('metadata', {}).get('from_cache'))
        if cache_hits > 0:
            print(f"\nüí° Semantic index cache –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è {cache_hits} –∏–∑ {len(results)} –∑–∞–ø—Ä–æ—Å–æ–≤")


if __name__ == '__main__':
    main()

