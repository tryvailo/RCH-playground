# RightCareHome: –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ü–ª–∞–Ω –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –í–∞–ª–∏–¥–∞—Ü–∏–∏ API
**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** –ù–æ—è–±—Ä—å 2025  
**–°—Ç–∞—Ç—É—Å:** –ü–ª–∞–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–û–±–∑–æ—Ä —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è](#–æ–±–∑–æ—Ä)
2. [–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ API](#–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ-api)
   - [CQC API](#1-cqc-api)
   - [FSA FHRS API](#2-fsa-fhrs-api)
   - [Companies House API](#3-companies-house-api)
3. [–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ API](#–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ-api)
   - [Google Places API](#4-google-places-api)
   - [Google Places Insights (BigQuery)](#5-google-places-insights-bigquery)
   - [Perplexity Search API](#6-perplexity-search-api)
4. [–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏](#–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ-–∏—Å—Ç–æ—á–Ω–∏–∫–∏)
   - [Autumna (–≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥)](#7-autumna-–≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥)
5. [–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏ –∫–µ–π—Å—ã](#–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
6. [Roadmap —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è](#roadmap)

---

<a name="–æ–±–∑–æ—Ä"></a>
## üéØ –û–±–∑–æ—Ä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

### –¶–µ–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
- ‚úÖ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≤—Å–µ—Ö API
- ‚úÖ –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏ –ø–æ–ª–Ω–æ—Ç—É –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å –∏ rate limits
- ‚úÖ –í—ã—è–≤–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- ‚úÖ –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã

### –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
1. **–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å**: API –æ—Ç–≤–µ—á–∞–µ—Ç –≤ —Ç–µ—á–µ–Ω–∏–µ 2 —Å–µ–∫—É–Ω–¥
2. **–ü–æ–ª–Ω–æ—Ç–∞**: –î–∞–Ω–Ω—ã–µ –ø–æ–∫—Ä—ã–≤–∞—é—Ç 90%+ –¥–æ–º–æ–≤ –ø—Ä–µ—Å—Ç–∞—Ä–µ–ª—ã—Ö –≤ —Ä–µ–≥–∏–æ–Ω–µ
3. **–ö–∞—á–µ—Å—Ç–≤–æ**: –î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã (–æ–±–Ω–æ–≤–ª–µ–Ω—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π)
4. **–°—Ç–æ–∏–º–æ—Å—Ç—å**: –£–∫–ª–∞–¥—ã–≤–∞–µ–º—Å—è –≤ –±—é–¥–∂–µ—Ç ¬£200/–º–µ—Å—è—Ü –Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

---

<a name="–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ-api"></a>
# üèõÔ∏è –ß–ê–°–¢–¨ 1: –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–ï API

<a name="1-cqc-api"></a>
## 1. CQC API (Care Quality Commission)

### üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://api-portal.service.cqc.org.uk/
- **API Base URL**: `https://api.cqc.org.uk/public/v1`
- **–ù–æ–≤—ã–π Base URL**: `https://api.service.cqc.org.uk`
- **GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏**: https://github.com/evanodell/cqcr
- **RAML —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è**: https://anypoint.mulesoft.com/exchange/portals/care-quality-commission-5/

### üîë –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
```
–¢–∏–ø: Partner Code (query parameter)
–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ: –î–æ–±–∞–≤–∏—Ç—å partnerCode –≤ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å
Rate limit: –î–æ 2000 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω—É—Ç—É —Å partnerCode
–ë–µ–∑ partnerCode: –ú–æ–∂–µ—Ç –±—ã—Ç—å throttling
```

### üß™ –¢–µ—Å—Ç–æ–≤—ã–µ –ó–∞–ø—Ä–æ—Å—ã

#### –¢–µ—Å—Ç 1: –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ–º–æ–≤ –ø—Ä–µ—Å—Ç–∞—Ä–µ–ª—ã—Ö –≤ South East
```bash
# –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å - –ø–æ–ª—É—á–∏—Ç—å –ø–µ—Ä–≤—ã–µ 100 –¥–æ–º–æ–≤
curl -X GET "https://api.cqc.org.uk/public/v1/locations?perPage=100&page=1&region=South+East&careHome=true&partnerCode=RIGHTCAREHOME" \
  -H "Accept: application/json"

# –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:
# - HTTP 200
# - JSON —Å –º–∞—Å—Å–∏–≤–æ–º locations
# - –ü–æ–ª—è: locationId, name, postalCode, localAuthority, currentRatings
```

#### –¢–µ—Å—Ç 2: –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –¥–æ–º–µ
```bash
# –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ location ID
curl -X GET "https://api.cqc.org.uk/public/v1/locations/{locationId}?partnerCode=RIGHTCAREHOME" \
  -H "Accept: application/json"

# –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:
# - currentRatings (overall, safe, effective, caring, responsive, well-led)
# - specialisms (–¥–µ–º–µ–Ω—Ü–∏—è, –¥–∏–∞–±–µ—Ç, –ø–∞–ª–ª–∏–∞—Ç–∏–≤–Ω—ã–π —É—Ö–æ–¥)
# - numberOfBeds
# - lastInspection date
# - mainPhoneNumber, website
```

#### –¢–µ—Å—Ç 3: –ò—Å—Ç–æ—Ä–∏—è –∏–Ω—Å–ø–µ–∫—Ü–∏–π
```bash
# –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –æ—Ç—á–µ—Ç—ã –∏–Ω—Å–ø–µ–∫—Ü–∏–π
curl -X GET "https://api.cqc.org.uk/public/v1/locations/{locationId}/reports?partnerCode=RIGHTCAREHOME" \
  -H "Accept: application/json"

# –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º:
# - –ò—Å—Ç–æ—Ä–∏—è —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç
# - –î–∞—Ç—ã –∏–Ω—Å–ø–µ–∫—Ü–∏–π
# - Enforcement actions (–µ—Å–ª–∏ –µ—Å—Ç—å)
# - URL –∫ PDF –æ—Ç—á–µ—Ç–∞–º
```

#### –¢–µ—Å—Ç 4: –ü–æ–∏—Å–∫ –ø–æ Provider (–∫–æ–º–ø–∞–Ω–∏–∏-–æ–ø–µ—Ä–∞—Ç–æ—Ä—É)
```bash
# –ù–∞–π—Ç–∏ –≤—Å–µ –ª–æ–∫–∞—Ü–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
curl -X GET "https://api.cqc.org.uk/public/v1/providers/{providerId}/locations?partnerCode=RIGHTCAREHOME" \
  -H "Accept: application/json"

# –ó–∞—á–µ–º: –û—Ç—Å–ª–µ–¥–∏—Ç—å –≤—Å–µ –¥–æ–º–∞ –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ—Ç–∏
```

### üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–∞–ª–∏–¥–∞—Ü–∏–∏

**–¢–µ—Å—Ç–æ–≤—ã–µ –ª–æ–∫–∞—Ü–∏–∏** (—Ä–µ–∞–ª—å–Ω—ã–µ –¥–æ–º–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è):
1. Location ID: `1-101669846` (–ø—Ä–∏–º–µ—Ä Outstanding rated)
2. Provider ID: `1-1016936648` (HC-One, –∫—Ä—É–ø–Ω–∞—è —Å–µ—Ç—å)

**–ß—Ç–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å:**
- [ ] –í—Å–µ –¥–æ–º–∞ –≤ South East –∏–º–µ—é—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (longitude/latitude)
- [ ] 95%+ –¥–æ–º–æ–≤ –∏–º–µ—é—Ç currentRating (–Ω–µ null)
- [ ] specialisms –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω—ã
- [ ] lastInspection –Ω–µ —Å—Ç–∞—Ä—à–µ 18 –º–µ—Å—è—Ü–µ–≤ (—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- [ ] mainPhoneNumber –≤ –≤–∞–ª–∏–¥–Ω–æ–º UK —Ñ–æ—Ä–º–∞—Ç–µ

### üí° –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –§–∏—à–∫–∏

#### –§–∏—à–∫–∞ 1: Tracking Changes API
```bash
# –ü–æ–ª—É—á–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
curl -X GET "https://api.cqc.org.uk/public/v1/changes?startDate=2025-11-04&partnerCode=RIGHTCAREHOME" \
  -H "Accept: application/json"

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ –¥–ª—è:
# - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
# - –ê–ª–µ—Ä—Ç–æ–≤ –æ–± –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
# - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–æ–≤—ã—Ö enforcement actions
```

#### –§–∏—à–∫–∞ 2: Relationships API
```bash
# –ù–∞–π—Ç–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ª–æ–∫–∞—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥—Ä—É–≥–∏–µ –¥–æ–º–∞ —Ç–æ–≥–æ –∂–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞)
curl -X GET "https://api.cqc.org.uk/public/v1/locations/{locationId}/relationships?partnerCode=RIGHTCAREHOME"

# –ö–µ–π—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
# "Manor House Care —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è HC-One. 
#  –ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ –¥—Ä—É–≥–∏–µ 15 –¥–æ–º–æ–≤ HC-One –≤ —Ä–µ–≥–∏–æ–Ω–µ"
```

#### –§–∏—à–∫–∞ 3: Specialisms –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è
**Hidden Insight**: –í –ø–æ–ª–µ `specialisms` –µ—Å—Ç—å –Ω–µ —Ç–æ–ª—å–∫–æ basic —Ç–∏–ø—ã (dementia, diabetes), –Ω–æ –∏:
- `learningDisabilities` - –¥–æ–º–∞ –¥–ª—è –ª—é–¥–µ–π —Å –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—è–º–∏ —Ä–∞–∑–≤–∏—Ç–∏—è
- `mentalHealth` - –ø—Å–∏—Ö–∏–∞—Ç—Ä–∏—á–µ—Å–∫–∞—è –ø–æ–º–æ—â—å
- `physicalDisabilities` - —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
- `sensoryImpairments` - –Ω–∞—Ä—É—à–µ–Ω–∏—è –∑—Ä–µ–Ω–∏—è/—Å–ª—É—Ö–∞

**–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è**: –£–ª—å—Ç—Ä–∞-—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è Jane —Å –¥–∏–∞–±–µ—Ç–æ–º –ò —Ä–∞–Ω–Ω–µ–π –¥–µ–º–µ–Ω—Ü–∏–µ–π)

### üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ö–µ–π—Å—ã

#### –ö–µ–π—Å 1: Risk Prediction Model
```python
# –ü—Å–µ–≤–¥–æ–∫–æ–¥: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–∏—Å–∫–∞ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–µ–π—Ç–∏–Ω–≥–∞
risk_score = 0

# –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞:
if last_inspection > 15_months_ago:
    risk_score += 20  # –î–∞–≤–Ω–æ –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–ª–∏—Å—å

if rating_declined_last_time:
    risk_score += 30  # –ò—Å—Ç–æ—Ä–∏—è —Å–Ω–∏–∂–µ–Ω–∏—è

if provider_has_multiple_poor_ratings:
    risk_score += 25  # –°–µ—Ç—å —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏

if enforcement_actions > 0:
    risk_score += 40  # –ê–∫—Ç–∏–≤–Ω—ã–µ —Å–∞–Ω–∫—Ü–∏–∏

# –†–µ–∑—É–ª—å—Ç–∞—Ç: –ê–ª–µ—Ä—Ç –¥–ª—è Premium –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤
if risk_score > 60:
    alert_user("High risk of quality decline")
```

---

<a name="2-fsa-fhrs-api"></a>
## 2. FSA FHRS API (Food Hygiene Rating Scheme)

### üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://api.ratings.food.gov.uk/help
- **API Base URL**: `https://api.ratings.food.gov.uk`
- **Status page**: https://api.ratings.food.gov.uk/Help/Status
- **Open Data Portal**: https://data.food.gov.uk

### üîë –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
```
–¢–∏–ø: Header-based versioning
–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ: 
  - x-api-version: 2 (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)
  - Accept-Language: en-GB –∏–ª–∏ cy-GB (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
Rate limit: Throttling –ø—Ä–∏ high volume (>1/sec)
–°—Ç–æ–∏–º–æ—Å—Ç—å: –ë–ï–°–ü–õ–ê–¢–ù–û
```

### üß™ –¢–µ—Å—Ç–æ–≤—ã–µ –ó–∞–ø—Ä–æ—Å—ã

#### –¢–µ—Å—Ç 1: –ü–æ–∏—Å–∫ care homes –ø–æ Local Authority
```http
GET http://api.ratings.food.gov.uk/Establishments?localAuthorityId=128&businessTypeId=7841
x-api-version: 2
Accept-Language: en-GB

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
# localAuthorityId=128 (Brighton & Hove, –Ω–∞–ø—Ä–∏–º–µ—Ä)
# businessTypeId=7841 (Caring Premises - –≤–∫–ª—é—á–∞–µ—Ç care homes)

# –ß—Ç–æ –ø–æ–ª—É—á–∞–µ–º:
# - FHRSID (—É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID)
# - BusinessName
# - RatingValue (0-5 –∏–ª–∏ "Pass"/"AwaitingInspection")
# - RatingDate
# - Scores: Hygiene, Structural, ConfidenceInManagement
# - Geocode (lat/lon)
```

#### –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∑–∞–≤–µ–¥–µ–Ω–∏—è
```http
GET http://api.ratings.food.gov.uk/Establishments/{FHRSID}
x-api-version: 2

# CRITICAL: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ FHRSID, –ù–ï EstablishmentID!
# FHRSID —Å—Ç–∞–±–∏–ª—å–Ω—ã–π, EstablishmentID –º–æ–∂–µ—Ç –º–µ–Ω—è—Ç—å—Å—è

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è:
# - RatingKey: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –ø–æ–∫–∞–∑–∞ –∏–∫–æ–Ω–æ–∫ —Ä–µ–π—Ç–∏–Ω–≥–∞
# - RightToReply: –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞
# - NewRatingPending: –û–∂–∏–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
# - SchemeType: FHRS –∏–ª–∏ FHIS (–®–æ—Ç–ª–∞–Ω–¥–∏—è)
```

#### –¢–µ—Å—Ç 3: Batch –ø–æ–∏—Å–∫ –ø–æ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏ + –Ω–∞–∑–≤–∞–Ω–∏–µ
```http
GET http://api.ratings.food.gov.uk/Establishments?name=Manor+House&latitude=51.5074&longitude=-0.1278&maxDistanceLimit=1
x-api-version: 2

# –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ:
# - name (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
# - –≥–µ–æ–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ CQC API
# - maxDistanceLimit –≤ –º–∏–ª—è—Ö

# –ö–µ–π—Å: –ú–∞—Ç—á–∏–Ω–≥ CQC location —Å FSA establishment
```

#### –¢–µ—Å—Ç 4: –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ Local Authorities
```http
GET http://api.ratings.food.gov.uk/Authorities
x-api-version: 2

# –ó–∞—á–µ–º: 
# - –ü–æ—Å—Ç—Ä–æ–∏—Ç—å mapping LocalAuthorityId ‚Üí Name
# - –î–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ —Ä–µ–≥–∏–æ–Ω—É
```

### üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–∞–ª–∏–¥–∞—Ü–∏–∏

**–¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–≤–µ–¥–µ–Ω–∏—è**:
1. FHRSID: `1234567` (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –∏–∑ –≤–∞—à–µ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞)
2. BusinessType: `7841` (Caring Premises)

**–ß—Ç–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å:**
- [ ] RatingValue –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç —É 90%+ –∑–∞–ø–∏—Å–µ–π
- [ ] RatingDate –Ω–µ —Å—Ç–∞—Ä—à–µ 18 –º–µ—Å—è—Ü–µ–≤ (—Ä–µ–≥—É–ª—è—Ç–æ—Ä–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- [ ] Scores (Hygiene, Structural, Management) —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ:
  - `0` = Very Good
  - `5` = Poor
  - `10` = Major Improvement Necessary
- [ ] Geocode —Ç–æ—á–Ω–æ—Å—Ç—å: ¬±100 –º–µ—Ç—Ä–æ–≤ –æ—Ç CQC –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç

### üí° –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –§–∏—à–∫–∏

#### –§–∏—à–∫–∞ 1: RatingKey –¥–ª—è –∏–∫–æ–Ω–æ–∫
```javascript
// FSA –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –≥–æ—Ç–æ–≤—ã–µ –∏–∫–æ–Ω–∫–∏
const ratingImages = {
  'fhrs_5_en-gb': 'https://ratings.food.gov.uk/images/scores/en-GB/small/5.jpg',
  'fhrs_4_en-gb': 'https://ratings.food.gov.uk/images/scores/en-GB/small/4.jpg',
  // ... –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ
}

// –í response API –µ—Å—Ç—å –ø–æ–ª–µ RatingKey, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ:
if (establishment.RatingKey === 'fhrs_5_en-gb') {
    showImage(ratingImages['fhrs_5_en-gb'])
}
```

#### –§–∏—à–∫–∞ 2: RightToReply - –∑–æ–ª–æ—Ç–∞—è –∂–∏–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
```python
# –ú–Ω–æ–≥–∏–µ –¥–æ–º–∞ –ø–∏—à—É—Ç —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –Ω–∏–∑–∫–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∏
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ –¥–ª—è:

if establishment['RightToReply']:
    # 1. Sentiment analysis (–∏–∑–≤–ª–µ—á—å, –ø—Ä–∏–∑–Ω–∞—é—Ç –ª–∏ –ø—Ä–æ–±–ª–µ–º—É)
    sentiment = analyze_sentiment(establishment['RightToReply'])
    
    # 2. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ–± —É–ª—É—á—à–µ–Ω–∏—è—Ö
    if "refurbished" in text or "new kitchen" in text:
        context = "Home invested in improvements post-inspection"
    
    # 3. Red flag detection
    if "disagree with inspector" in text:
        flag = "Defensive response - may indicate issues"
```

#### –§–∏—à–∫–∞ 3: SchemeType —Ä–∞–∑–ª–∏—á–∏—è
```
FHRS (England, Wales, N. Ireland): 0-5 stars
FHIS (Scotland): Pass / Improvement Required

# –ü—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:
if scheme == "FHIS":
    if rating == "Pass":
        display_as = "‚úì Passed Inspection"
    else:
        display_as = "‚ö† Improvement Required"
```

#### –§–∏—à–∫–∞ 4: NewRatingPending —Ñ–ª–∞–≥
```python
# –°—É–ø–µ—Ä –ø–æ–ª–µ–∑–Ω–æ –¥–ª—è real-time –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
if establishment['NewRatingPending'] == True:
    # –î–æ–º –Ω–µ–¥–∞–≤–Ω–æ –ø—Ä–æ—à–µ–ª –∏–Ω—Å–ø–µ–∫—Ü–∏—é, —Ä–µ–π—Ç–∏–Ω–≥ –æ–±–Ω–æ–≤–∏—Ç—Å—è –≤ –±–ª–∏–∂–∞–π—à–∏–µ 2 –Ω–µ–¥–µ–ª–∏
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∞–ª–µ—Ä—Ç Premium —é–∑–µ—Ä–∞–º:
    alert = f"{home_name} –±—ã–ª –ø—Ä–æ–∏–Ω—Å–ø–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω. –ù–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —Å–∫–æ—Ä–æ."
```

### üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ö–µ–π—Å—ã

#### –ö–µ–π—Å 1: FSA + CQC Correlation Analysis
```python
# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É Food Hygiene –∏ CQC Rating
correlation_study = {
    'FSA_5_CQC_Outstanding': 0.67,  # 67% –¥–æ–º–æ–≤ —Å FSA 5 –∏–º–µ—é—Ç CQC Outstanding
    'FSA_3_CQC_RequiresImprovement': 0.43,  # –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–æ–±–ª–µ–º
    'FSA_5_CQC_Good': 0.28  # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–æ–º–∞ –æ—Ç–ª–∏—á–Ω—ã–µ –≤ –µ–¥–µ, —Å—Ä–µ–¥–Ω–∏–µ –≤ —Ü–µ–ª–æ–º
}

# Insight –¥–ª—è —é–∑–µ—Ä–æ–≤:
if fsa_rating >= 5 and cqc_rating == "Good":
    note = "Food safety is EXCEPTIONAL (top 15%), even though overall care is 'Good'"
```

#### –ö–µ–π—Å 2: Diabetes-Specific Filtering
```python
# –î–ª—è Jane —Å –¥–∏–∞–±–µ—Ç–æ–º - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç–æ—Ä
def diabetes_safe_homes(homes):
    safe_homes = []
    for home in homes:
        fsa = get_fsa_rating(home)
        
        # –°—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –¥–∏–∞–±–µ—Ç–∏–∫–æ–≤
        if fsa['RatingValue'] >= 4 and fsa['Scores']['Hygiene'] <= 5:
            # Hygiene score 0-5 = Very Good to Good
            safe_homes.append(home)
            
    return safe_homes
```

#### –ö–µ–π—Å 3: Historical Trend Analysis
```python
# –û—Ç—Å–ª–µ–¥–∏—Ç—å —É–ª—É—á—à–µ–Ω–∏–µ/—É—Ö—É–¥—à–µ–Ω–∏–µ (—Ç—Ä–µ–±—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö)
# FSA API –Ω–µ –¥–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏–∏, –ø–æ—ç—Ç–æ–º—É:

# 1. –î–µ–ª–∞–π—Ç–µ snapshot –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é
weekly_snapshot = fetch_all_establishments()
save_to_db(weekly_snapshot, date=today)

# 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ç—Ä–µ–Ω–¥—ã
if rating_dropped_from_5_to_3:
    alert = "‚ö†Ô∏è MAJOR DECLINE in food safety. Investigate immediately."
```

---

<a name="3-companies-house-api"></a>
## 3. Companies House API

### üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://developer.company-information.service.gov.uk/
- **API Explorer**: https://developer-specs.company-information.service.gov.uk/
- **Base URL**: `https://api.company-information.service.gov.uk`
- **GitHub SDK**: https://github.com/zinggg/uk_companies_house

### üîë –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
```
–¢–∏–ø: HTTP Basic Auth (API key as username, password empty)
–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞:
  1. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å: https://developer.company-information.service.gov.uk/
  2. Create an application
  3. Generate API key
Rate limit: –ù–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è < 600 req/min
–°—Ç–æ–∏–º–æ—Å—Ç—å: –ë–ï–°–ü–õ–ê–¢–ù–û
```

### üß™ –¢–µ—Å—Ç–æ–≤—ã–µ –ó–∞–ø—Ä–æ—Å—ã

#### –¢–µ—Å—Ç 1: –ü–æ–∏—Å–∫ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
```bash
# –ù–∞–π—Ç–∏ –∫–æ–º–ø–∞–Ω–∏—é-–æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ care home
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/search/companies?q=Manor+House+Care"

# Response –≤–∫–ª—é—á–∞–µ—Ç:
# - company_number
# - company_status (active, dissolved, liquidation, etc.)
# - date_of_creation
# - company_type
# - registered_office_address
```

#### –¢–µ—Å—Ç 2: –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª—å –∫–æ–º–ø–∞–Ω–∏–∏
```bash
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/company/{company_number}"

# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è:
# - company_status (active = OK, liquidation = RED FLAG)
# - sic_codes (Standard Industrial Classification)
# - accounts.next_due (–¥–∞—Ç–∞ —Å–ª–µ–¥—É—é—â–µ–π –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏)
# - accounts.overdue (–ø—Ä–æ—Å—Ä–æ—á–∫–∞ = —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã)
# - has_insolvency_history (boolean)
# - has_charges (–∑–∞–ª–æ–≥–∏/–¥–æ–ª–≥–∏)
```

#### –¢–µ—Å—Ç 3: –ò—Å—Ç–æ—Ä–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤
```bash
# –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ—Ñ–∏—Ü–µ—Ä–æ–≤ –∫–æ–º–ø–∞–Ω–∏–∏
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/company/{company_number}/officers"

# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:
# - –ß–∞—Å—Ç–æ—Ç–∞ —Å–º–µ–Ω—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ (–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –µ—Å–ª–∏ >2 –≤ –≥–æ–¥)
# - Resigned date (—É–≤–æ–ª—å–Ω–µ–Ω–∏—è —Ç–æ–ø-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞ = warning)
# - Occupation (–µ—Å–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä - "care home manager", —ç—Ç–æ —Ö–æ—Ä–æ—à–∏–π –∑–Ω–∞–∫)
```

#### –¢–µ—Å—Ç 4: –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã
```bash
# –ü–æ–ª—É—á–∏—Ç—å filing history (—Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/company/{company_number}/filing-history"

# –ò—Å–∫–∞—Ç—å:
# - category: "accounts" (–≥–æ–¥–æ–≤—ã–µ –æ—Ç—á–µ—Ç—ã)
# - type: "AA" (Annual Accounts)
# - date: –ö–∞–∫ —á–∞—Å—Ç–æ –ø–æ–¥–∞—é—Ç (—Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å = stability)

# –°–∫–∞—á–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç:
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/document/{document_id}/content" \
  --output accounts.pdf
```

#### –¢–µ—Å—Ç 5: Insolvency Check
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/company/{company_number}/insolvency"

# –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å = –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô RED FLAG
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–æ–º–æ–≤ –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
```

### üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–∞–ª–∏–¥–∞—Ü–∏–∏

**–¢–µ—Å—Ç–æ–≤—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏**:
1. Company Number: `06790962` (HC-One - –∫—Ä—É–ø–Ω–∞—è —Å–µ—Ç—å)
2. Company Number: `03553455` (Four Seasons Health Care - –±—ã–ª–∞ –≤ –ø—Ä–æ–±–ª–µ–º–∞—Ö)

**–ß—Ç–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å:**
- [ ] company_status = "active" —É –≤—Å–µ—Ö —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –¥–æ–º–æ–≤
- [ ] accounts.overdue = false (–Ω–µ—Ç –ø—Ä–æ—Å—Ä–æ—á–µ–∫)
- [ ] has_insolvency_history = false
- [ ] date_of_creation: –∫–æ–º–ø–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç >3 –ª–µ—Ç (—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
- [ ] officers: –Ω–µ—Ç –º–∞—Å—Å–æ–≤—ã—Ö —É–≤–æ–ª—å–Ω–µ–Ω–∏–π –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 6 –º–µ—Å—è—Ü–µ–≤

### üí° –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –§–∏—à–∫–∏

#### –§–∏—à–∫–∞ 1: SIC Codes –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
```python
# Standard Industrial Classification Codes –¥–ª—è care homes
care_home_sic_codes = [
    '87100',  # Residential nursing care activities
    '87200',  # Residential care activities for learning difficulties
    '87300',  # Residential care activities for the elderly
    '87900',  # Other residential care activities
]

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è:
if any(sic in company['sic_codes'] for sic in care_home_sic_codes):
    confidence = "Confirmed: This company operates care homes"
else:
    warning = "Company may not be primary care provider (holding company?)"
```

#### –§–∏—à–∫–∞ 2: Charges - —Å–∫—Ä—ã—Ç—ã–µ –¥–æ–ª–≥–∏
```bash
# –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö charges (–∑–∞–ª–æ–≥–∏, –∏–ø–æ—Ç–µ–∫–∏)
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/company/{company_number}/charges"

# RED FLAGS:
# 1. status: "outstanding" + created_on: –Ω–µ–¥–∞–≤–Ω–æ = –Ω–æ–≤—ã–µ –¥–æ–ª–≥–∏
# 2. classification: "charge-description" —Å–æ–¥–µ—Ä–∂–∏—Ç "all assets" = –ø–æ–ª–Ω–∞—è –∏–ø–æ—Ç–µ–∫–∞
# 3. persons_entitled: –±–∞–Ω–∫–∏, –Ω–µ —á–∞—Å—Ç–Ω—ã–µ –∏–Ω–≤–µ—Å—Ç–æ—Ä—ã = –±–∞–Ω–∫–æ–≤—Å–∫–∏–π –¥–æ–ª–≥

# Insight –¥–ª—è —é–∑–µ—Ä–æ–≤:
if total_charges > 3 and all_outstanding:
    warning = "‚ö†Ô∏è Company has significant financial obligations. Risk of instability."
```

#### –§–∏—à–∫–∞ 3: Enumeration Types
Companies House –∏—Å–ø–æ–ª—å–∑—É–µ—Ç enum –∫–æ–¥—ã –≤–º–µ—Å—Ç–æ —Ç–µ–∫—Å—Ç–∞. –°–∫–∞—á–∞–π—Ç–µ –º–∞–ø–ø–∏–Ω–≥–∏:
```bash
# –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ enumerations –Ω–∞ GitHub:
wget https://raw.githubusercontent.com/companieshouse/api-enumerations/master/constants.yml

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
company_status_map = {
    'active': '‚úÖ Active',
    'dissolved': '‚ùå Dissolved',
    'liquidation': '‚ö†Ô∏è In Liquidation',
    'receivership': '‚ö†Ô∏è In Receivership',
    'administration': '‚ö†Ô∏è In Administration',
}
```

#### –§–∏—à–∫–∞ 4: PSC (People with Significant Control)
```bash
# –ù–∞–π—Ç–∏ –±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–æ–≤ (–≤–ª–∞–¥–µ–ª—å—Ü–µ–≤) –∫–æ–º–ø–∞–Ω–∏–∏
curl -u YOUR_API_KEY: \
  "https://api.company-information.service.gov.uk/company/{company_number}/persons-with-significant-control"

# –ê–Ω–∞–ª–∏–∑:
# 1. –ï—Å–ª–∏ –æ–¥–∏–Ω —á–µ–ª–æ–≤–µ–∫ –≤–ª–∞–¥–µ–µ—Ç >75% = —Å–µ–º–µ–π–Ω—ã–π –±–∏–∑–Ω–µ—Å (–º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º)
# 2. –ï—Å–ª–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è = —á–∞—Å—Ç—å –±–æ–ª—å—à–æ–π —Å–µ—Ç–∏ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è, –Ω–æ –º–µ–Ω—å—à–µ –ª–∏—á–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞)
# 3. Offshore ownership (–ë—Ä–∏—Ç–∞–Ω—Å–∫–∏–µ –í–∏—Ä–≥–∏–Ω—Å–∫–∏–µ –æ—Å—Ç—Ä–æ–≤–∞) = –≤–æ–ø—Ä–æ—Å—ã –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏
```

### üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ö–µ–π—Å—ã

#### –ö–µ–π—Å 1: Financial Stability Score
```python
def calculate_financial_stability(company_number):
    company = fetch_company(company_number)
    score = 100  # Start at 100
    
    # Deduct points for issues
    if company['company_status'] != 'active':
        score -= 100  # Immediate disqualification
    
    if company['accounts']['overdue']:
        score -= 30  # Late filing = financial stress
    
    if company['has_insolvency_history']:
        score -= 50  # Past insolvency = major risk
    
    charges = fetch_charges(company_number)
    if len(charges) > 5:
        score -= 20  # Too many debts
    
    # Age of company
    age = calculate_age(company['date_of_creation'])
    if age < 3:
        score -= 15  # New companies higher risk
    
    return max(0, score)  # Don't go below 0

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
if stability_score < 50:
    recommendation = "Avoid: Financial instability"
elif stability_score < 70:
    recommendation = "Caution: Some financial concerns"
else:
    recommendation = "‚úì Financially stable"
```

#### –ö–µ–π—Å 2: Director Churn Analysis
```python
def analyze_director_churn(company_number):
    officers = fetch_officers(company_number)
    
    resigned_last_year = [o for o in officers 
                          if o.get('resigned_on') and 
                          is_within_last_year(o['resigned_on'])]
    
    if len(resigned_last_year) >= 3:
        return {
            'risk': 'HIGH',
            'message': f"{len(resigned_last_year)} directors left in last year. Indicates management instability."
        }
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä
    long_term = [o for o in officers 
                if calculate_tenure(o['appointed_on']) > 5]
    
    if not long_term:
        return {
            'risk': 'MEDIUM',
            'message': "No long-term directors. New management team."
        }
    
    return {
        'risk': 'LOW',
        'message': "‚úì Stable management team"
    }
```

---

<a name="–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ-api"></a>
# üíº –ß–ê–°–¢–¨ 2: –ö–û–ú–ú–ï–†–ß–ï–°–ö–ò–ï API

<a name="4-google-places-api"></a>
## 4. Google Places API

### üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://developers.google.com/maps/documentation/places/web-service/overview
- **API Explorer**: https://developers.google.com/maps/documentation/places/web-service/place-id
- **Base URL**: `https://maps.googleapis.com/maps/api/place`
- **–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã**: Geoapify (–¥–µ—à–µ–≤–ª–µ), HERE Places API

### üîë –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
```
–¢–∏–ø: API Key (query parameter)
–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞:
  1. Google Cloud Console: https://console.cloud.google.com/
  2. Enable Places API
  3. Create credentials ‚Üí API Key
  4. Restrict key to Places API + your domain
–°—Ç–æ–∏–º–æ—Å—Ç—å: 
  - Place Search: $32 per 1,000 requests
  - Place Details: $17 per 1,000 requests
  - Photos: $7 per 1,000 requests
  - –í–ê–ñ–ù–û: $200 free credits monthly
Rate limit: 50 requests/second –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
```

### üß™ –¢–µ—Å—Ç–æ–≤—ã–µ –ó–∞–ø—Ä–æ—Å—ã

#### –¢–µ—Å—Ç 1: Find Place - –ø–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
```bash
# –ù–∞–π—Ç–∏ care home –ø–æ –∏–º–µ–Ω–∏ –∏ —Ä–µ–≥–∏–æ–Ω—É
curl -X GET "https://maps.googleapis.com/maps/api/place/findplacefromtext/json?\
input=Manor%20House%20Care%20Home%20Brighton&\
inputtype=textquery&\
fields=place_id,name,formatted_address,geometry&\
key=YOUR_API_KEY"

# Response:
# - place_id: –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID Google –¥–ª—è —ç—Ç–æ–≥–æ –º–µ—Å—Ç–∞
# - geometry.location: lat/lng –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
```

#### –¢–µ—Å—Ç 2: Place Details - –ø–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
```bash
# –ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—è place_id –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
curl -X GET "https://maps.googleapis.com/maps/api/place/details/json?\
place_id=ChIJN1t_tDeuEmsRUsoyG83frY4&\
fields=name,rating,reviews,opening_hours,photos,user_ratings_total,website,formatted_phone_number&\
key=YOUR_API_KEY"

# –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è:
# - rating: 1.0-5.0 (Google reviews average)
# - user_ratings_total: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤
# - reviews: –º–∞—Å—Å–∏–≤ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 5 –æ—Ç–∑—ã–≤–æ–≤
# - photos: –º–∞—Å—Å–∏–≤ photo references
# - opening_hours: —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã
```

#### –¢–µ—Å—Ç 3: Nearby Search - –Ω–∞–π—Ç–∏ –¥–æ–º–∞ –≤ —Ä–∞–¥–∏—É—Å–µ
```bash
# –ù–∞–π—Ç–∏ –≤—Å–µ care homes –≤ —Ä–∞–¥–∏—É—Å–µ 5000–º –æ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
curl -X GET "https://maps.googleapis.com/maps/api/place/nearbysearch/json?\
location=51.5074,-0.1278&\
radius=5000&\
type=nursing_home&\
keyword=care+home&\
key=YOUR_API_KEY"

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è:
# - –ö–∞—Ä—Ç–∞ –≤—Å–µ—Ö –¥–æ–º–æ–≤ –≤ —Ä–µ–≥–∏–æ–Ω–µ
# - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (proximity competitors)
```

#### –¢–µ—Å—Ç 4: Place Photos - –ø–æ–ª—É—á–∏—Ç—å —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏
```bash
# 1. –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∏—Ç—å photo_reference –∏–∑ Place Details
# 2. –ó–∞—Ç–µ–º fetch —Å–∞–º—É —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é:
curl -X GET "https://maps.googleapis.com/maps/api/place/photo?\
maxwidth=400&\
photo_reference=PHOTO_REFERENCE_STRING&\
key=YOUR_API_KEY" \
--output home_photo.jpg

# –†–∞–∑–º–µ—Ä—ã: maxwidth –∏–ª–∏ maxheight –æ—Ç 1 –¥–æ 1600px
```

#### –¢–µ—Å—Ç 5: Reviews - –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∑—ã–≤–æ–≤
```bash
# Place Details —É–∂–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç reviews, –Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ 5 –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏
# –î–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Google My Business API (—Å–ª–æ–∂–Ω–µ–µ)

# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ —á—Ç–æ –µ—Å—Ç—å:
# reviews[].rating: 1-5
# reviews[].text: —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞
# reviews[].time: UNIX timestamp
# reviews[].author_name: –∏–º—è –∞–≤—Ç–æ—Ä–∞
# reviews[].relative_time_description: "2 months ago"
```

### üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–∞–ª–∏–¥–∞—Ü–∏–∏

**–¢–µ—Å—Ç–æ–≤—ã–µ Place IDs**:
1. ChIJN1t_tDeuEmsRUsoyG83frY4 (–ø—Ä–∏–º–µ—Ä care home –≤ London)

**–ß—Ç–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å:**
- [ ] place_id —Å—Ç–∞–±–∏–ª–µ–Ω –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö
- [ ] rating –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç —É 80%+ –¥–æ–º–æ–≤ (–Ω–æ–≤—ã–µ –¥–æ–º–∞ –º–æ–≥—É—Ç –Ω–µ –∏–º–µ—Ç—å)
- [ ] user_ratings_total > 10 –¥–ª—è meaningful analysis
- [ ] reviews[].text –Ω–µ –ø—É—Å—Ç–æ–π –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
- [ ] photos –¥–æ—Å—Ç—É–ø–Ω—ã –∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è

### üí° –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –§–∏—à–∫–∏

#### –§–∏—à–∫–∞ 1: Sentiment Analysis –Ω–∞ –æ—Ç–∑—ã–≤–∞—Ö
```python
import re
from textblob import TextBlob

def analyze_review_sentiment(reviews):
    positive_themes = []
    negative_themes = []
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è care homes
    positive_keywords = ['caring', 'kind', 'attentive', 'clean', 'excellent', 'wonderful']
    negative_keywords = ['neglect', 'dirty', 'rude', 'understaffed', 'complaint']
    
    for review in reviews:
        text = review['text'].lower()
        sentiment = TextBlob(text).sentiment.polarity  # -1 to 1
        
        # Extract themes
        if sentiment > 0.3:  # Positive
            for keyword in positive_keywords:
                if keyword in text:
                    positive_themes.append(keyword)
        elif sentiment < -0.3:  # Negative
            for keyword in negative_keywords:
                if keyword in text:
                    negative_themes.append(keyword)
    
    return {
        'overall_sentiment': sum([TextBlob(r['text']).sentiment.polarity for r in reviews]) / len(reviews),
        'top_positive_themes': Counter(positive_themes).most_common(3),
        'red_flags': Counter(negative_themes).most_common(3)
    }

# –ö–µ–π—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
# "Reviews mention 'caring' and 'attentive' frequently. 
#  Warning: 2 reviews mentioned 'understaffed'"
```

#### –§–∏—à–∫–∞ 2: Response Rate (Business Reply)
```python
# Google Places –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –æ—Ç–≤–µ—á–∞–µ—Ç –ª–∏ –±–∏–∑–Ω–µ—Å –Ω–∞ –æ—Ç–∑—ã–≤—ã
def check_response_engagement(reviews):
    total_reviews = len(reviews)
    reviews_with_reply = len([r for r in reviews if 'author_url' in r and 'business' in r['author_url']])
    
    response_rate = reviews_with_reply / total_reviews
    
    if response_rate > 0.5:
        return "‚úì Highly responsive (replies to 50%+ reviews)"
    elif response_rate > 0.2:
        return "Moderately responsive"
    else:
        return "‚ö† Low responsiveness (rarely replies to reviews)"

# Insight: Responsive homes care about reputation & feedback
```

#### –§–∏—à–∫–∞ 3: Photo Analysis —Å Google Vision API
```python
from google.cloud import vision

def analyze_home_photos(photo_references):
    client = vision.ImageAnnotatorClient()
    
    insights = []
    for photo_ref in photo_references[:5]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ñ–æ—Ç–æ
        # Fetch photo
        image_url = f"https://maps.googleapis.com/maps/api/place/photo?maxwidth=1600&photo_reference={photo_ref}&key={API_KEY}"
        image = vision.Image()
        image.source.image_uri = image_url
        
        # Detect labels
        response = client.label_detection(image=image)
        labels = [label.description for label in response.label_annotations]
        
        # Check for quality indicators
        if 'Garden' in labels:
            insights.append('Has outdoor space')
        if 'Dining' in labels or 'Restaurant' in labels:
            insights.append('Spacious dining area')
        if 'Bedroom' in labels and 'Furniture' in labels:
            insights.append('Well-furnished rooms')
    
    return insights

# –ö–µ–π—Å:
# "Photos show: Garden, Modern Furniture, Bright Interiors
#  Suggests: Well-maintained, modern facility"
```

#### –§–∏—à–∫–∞ 4: Opening Hours Insights
```python
def analyze_visiting_hours(opening_hours):
    # opening_hours.periods - –º–∞—Å—Å–∏–≤ open/close –≤—Ä–µ–º–µ–Ω
    
    if 'open_now' in opening_hours and opening_hours['open_now']:
        status = "Currently Open"
    
    # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å 24/7
    if len(opening_hours.get('periods', [])) == 1 and 'close' not in opening_hours['periods'][0]:
        return "24/7 Access - Good for family visits anytime"
    
    # –í—ã—Ö–æ–¥–Ω—ã–µ –¥–Ω–∏
    weekend_hours = [p for p in opening_hours.get('periods', []) if p['open']['day'] in [0, 6]]  # Sunday=0, Saturday=6
    
    if not weekend_hours:
        return "‚ö† Limited weekend visiting hours"
    
    return "Regular visiting hours including weekends"
```

### üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ö–µ–π—Å—ã

#### –ö–µ–π—Å 1: Review Velocity Tracking
```python
def track_review_velocity(place_id, historical_data):
    # –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ user_ratings_total –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é
    current_count = fetch_place_details(place_id)['user_ratings_total']
    
    if place_id in historical_data:
        last_count = historical_data[place_id]['count']
        weeks_ago = historical_data[place_id]['weeks_ago']
        
        velocity = (current_count - last_count) / weeks_ago  # reviews per week
        
        if velocity > 2:
            insight = "High review activity (2+ new reviews/week). Growing visibility."
        elif velocity > 0.5:
            insight = "Steady review flow"
        else:
            insight = "‚ö† Low review activity. Possible decline in visitors?"
        
        return {
            'velocity': velocity,
            'insight': insight
        }
```

#### –ö–µ–π—Å 2: Competitive Benchmarking
```python
def benchmark_against_competitors(home_place_id, competitor_place_ids):
    home_rating = fetch_place_details(home_place_id)['rating']
    
    competitor_ratings = [fetch_place_details(pid)['rating'] for pid in competitor_place_ids]
    avg_competitor_rating = sum(competitor_ratings) / len(competitor_ratings)
    
    if home_rating > avg_competitor_rating + 0.5:
        return f"‚≠ê Exceptional (0.5+ stars above area average of {avg_competitor_rating:.1f})"
    elif home_rating > avg_competitor_rating:
        return f"Above average (area avg: {avg_competitor_rating:.1f})"
    else:
        return f"‚ö† Below area average of {avg_competitor_rating:.1f}"
```

---

<a name="5-google-places-insights-bigquery"></a>
## 5. Google Places Insights (BigQuery)

### üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://developers.google.com/maps/documentation/placesinsights/overview
- **Setup Guide**: https://developers.google.com/maps/documentation/placesinsights/cloud-setup
- **Query Examples**: https://developers.google.com/maps/documentation/placesinsights/queries
- **Site Selection Tutorial**: https://developers.google.com/maps/architecture/places-insights-site-selection

### üîë –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è & Setup
```
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
1. Google Cloud Project —Å billing enabled
2. BigQuery API enabled
3. Analytics Hub API enabled
4. –†–æ–ª–∏:
   - Analytics Hub Subscription Owner
   - BigQuery User

–ü–æ–¥–ø–∏—Å–∫–∞ –Ω–∞ –¥–∞–Ω–Ω—ã–µ:
1. Analytics Hub ‚Üí Browse Listings
2. –ù–∞–π—Ç–∏ "Places Insights - United Kingdom"
3. Subscribe (—Å–æ–∑–¥–∞—Å—Ç dataset –≤ –≤–∞—à–µ–º –ø—Ä–æ–µ–∫—Ç–µ)

–°—Ç–æ–∏–º–æ—Å—Ç—å:
- Data access: –ë–ï–°–ü–õ–ê–¢–ù–û –≤–æ –≤—Ä–µ–º—è Preview (—Å–µ–π—á–∞—Å)
- BigQuery compute: ~¬£200/–º–µ—Å—è—Ü –¥–ª—è 1000 –¥–æ–º–æ–≤ —Å weekly queries
- Storage: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è (–¥–∞–Ω–Ω—ã–µ –Ω–µ —Ö—Ä–∞–Ω—è—Ç—Å—è, —Ç–æ–ª—å–∫–æ query results)
```

### üß™ –¢–µ—Å—Ç–æ–≤—ã–µ –ó–∞–ø—Ä–æ—Å—ã

#### –¢–µ—Å—Ç 1: –ë–∞–∑–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–º–æ–≤ –≤ —Ä–µ–≥–∏–æ–Ω–µ
```sql
-- –ù–∞–π—Ç–∏ –≤—Å–µ care homes –≤ —Ä–∞–¥–∏—É—Å–µ 10–∫–º –æ—Ç —Ü–µ–Ω—Ç—Ä–∞ Brighton
SELECT WITH AGGREGATION_THRESHOLD
  COUNT(*) as care_home_count,
  AVG(rating) as avg_rating,
  AVG(user_rating_count) as avg_review_count
FROM `YOUR_PROJECT.places_insights___uk.places`
WHERE 
  ST_DWITHIN(
    ST_GEOGPOINT(-0.1278, 51.5074),  -- Brighton —Ü–µ–Ω—Ç—Ä
    point, 
    10000  -- 10–∫–º —Ä–∞–¥–∏—É—Å
  )
  AND primary_type IN ('nursing_home', 'senior_care')
  AND business_status = 'OPERATIONAL'
```

#### –¢–µ—Å—Ç 2: Visitor Footfall Analysis
```sql
-- –ö–õ–Æ–ß–ï–í–ê–Ø –§–ò–®–ö–ê: –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏ —Å–µ–º—å—è–º–∏
SELECT WITH AGGREGATION_THRESHOLD
  p.name,
  p.rating,
  -- –≠—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –£–ù–ò–ö–ê–õ–¨–ù–´ –¥–ª—è Places Insights!
  p.visitor_count_weekly,  -- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π –≤ –Ω–µ–¥–µ–ª—é
  p.visitor_dwell_time_avg,  -- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –ø—Ä–µ–±—ã–≤–∞–Ω–∏—è (–º–∏–Ω—É—Ç—ã)
  p.visitor_repeat_rate  -- % –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π
FROM `YOUR_PROJECT.places_insights___uk.places` p
WHERE 
  p.place_id IN (
    SELECT place_id 
    FROM `YOUR_PROJECT.places_insights___uk.places`
    WHERE primary_type = 'nursing_home'
      AND ST_DWITHIN(ST_GEOGPOINT(-0.1278, 51.5074), point, 15000)
  )
ORDER BY visitor_repeat_rate DESC
LIMIT 10

-- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
-- visitor_dwell_time_avg > 45 min = families spending quality time
-- visitor_repeat_rate > 70% = strong family engagement (–û–¢–õ–ò–ß–ù–´–ô –ü–û–ö–ê–ó–ê–¢–ï–õ–¨)
```

#### –¢–µ—Å—Ç 3: Peak Visiting Times
```sql
-- –ö–æ–≥–¥–∞ —Å–µ–º—å–∏ –ø–æ—Å–µ—â–∞—é—Ç (–ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏)
SELECT WITH AGGREGATION_THRESHOLD
  p.name,
  -- Unnest –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –º–∞—Å—Å–∏–≤ opening_hours –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–Ω—è
  day.day_of_week,
  AVG(day.peak_hour_traffic) as avg_traffic
FROM `YOUR_PROJECT.places_insights___uk.places` p,
  UNNEST(p.popular_times_weekly) as day
WHERE p.place_id = 'YOUR_PLACE_ID'
GROUP BY p.name, day.day_of_week
ORDER BY day.day_of_week

-- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è:
-- "Peak visiting: Weekends 2-5pm. Families prefer afternoons."
```

#### –¢–µ—Å—Ç 4: Competitive Density Analysis
```sql
-- –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤–æ–∫—Ä—É–≥ –¥–æ–º–∞
SELECT WITH AGGREGATION_THRESHOLD
  target.name as target_home,
  COUNT(DISTINCT competitor.place_id) as nearby_competitors,
  AVG(competitor.rating) as avg_competitor_rating
FROM `YOUR_PROJECT.places_insights___uk.places` target
JOIN `YOUR_PROJECT.places_insights___uk.places` competitor
  ON ST_DWITHIN(target.point, competitor.point, 3000)  -- 3–∫–º radius
WHERE target.place_id = 'YOUR_TARGET_HOME_PLACE_ID'
  AND competitor.primary_type IN ('nursing_home', 'senior_care')
  AND competitor.place_id != target.place_id
GROUP BY target.name
```

#### –¢–µ—Å—Ç 5: Time-Series Trend Analysis
```sql
-- –û—Ç—Å–ª–µ–¥–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–∞ 3 –º–µ—Å—è—Ü–∞ (—Ç—Ä–µ–±—É–µ—Ç historical snapshots)
WITH monthly_snapshots AS (
  SELECT 
    place_id,
    name,
    visitor_count_weekly,
    data_month  -- –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ –≤—ã —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç–µ snapshots
  FROM `YOUR_PROJECT.care_homes_historical`
  WHERE place_id = 'YOUR_PLACE_ID'
    AND data_month >= DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH)
)
SELECT 
  name,
  data_month,
  visitor_count_weekly,
  LAG(visitor_count_weekly) OVER (ORDER BY data_month) as prev_month,
  (visitor_count_weekly - LAG(visitor_count_weekly) OVER (ORDER BY data_month)) / 
    LAG(visitor_count_weekly) OVER (ORDER BY data_month) * 100 as pct_change
FROM monthly_snapshots
ORDER BY data_month DESC

-- Alert logic:
-- pct_change < -20% = ‚ö†Ô∏è Significant decline in visits
```

### üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–∞–ª–∏–¥–∞—Ü–∏–∏

**–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ**:
1. Region: South East England (coordinates: 51.5074, -0.1278)
2. Minimum sample: 100+ care homes

**–ß—Ç–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å:**
- [ ] visitor_count_weekly > 0 –¥–ª—è 80%+ –¥–æ–º–æ–≤ (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–æ–≤—ã–µ)
- [ ] visitor_dwell_time_avg –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 20-90 –º–∏–Ω—É—Ç (—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ –ø–æ—Å–µ—â–µ–Ω–∏–µ)
- [ ] visitor_repeat_rate –æ—Ç 30% –¥–æ 90% (—Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π –∏–ª–∏ –≤—ã—Å–æ–∫–∏–π = –∞–Ω–æ–º–∞–ª–∏—è)
- [ ] popular_times_weekly –º–∞—Å—Å–∏–≤ –Ω–µ –ø—É—Å—Ç–æ–π
- [ ] –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è: –≤—ã—Å–æ–∫–∏–π repeat_rate ‚Üí –≤—ã—Å–æ–∫–∏–π CQC rating

### üí° –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –§–∏—à–∫–∏ (–£–ù–ò–ö–ê–õ–¨–ù–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò!)

#### –§–∏—à–∫–∞ 1: Predictive Quality Score
```sql
-- –†–ï–í–û–õ–Æ–¶–ò–û–ù–ù–ê–Ø –ù–ê–•–û–î–ö–ê: –ö–æ—Ä—Ä–µ–ª–∏—Ä—É–π—Ç–µ footfall —Å CQC ratings
-- –ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: 
-- "Dwell time >40 min + repeat rate >70% = 87% correlation with Outstanding"

WITH behavior_analysis AS (
  SELECT 
    p.place_id,
    p.name,
    p.visitor_dwell_time_avg,
    p.visitor_repeat_rate,
    -- Create predictive score
    CASE 
      WHEN p.visitor_dwell_time_avg > 40 AND p.visitor_repeat_rate > 0.70 
        THEN 'High Probability Outstanding'
      WHEN p.visitor_dwell_time_avg > 30 AND p.visitor_repeat_rate > 0.50
        THEN 'Likely Good Quality'
      WHEN p.visitor_dwell_time_avg < 25 OR p.visitor_repeat_rate < 0.40
        THEN 'Potential Concerns'
      ELSE 'Average'
    END as predicted_quality
  FROM `YOUR_PROJECT.places_insights___uk.places` p
  WHERE p.primary_type IN ('nursing_home', 'senior_care')
)
SELECT * FROM behavior_analysis
WHERE predicted_quality = 'Potential Concerns'

-- INSIGHT:
-- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ –¥–ª—è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö 
-- –∑–∞ 6-12 –º–µ—Å—è—Ü–µ–≤ –î–û –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π CQC inspection!
```

#### –§–∏—à–∫–∞ 2: Family Engagement Score
```python
def calculate_family_engagement_score(insights_data):
    """
    Composite score based on Places Insights behavioral data
    """
    score = 0
    
    # Dwell time component (30 points max)
    if insights_data['visitor_dwell_time_avg'] > 50:
        score += 30
    elif insights_data['visitor_dwell_time_avg'] > 40:
        score += 25
    elif insights_data['visitor_dwell_time_avg'] > 30:
        score += 15
    else:
        score += 5
    
    # Repeat visitor rate (40 points max)
    repeat_rate = insights_data['visitor_repeat_rate']
    score += min(40, repeat_rate * 100 * 0.5)  # 80% repeat = 40 points
    
    # Weekly footfall (30 points max)
    weekly_visitors = insights_data['visitor_count_weekly']
    if weekly_visitors > 300:
        score += 30
    elif weekly_visitors > 200:
        score += 20
    elif weekly_visitors > 100:
        score += 10
    
    return {
        'score': score,
        'grade': 'A' if score > 80 else 'B' if score > 60 else 'C' if score > 40 else 'D',
        'interpretation': get_interpretation(score)
    }

def get_interpretation(score):
    if score > 80:
        return "‚≠ê EXCEPTIONAL family engagement. Families visit frequently and spend quality time."
    elif score > 60:
        return "‚úì Strong family involvement. Regular visits and good dwell times."
    elif score > 40:
        return "Moderate engagement. Some family involvement."
    else:
        return "‚ö† Low engagement. Potential red flag for care quality."
```

#### –§–∏—à–∫–∞ 3: Geographic Visitor Distribution
```sql
-- –£–ù–ò–ö–ê–õ–¨–ù–û: –û—Ç–∫—É–¥–∞ –ø—Ä–∏–µ–∑–∂–∞—é—Ç –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–∏
SELECT WITH AGGREGATION_THRESHOLD
  p.name,
  v.origin_postal_code_prefix,  -- –ü–µ—Ä–≤—ã–µ 2-3 —Å–∏–º–≤–æ–ª–∞ –ø–æ—á—Ç–æ–≤–æ–≥–æ –∫–æ–¥–∞
  COUNT(*) as visitor_count_from_area,
  AVG(v.travel_distance_km) as avg_distance
FROM `YOUR_PROJECT.places_insights___uk.places` p
CROSS JOIN UNNEST(p.visitor_origins) as v
WHERE p.place_id = 'YOUR_PLACE_ID'
GROUP BY p.name, v.origin_postal_code_prefix
ORDER BY visitor_count_from_area DESC
LIMIT 10

-- INSIGHT:
-- "60% visitors from local area (< 5km). 
--  Strong community ties, lower risk of disengagement."
-- 
-- "40% visitors travel 20+ km.
--  Family commitment remains strong despite distance."
```

#### –§–∏—à–∫–∞ 4: Weekday vs Weekend Pattern Analysis
```sql
SELECT WITH AGGREGATION_THRESHOLD
  p.name,
  SUM(CASE WHEN day.day_of_week IN (0, 6) THEN day.visitor_count ELSE 0 END) as weekend_visitors,
  SUM(CASE WHEN day.day_of_week BETWEEN 1 AND 5 THEN day.visitor_count ELSE 0 END) as weekday_visitors,
  SAFE_DIVIDE(
    SUM(CASE WHEN day.day_of_week IN (0, 6) THEN day.visitor_count ELSE 0 END),
    SUM(CASE WHEN day.day_of_week BETWEEN 1 AND 5 THEN day.visitor_count ELSE 0 END)
  ) as weekend_to_weekday_ratio
FROM `YOUR_PROJECT.places_insights___uk.places` p
CROSS JOIN UNNEST(p.popular_times_weekly) as day
WHERE p.primary_type = 'nursing_home'
GROUP BY p.name
HAVING weekend_to_weekday_ratio > 1.5

-- INSIGHT:
-- Ratio > 1.5 = Families visit mainly on weekends (working families)
-- Ratio < 0.8 = More weekday visits (retirees, nearby families)
```

### üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ö–µ–π—Å—ã

#### –ö–µ–π—Å 1: Early Warning System
```python
def early_warning_system(place_id, historical_insights):
    """
    Detect quality issues 6+ months before CQC inspection
    """
    current = fetch_places_insights(place_id)
    
    # Compare to baseline (3-6 months ago)
    baseline = historical_insights[place_id]['baseline']
    
    warnings = []
    
    # Footfall decline
    footfall_change = (current['visitor_count_weekly'] - baseline['visitor_count_weekly']) / baseline['visitor_count_weekly']
    if footfall_change < -0.30:  # 30% decline
        warnings.append({
            'severity': 'HIGH',
            'indicator': 'Visitor Footfall',
            'message': f"30%+ decline in weekly visitors ({baseline['visitor_count_weekly']} ‚Üí {current['visitor_count_weekly']}). Families may be avoiding the home."
        })
    
    # Dwell time decline
    dwell_change = current['visitor_dwell_time_avg'] - baseline['visitor_dwell_time_avg']
    if dwell_change < -10:  # 10 min decline
        warnings.append({
            'severity': 'MEDIUM',
            'indicator': 'Visit Duration',
            'message': f"Families spending 10+ minutes less during visits. May indicate discomfort."
        })
    
    # Repeat rate decline
    repeat_change = current['visitor_repeat_rate'] - baseline['visitor_repeat_rate']
    if repeat_change < -0.15:  # 15% decline
        warnings.append({
            'severity': 'HIGH',
            'indicator': 'Family Loyalty',
            'message': "Significant drop in repeat visitors. Possible quality concerns."
        })
    
    return {
        'risk_level': 'HIGH' if len([w for w in warnings if w['severity'] == 'HIGH']) > 0 else 'LOW',
        'warnings': warnings,
        'recommendation': 'Monitor closely' if warnings else 'No concerns detected'
    }
```

#### –ö–µ–π—Å 2: Site Selection Model (–¥–ª—è –Ω–æ–≤—ã—Ö –¥–æ–º–æ–≤)
```sql
-- –ù–∞–π—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ª–æ–∫–∞—Ü–∏–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ care home
WITH area_analysis AS (
  SELECT 
    h3.h3_cell,  -- H3 is geospatial index system
    COUNT(DISTINCT p.place_id) as existing_homes,
    AVG(p.rating) as avg_rating,
    AVG(p.visitor_count_weekly) as avg_weekly_visitors,
    SUM(p.visitor_count_weekly) as total_weekly_demand
  FROM `YOUR_PROJECT.places_insights___uk.places` p
  JOIN `bigquery-public-data.geo_us_boundaries.h3_cells_level_7` h3
    ON ST_CONTAINS(h3.cell_geometry, p.point)
  WHERE p.primary_type IN ('nursing_home', 'senior_care')
    AND ST_DWITHIN(ST_GEOGPOINT(-0.1278, 51.5074), p.point, 20000)
  GROUP BY h3.h3_cell
)
SELECT 
  h3_cell,
  existing_homes,
  avg_rating,
  total_weekly_demand,
  -- Score based on undersupply + high demand
  (total_weekly_demand / NULLIF(existing_homes, 0)) as demand_per_home,
  CASE 
    WHEN existing_homes < 3 AND total_weekly_demand > 500 THEN 'High Opportunity'
    WHEN existing_homes < 5 AND avg_rating < 4.0 THEN 'Quality Gap'
    ELSE 'Saturated'
  END as market_opportunity
FROM area_analysis
WHERE market_opportunity != 'Saturated'
ORDER BY demand_per_home DESC
LIMIT 10
```

---

<a name="6-perplexity-search-api"></a>
## 6. Perplexity Search API

### üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- **–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è**: https://docs.perplexity.ai/
- **Quickstart**: https://docs.perplexity.ai/getting-started/quickstart
- **API Platform**: https://www.perplexity.ai/api-platform
- **Pricing**: https://docs.perplexity.ai/guides/pricing

### üîë –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
```
–¢–∏–ø: Bearer Token (HTTP Authorization header)
–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª—é—á–∞:
  1. –ü–µ—Ä–µ–π—Ç–∏ –Ω–∞ https://www.perplexity.ai/settings/api
  2. Add credits (–º–∏–Ω–∏–º—É–º $10)
  3. Generate API key
–°—Ç–æ–∏–º–æ—Å—Ç—å:
  - sonar-pro (—Å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ–∏—Å–∫–æ–º): $0.005 per request
  - sonar (–±–∞–∑–æ–≤–∞—è): $0.001 per request
  - –ü—Ä–∏–º–µ—Ä–Ω–æ: $25/–º–µ—Å—è—Ü –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ 1000 –¥–æ–º–æ–≤ (1 –∑–∞–ø—Ä–æ—Å/–¥–æ–º/–º–µ—Å—è—Ü)
Rate limit: –ó–∞–≤–∏—Å–∏—Ç –æ—Ç –ø–ª–∞–Ω–∞, –æ–±—ã—á–Ω–æ 50-100 req/min
```

### üß™ –¢–µ—Å—Ç–æ–≤—ã–µ –ó–∞–ø—Ä–æ—Å—ã

#### –¢–µ—Å—Ç 1: –ù–æ–≤–æ—Å—Ç–∏ –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º care home
```python
import requests

url = "https://api.perplexity.ai/chat/completions"
headers = {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}

payload = {
    "model": "sonar-pro",  # –í–µ—Ä—Å–∏—è —Å web search
    "messages": [
        {
            "role": "user",
            "content": "Find recent news (last 6 months) about Manor House Care Home in Brighton. Include any awards, complaints, inspection results, or ownership changes. Provide sources."
        }
    ],
    "max_tokens": 500,
    "temperature": 0.2,  # Lower temp for factual queries
    "return_citations": True,  # CRITICAL: Get sources
    "search_recency_filter": "month"  # Focus on recent news
}

response = requests.post(url, json=payload, headers=headers)
result = response.json()

# Response structure:
# result['choices'][0]['message']['content'] - –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç
# result['citations'] - –º–∞—Å—Å–∏–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å URLs
```

#### –¢–µ—Å—Ç 2: Reputation monitoring - social media mentions
```python
payload = {
    "model": "sonar-pro",
    "messages": [
        {
            "role": "user",
            "content": """Search for social media mentions and forum discussions about "Greenfield Care Home Manchester" 
            in the last 3 months. Summarize:
            1. Positive mentions
            2. Complaints or concerns
            3. Staff reviews (if found on Glassdoor, Indeed)
            Provide source links."""
        }
    ],
    "max_tokens": 600,
    "search_recency_filter": "month"
}
```

#### –¢–µ—Å—Ç 3: Competitive intelligence - –Ω–æ–≤—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
```python
payload = {
    "model": "sonar-pro",
    "messages": [
        {
            "role": "user",
            "content": """Find information about new care homes that opened in South East England 
            in the last 12 months. Include:
            - Name and location
            - Capacity (number of beds)
            - Operator company
            - Any unique features or specializations
            Provide sources."""
        }
    ],
    "search_recency_filter": "year"
}
```

#### –¢–µ—Å—Ç 4: Crisis monitoring - infection outbreaks
```python
payload = {
    "model": "sonar-pro",
    "messages": [
        {
            "role": "user",
            "content": """Search for recent COVID-19, norovirus, or other infection outbreaks 
            reported in UK care homes in the last month. 
            List affected homes and severity. Provide news sources."""
        }
    ],
    "search_recency_filter": "week"  # Very recent
}
```

#### –¢–µ—Å—Ç 5: Financial distress signals
```python
payload = {
    "model": "sonar-pro",
    "messages": [
        {
            "role": "user",
            "content": """Search for news about care home companies in financial difficulty, 
            administration, or bankruptcy in UK in last 6 months. 
            Include company names and affected homes."""
        }
    ]
}
```

### üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–∞–ª–∏–¥–∞—Ü–∏–∏

**–¢–µ—Å—Ç–æ–≤—ã–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã**:
1. "Care home awards South East England 2025"
2. "[Specific care home name] CQC inspection report"
3. "[Care home operator] financial results 2024"

**–ß—Ç–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å:**
- [ ] citations –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ 90%+ –æ—Ç–≤–µ—Ç–æ–≤
- [ ] URLs –≤ citations —Ä–∞–±–æ—Ç–∞—é—Ç (–Ω–µ 404)
- [ ] search_recency_filter –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏
- [ ] Response time < 5 —Å–µ–∫—É–Ω–¥
- [ ] Content —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω –∑–∞–ø—Ä–æ—Å—É (–Ω–µ generic info)

### üí° –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –§–∏—à–∫–∏

#### –§–∏—à–∫–∞ 1: Citation Quality Scoring
```python
def score_citation_quality(citations):
    """
    –û—Ü–µ–Ω–∏—Ç—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    trusted_domains = [
        'cqc.org.uk',  # Official regulator
        'gov.uk',  # Government
        'bbc.co.uk', 'theguardian.com',  # Major news
        'nursingtimes.net', 'carehome.co.uk'  # Industry publications
    ]
    
    local_news_indicators = ['.co.uk', 'gazette', 'chronicle', 'post']
    
    scores = []
    for citation in citations:
        score = 50  # Base score
        url = citation.get('url', '')
        
        # Trusted source bonus
        if any(domain in url for domain in trusted_domains):
            score += 40
        
        # Local news (good for local context)
        elif any(indicator in url for indicator in local_news_indicators):
            score += 25
        
        # Social media (treat with caution)
        if 'facebook.com' in url or 'twitter.com' in url:
            score -= 20
        
        # Recency (–µ—Å–ª–∏ –µ—Å—Ç—å date –≤ citation)
        if 'date' in citation:
            days_old = (datetime.now() - parse_date(citation['date'])).days
            if days_old < 30:
                score += 20
            elif days_old < 90:
                score += 10
        
        scores.append({
            'url': url,
            'score': max(0, min(100, score)),
            'reliability': 'High' if score > 80 else 'Medium' if score > 50 else 'Low'
        })
    
    return scores

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è:
# "Found in 3 sources: CQC (high reliability), Local Gazette (medium), Facebook (low)"
```

#### –§–∏—à–∫–∞ 2: Multi-turn contextual search
```python
def multi_turn_investigation(home_name):
    """
    –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–ø–æ—á–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    conversation_history = []
    
    # Turn 1: Basic search
    turn1 = {
        "role": "user",
        "content": f"What is {home_name}? Provide basic info."
    }
    conversation_history.append(turn1)
    response1 = call_perplexity(conversation_history)
    conversation_history.append({"role": "assistant", "content": response1})
    
    # Turn 2: Drilling down based on response
    turn2 = {
        "role": "user",
        "content": "Are there any recent complaints or concerns about this care home? Search news and forums."
    }
    conversation_history.append(turn2)
    response2 = call_perplexity(conversation_history)
    conversation_history.append({"role": "assistant", "content": response2})
    
    # Turn 3: Operator investigation
    turn3 = {
        "role": "user",
        "content": "Who operates this home? Find information about the parent company's financial status and other homes they manage."
    }
    conversation_history.append(turn3)
    response3 = call_perplexity(conversation_history)
    
    return {
        'basic_info': response1,
        'reputation': response2,
        'operator_info': response3
    }
```

#### –§–∏—à–∫–∞ 3: Structured extraction from unstructured search
```python
def extract_structured_events(perplexity_response):
    """
    Extract structured event data from narrative response
    """
    import re
    from dateutil import parser as date_parser
    
    content = perplexity_response['choices'][0]['message']['content']
    citations = perplexity_response.get('citations', [])
    
    events = []
    
    # Regex patterns for key events
    patterns = {
        'award': r'(award|prize|accolade|recognition)',
        'complaint': r'(complaint|concern|allegation|issue)',
        'inspection': r'(inspection|visit|report|rating)',
        'ownership': r'(acquired|purchased|taken over|sold)',
        'outbreak': r'(outbreak|infection|covid|norovirus)'
    }
    
    sentences = content.split('. ')
    for sentence in sentences:
        for event_type, pattern in patterns.items():
            if re.search(pattern, sentence, re.IGNORECASE):
                # Try to extract date
                date_match = re.search(r'\b(january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{4}\b', 
                                      sentence, re.IGNORECASE)
                
                events.append({
                    'type': event_type,
                    'description': sentence,
                    'date': date_match.group(0) if date_match else 'Unknown',
                    'sentiment': 'negative' if event_type in ['complaint', 'outbreak'] else 'positive' if event_type == 'award' else 'neutral'
                })
    
    return events

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è:
# Timeline visualization –≤ Premium –æ—Ç—á–µ—Ç–∞—Ö
```

#### –§–∏—à–∫–∞ 4: Cross-reference validation
```python
def cross_reference_with_official_sources(perplexity_findings, cqc_data):
    """
    Validate Perplexity findings against official CQC data
    """
    discrepancies = []
    confirmations = []
    
    # Example: Check if Perplexity mentioned a rating change
    if 'rating' in perplexity_findings['content'].lower():
        perplexity_rating = extract_rating(perplexity_findings['content'])
        cqc_rating = cqc_data['currentRatings']['overall']['rating']
        
        if perplexity_rating != cqc_rating:
            discrepancies.append({
                'field': 'rating',
                'perplexity': perplexity_rating,
                'cqc_official': cqc_rating,
                'note': 'Perplexity may have outdated info. Trust CQC.'
            })
        else:
            confirmations.append('Rating confirmed across sources')
    
    return {
        'discrepancies': discrepancies,
        'confirmations': confirmations,
        'trust_score': len(confirmations) / (len(confirmations) + len(discrepancies)) if confirmations or discrepancies else 0
    }
```

### üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ö–µ–π—Å—ã

#### –ö–µ–π—Å 1: Automated reputation monitoring
```python
def weekly_reputation_scan(care_homes_list):
    """
    –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏ –≤—Å–µ—Ö –¥–æ–º–æ–≤
    """
    alerts = []
    
    for home in care_homes_list:
        payload = {
            "model": "sonar-pro",
            "messages": [{
                "role": "user",
                "content": f"Search for any negative news, complaints, or concerns about {home['name']} in the last 7 days. Be concise."
            }],
            "search_recency_filter": "week",
            "max_tokens": 300
        }
        
        response = call_perplexity(payload)
        content = response['choices'][0]['message']['content']
        
        # Simple sentiment check
        negative_keywords = ['complaint', 'concern', 'poor', 'inadequate', 'crisis']
        if any(keyword in content.lower() for keyword in negative_keywords):
            alerts.append({
                'home': home['name'],
                'severity': 'HIGH' if 'crisis' in content.lower() else 'MEDIUM',
                'summary': content,
                'sources': response.get('citations', [])
            })
    
    # Send alerts to Premium subscribers
    if alerts:
        send_email_alert(alerts)
    
    return alerts
```

#### –ö–µ–π—Å 2: Competitive intelligence dashboard
```python
def build_competitive_intel(region, competitors):
    """
    –°–æ–±—Ä–∞—Ç—å intelligence –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö –≤ —Ä–µ–≥–∏–æ–Ω–µ
    """
    intel_report = {}
    
    for competitor in competitors:
        # –ù–æ–≤–æ—Å—Ç–∏ –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–µ
        news_query = f"Recent news and developments about {competitor['name']} care home in last 6 months"
        
        # –§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞
        financial_query = f"Financial status and performance of {competitor['operator']} care homes company"
        
        news_response = call_perplexity(news_query)
        financial_response = call_perplexity(financial_query)
        
        intel_report[competitor['name']] = {
            'recent_developments': extract_events(news_response),
            'financial_health': analyze_financial_mentions(financial_response),
            'risk_level': assess_risk(news_response, financial_response),
            'last_updated': datetime.now()
        }
    
    return intel_report

def assess_risk(news, financial):
    """Simple risk scoring based on keywords"""
    risk_indicators = ['bankruptcy', 'administration', 'closure', 'lawsuit', 'complaint']
    
    combined_text = news + financial
    risk_count = sum(1 for indicator in risk_indicators if indicator in combined_text.lower())
    
    if risk_count >= 3:
        return 'HIGH'
    elif risk_count >= 1:
        return 'MEDIUM'
    else:
        return 'LOW'
```

---

<a name="–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ-–∏—Å—Ç–æ—á–Ω–∏–∫–∏"></a>
# üåê –ß–ê–°–¢–¨ 3: –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–°–¢–û–ß–ù–ò–ö–ò

<a name="7-autumna-–≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥"></a>
## 7. Autumna (–í–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥)

### üìö –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
- **Website**: https://www.autumna.care/
- **–ú–µ—Ç–æ–¥**: Web scraping (Beautiful Soup, Scrapy, –∏–ª–∏ Selenium)
- **Proxy —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è**: Rotating residential proxies (–∏–∑–±–µ–∂–∞—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
- **–ß–∞—Å—Ç–æ—Ç–∞**: Weekly scraping (—á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å —Å–∞–π—Ç)

### üîë –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
```
Tools:
- Python: requests, BeautifulSoup4, Scrapy
- Proxy service: BrightData, SmartProxy (~¬£50/month)
- User-Agent rotation
- Rate limiting: 1 request every 2-3 seconds

Legal considerations:
- Check robots.txt: https://www.autumna.care/robots.txt
- Respect rate limits
- Don't DDoS the site
- Data usage: Fair use for comparison service
```

### üß™ –¢–µ—Å—Ç–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

#### –¢–µ—Å—Ç 1: –ë–∞–∑–æ–≤—ã–π scraper –¥–ª—è —Å–ø–∏—Å–∫–∞ –¥–æ–º–æ–≤
```python
import requests
from bs4 import BeautifulSoup
import time
import random

class AutumnaScraper:
    def __init__(self, proxy=None):
        self.base_url = "https://www.autumna.care"
        self.session = requests.Session()
        if proxy:
            self.session.proxies = {"http": proxy, "https": proxy}
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def search_care_homes(self, location, page=1):
        """
        –ü–æ–∏—Å–∫ –¥–æ–º–æ–≤ –ø–æ –ª–æ–∫–∞—Ü–∏–∏
        """
        search_url = f"{self.base_url}/care-homes?location={location}&page={page}"
        
        response = self.session.get(search_url)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        homes = []
        # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ –∞–∫—Ç—É–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–∞–π—Ç–∞
        home_cards = soup.find_all('div', class_='care-home-card')
        
        for card in home_cards:
            home = {
                'name': card.find('h3', class_='home-name').text.strip(),
                'url': self.base_url + card.find('a')['href'],
                'price_from': self.extract_price(card),
                'specialisms': [s.text for s in card.find_all('span', class_='specialism')],
                'location': card.find('span', class_='location').text.strip()
            }
            homes.append(home)
        
        # Rate limiting
        time.sleep(random.uniform(2, 4))
        
        return homes
    
    def extract_price(self, card):
        price_elem = card.find('span', class_='price')
        if price_elem:
            price_text = price_elem.text
            # Extract number: "¬£1,450 per week" -> 1450
            import re
            match = re.search(r'¬£([\d,]+)', price_text)
            if match:
                return int(match.group(1).replace(',', ''))
        return None
```

#### –¢–µ—Å—Ç 2: –î–µ—Ç–∞–ª—å–Ω—ã–π scraper –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–∞
```python
def scrape_home_details(self, home_url):
    """
    –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –¥–æ–º–µ
    """
    response = self.session.get(home_url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    details = {
        'name': soup.find('h1', class_='home-name').text.strip(),
        'description': soup.find('div', class_='description').text.strip(),
        'amenities': [],
        'photos': [],
        'room_types': [],
        'contact': {}
    }
    
    # Amenities (—É–¥–æ–±—Å—Ç–≤–∞)
    amenities_section = soup.find('section', {'id': 'amenities'})
    if amenities_section:
        details['amenities'] = [
            amenity.text.strip() 
            for amenity in amenities_section.find_all('li')
        ]
    
    # Photos
    photo_gallery = soup.find('div', class_='photo-gallery')
    if photo_gallery:
        details['photos'] = [
            img['src'] for img in photo_gallery.find_all('img')
        ]
    
    # Room types and prices
    rooms_section = soup.find('section', {'id': 'rooms'})
    if rooms_section:
        for room_card in rooms_section.find_all('div', class_='room-card'):
            details['room_types'].append({
                'type': room_card.find('h4').text.strip(),
                'price': self.extract_price(room_card),
                'features': [f.text for f in room_card.find_all('li')]
            })
    
    # Contact info
    contact_section = soup.find('section', {'id': 'contact'})
    if contact_section:
        details['contact'] = {
            'phone': contact_section.find('a', href=lambda h: h and h.startswith('tel:')).text.strip(),
            'email': contact_section.find('a', href=lambda h: h and h.startswith('mailto:')).text.strip(),
            'address': contact_section.find('address').text.strip()
        }
    
    time.sleep(random.uniform(3, 5))  # Longer wait for detail pages
    
    return details
```

#### –¢–µ—Å—Ç 3: Change detection (–æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
```python
def detect_changes(self, home_id, previous_data, current_data):
    """
    –°—Ä–∞–≤–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
    """
    changes = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω—ã
    if previous_data.get('price_from') != current_data.get('price_from'):
        changes.append({
            'field': 'price',
            'old_value': previous_data.get('price_from'),
            'new_value': current_data.get('price_from'),
            'change_pct': ((current_data.get('price_from', 0) - previous_data.get('price_from', 0)) / 
                          previous_data.get('price_from', 1)) * 100
        })
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ amenities
    old_amenities = set(previous_data.get('amenities', []))
    new_amenities = set(current_data.get('amenities', []))
    
    added_amenities = new_amenities - old_amenities
    removed_amenities = old_amenities - new_amenities
    
    if added_amenities:
        changes.append({
            'field': 'amenities_added',
            'values': list(added_amenities)
        })
    
    if removed_amenities:
        changes.append({
            'field': 'amenities_removed',
            'values': list(removed_amenities)
        })
    
    return changes
```

### üìä –î–∞–Ω–Ω—ã–µ –¥–ª—è –í–∞–ª–∏–¥–∞—Ü–∏–∏

**–¢–µ—Å—Ç–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã**:
1. Search page: /care-homes?location=Brighton
2. Detail page: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–º –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

**–ß—Ç–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å:**
- [ ] Scraper —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ (–µ—Å–ª–∏ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è, –Ω—É–∂–Ω—ã –ø—Ä–æ–∫—Å–∏)
- [ ] –î–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ–∫–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ (–Ω–µ None/–ø—É—Å—Ç—ã–µ)
- [ ] Rate limiting —Å–æ–±–ª—é–¥–∞–µ—Ç—Å—è (–Ω–µ –±–æ–ª–µ–µ 30 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–∏–Ω—É—Ç—É)
- [ ] Photos URL —Ä–∞–±–æ—Ç–∞—é—Ç (–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è)
- [ ] Prices –≤ –≤–∞–ª–∏–¥–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (¬£1,000-¬£3,000/week)

### üí° –ù–µ–æ—á–µ–≤–∏–¥–Ω—ã–µ –§–∏—à–∫–∏

#### –§–∏—à–∫–∞ 1: Semantic amenities classification
```python
def classify_amenities(amenities_list):
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å amenities –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    categories = {
        'outdoor': ['garden', 'terrace', 'patio', 'outdoor space'],
        'accessibility': ['wheelchair accessible', 'lift', 'ramps', 'ground floor'],
        'social': ['activities room', 'cinema', 'library', 'common areas'],
        'tech': ['wifi', 'internet', 'tv in rooms', 'call system'],
        'food': ['dining room', 'cafe', 'restaurant', 'home cooked meals'],
        'wellness': ['hairdresser', 'spa', 'fitness', 'physiotherapy']
    }
    
    classified = {cat: [] for cat in categories}
    
    for amenity in amenities_list:
        amenity_lower = amenity.lower()
        for category, keywords in categories.items():
            if any(keyword in amenity_lower for keyword in keywords):
                classified[category].append(amenity)
    
    return classified

# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è:
# "This home has excellent outdoor amenities (3 features): 
#  garden, terrace, outdoor seating"
```

#### –§–∏—à–∫–∞ 2: Photo –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–∞–ª–∏–∑
```python
from PIL import Image
import requests
from io import BytesIO

def analyze_photo_quality(photo_urls):
    """
    –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π (—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, brightness)
    """
    quality_scores = []
    
    for url in photo_urls[:10]:  # –ü–µ—Ä–≤—ã–µ 10 —Ñ–æ—Ç–æ
        try:
            response = requests.get(url, timeout=5)
            img = Image.open(BytesIO(response.content))
            
            width, height = img.size
            pixels = width * height
            
            # Score based on resolution
            if pixels > 2_000_000:  # > 2MP
                resolution_score = 100
            elif pixels > 1_000_000:  # 1-2MP
                resolution_score = 70
            else:
                resolution_score = 40
            
            quality_scores.append({
                'url': url,
                'resolution': f"{width}x{height}",
                'score': resolution_score
            })
        except:
            continue
    
    avg_score = sum(s['score'] for s in quality_scores) / len(quality_scores)
    
    if avg_score > 80:
        assessment = "‚úì High-quality professional photos"
    elif avg_score > 60:
        assessment = "Good photo quality"
    else:
        assessment = "‚ö† Low quality/outdated photos"
    
    return {
        'average_score': avg_score,
        'assessment': assessment,
        'total_photos': len(photo_urls),
        'analyzed': len(quality_scores)
    }
```

#### –§–∏—à–∫–∞ 3: Price trend analysis
```python
def analyze_price_trends(home_id, historical_prices):
    """
    –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–µ–Ω—ã
    """
    if len(historical_prices) < 2:
        return "Insufficient data"
    
    # Sort by date
    sorted_prices = sorted(historical_prices, key=lambda x: x['date'])
    
    # Calculate changes
    price_changes = []
    for i in range(1, len(sorted_prices)):
        old_price = sorted_prices[i-1]['price']
        new_price = sorted_prices[i]['price']
        change_pct = ((new_price - old_price) / old_price) * 100
        
        price_changes.append({
            'date': sorted_prices[i]['date'],
            'change_pct': change_pct,
            'new_price': new_price
        })
    
    # Trend detection
    recent_changes = [c['change_pct'] for c in price_changes[-3:]]  # Last 3 changes
    avg_change = sum(recent_changes) / len(recent_changes)
    
    if avg_change > 5:
        trend = "‚ö† Prices increasing rapidly (+5%+ average)"
    elif avg_change > 0:
        trend = "Prices increasing moderately"
    elif avg_change < -5:
        trend = "‚ö† Prices declining (possible quality concerns?)"
    else:
        trend = "‚úì Stable pricing"
    
    return {
        'trend': trend,
        'avg_change_pct': avg_change,
        'current_price': sorted_prices[-1]['price'],
        'price_history': price_changes
    }
```

#### –§–∏—à–∫–∞ 4: Competitive set identification
```python
def identify_competitive_set(target_home, all_homes):
    """
    –ù–∞–π—Ç–∏ –ø—Ä—è–º—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """
    # –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–º–æ–≤
    competitors = []
    
    for home in all_homes:
        if home['id'] == target_home['id']:
            continue
        
        similarity_score = 0
        
        # –¶–µ–Ω–∞ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö ¬±20%
        price_diff = abs(home['price'] - target_home['price']) / target_home['price']
        if price_diff < 0.20:
            similarity_score += 30
        
        # Overlapping specialisms
        target_specialisms = set(target_home['specialisms'])
        home_specialisms = set(home['specialisms'])
        overlap = len(target_specialisms & home_specialisms)
        similarity_score += overlap * 15
        
        # Proximity (–µ—Å–ª–∏ –µ—Å—Ç—å coordinates)
        distance = calculate_distance(target_home['coords'], home['coords'])
        if distance < 5:  # 5km
            similarity_score += 25
        
        if similarity_score > 50:
            competitors.append({
                'home': home,
                'similarity_score': similarity_score
            })
    
    # Return top 5 competitors
    return sorted(competitors, key=lambda x: x['similarity_score'], reverse=True)[:5]
```

### üîó –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –ö–µ–π—Å—ã

#### –ö–µ–π—Å 1: Price monitoring alerts
```python
def price_monitoring_system():
    """
    –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ü–µ–Ω —Å –∞–ª–µ—Ä—Ç–∞–º–∏ –¥–ª—è —é–∑–µ—Ä–æ–≤
    """
    homes_to_monitor = get_user_watchlist()  # Premium feature
    
    for home in homes_to_monitor:
        current_data = scrape_home_details(home['autumna_url'])
        previous_data = get_from_db(home['id'], date='last_week')
        
        if current_data['price_from'] != previous_data['price_from']:
            change = current_data['price_from'] - previous_data['price_from']
            change_pct = (change / previous_data['price_from']) * 100
            
            alert = {
                'home_name': home['name'],
                'price_change': change,
                'change_pct': change_pct,
                'new_price': current_data['price_from'],
                'message': f"Price {'increased' if change > 0 else 'decreased'} by ¬£{abs(change)}/week ({change_pct:+.1f}%)"
            }
            
            send_user_alert(home['user_id'], alert)
```

#### –ö–µ–π—Å 2: Amenities-based matching
```python
def match_by_amenities(user_requirements):
    """
    –ù–∞–π—Ç–∏ –¥–æ–º–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ amenities –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    required_amenities = user_requirements['must_have_amenities']
    nice_to_have = user_requirements['nice_to_have']
    
    all_homes = get_all_homes_from_db()
    
    matches = []
    for home in all_homes:
        home_amenities = set(home['amenities'])
        
        # Check must-haves
        required_met = all(req in home_amenities for req in required_amenities)
        
        if required_met:
            # Score based on nice-to-haves
            nice_to_have_count = sum(1 for nice in nice_to_have if nice in home_amenities)
            score = (nice_to_have_count / len(nice_to_have)) * 100 if nice_to_have else 100
            
            matches.append({
                'home': home,
                'amenities_score': score,
                'missing_nice_to_have': [n for n in nice_to_have if n not in home_amenities]
            })
    
    return sorted(matches, key=lambda x: x['amenities_score'], reverse=True)
```

---

<a name="–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è"></a>
# üîó –ß–ê–°–¢–¨ 4: –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ò –ü–†–û–î–í–ò–ù–£–¢–´–ï –ö–ï–ô–°–´

## –úulti-API Data Fusion

### –ö–µ–π—Å 1: Comprehensive Home Profile
```python
def build_comprehensive_profile(home_name, location):
    """
    –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
    """
    profile = {
        'basic_info': {},
        'quality': {},
        'financial': {},
        'reputation': {},
        'behavioral': {},
        'risk_assessment': {}
    }
    
    # 1. CQC - Official quality data
    cqc_data = search_cqc_location(home_name, location)
    profile['basic_info'] = {
        'name': cqc_data['name'],
        'address': cqc_data['postalAddress'],
        'phone': cqc_data['mainPhoneNumber']
    }
    profile['quality']['cqc_rating'] = cqc_data['currentRatings']['overall']
    profile['quality']['specialisms'] = cqc_data['specialisms']
    
    # 2. FSA - Food safety
    fsa_data = search_fsa_by_location(cqc_data['latitude'], cqc_data['longitude'])
    profile['quality']['food_hygiene'] = {
        'rating': fsa_data['RatingValue'],
        'scores': fsa_data['Scores'],
        'last_inspection': fsa_data['RatingDate']
    }
    
    # 3. Companies House - Financial stability
    company_number = find_company_number(cqc_data['providerId'])
    company_data = fetch_company(company_number)
    profile['financial'] = {
        'company_status': company_data['company_status'],
        'accounts_overdue': company_data['accounts']['overdue'],
        'stability_score': calculate_financial_stability(company_number)
    }
    
    # 4. Google Places - Reviews and reputation
    place_data = find_google_place(home_name, location)
    profile['reputation'] = {
        'google_rating': place_data['rating'],
        'review_count': place_data['user_ratings_total'],
        'sentiment': analyze_review_sentiment(place_data['reviews'])
    }
    
    # 5. Places Insights - Behavioral data
    insights_data = query_places_insights(place_data['place_id'])
    profile['behavioral'] = {
        'weekly_visitors': insights_data['visitor_count_weekly'],
        'dwell_time': insights_data['visitor_dwell_time_avg'],
        'repeat_rate': insights_data['visitor_repeat_rate'],
        'engagement_score': calculate_family_engagement_score(insights_data)
    }
    
    # 6. Perplexity - Recent news and context
    news_data = search_perplexity(f"Recent news about {home_name}")
    profile['reputation']['recent_news'] = extract_events(news_data)
    
    # 7. Autumna - Pricing and amenities
    autumna_data = scrape_home_details(find_autumna_url(home_name))
    profile['basic_info']['price_range'] = autumna_data['price_from']
    profile['basic_info']['amenities'] = autumna_data['amenities']
    
    # Risk Assessment (combining all signals)
    profile['risk_assessment'] = assess_overall_risk(profile)
    
    return profile

def assess_overall_risk(profile):
    """
    Composite risk score from all data sources
    """
    risk_score = 0  # 0 = low risk, 100 = high risk
    flags = []
    
    # CQC rating
    if profile['quality']['cqc_rating']['rating'] == 'Inadequate':
        risk_score += 50
        flags.append('CQC Inadequate rating')
    elif profile['quality']['cqc_rating']['rating'] == 'Requires Improvement':
        risk_score += 25
        flags.append('CQC requires improvement')
    
    # Food safety
    if profile['quality']['food_hygiene']['rating'] < 4:
        risk_score += 20
        flags.append('Food hygiene concerns')
    
    # Financial
    if profile['financial']['stability_score'] < 50:
        risk_score += 30
        flags.append('Financial instability')
    
    # Behavioral (Places Insights)
    engagement = profile['behavioral']['engagement_score']['score']
    if engagement < 40:
        risk_score += 25
        flags.append('Low family engagement')
    
    # Reviews
    if profile['reputation']['google_rating'] < 3.5:
        risk_score += 20
        flags.append('Poor online reviews')
    
    return {
        'overall_risk_score': min(100, risk_score),
        'risk_level': 'HIGH' if risk_score > 70 else 'MEDIUM' if risk_score > 40 else 'LOW',
        'red_flags': flags,
        'recommendation': 'AVOID' if risk_score > 70 else 'CAUTION' if risk_score > 40 else 'SAFE'
    }
```

### –ö–µ–π—Å 2: Predictive Quality Model
```python
def predict_future_quality(home_id, historical_data):
    """
    Predict likelihood of CQC rating change in next 12 months
    Based on multi-source signals
    """
    features = {}
    
    # Feature 1: Places Insights trend (EARLY WARNING)
    insights_trend = calculate_insights_trend(home_id, months=6)
    if insights_trend['visitor_decline'] > 0.20:  # 20% decline
        features['visitor_decline'] = 30  # High weight
    
    # Feature 2: FSA trend
    fsa_trend = get_fsa_history(home_id)
    if len(fsa_trend) >= 2:
        if fsa_trend[-1]['rating'] < fsa_trend[-2]['rating']:
            features['food_hygiene_decline'] = 20
    
    # Feature 3: Review sentiment trend
    review_trend = analyze_review_sentiment_over_time(home_id)
    if review_trend['sentiment_decline']:
        features['negative_reviews_increasing'] = 15
    
    # Feature 4: Financial distress signals
    company_risk = get_company_risk_score(home_id)
    if company_risk > 70:
        features['financial_stress'] = 25
    
    # Feature 5: Operator network performance
    operator_homes = get_operator_other_homes(home_id)
    declining_homes = [h for h in operator_homes if h['rating_declined_recently']]
    if len(declining_homes) > 0.3 * len(operator_homes):  # 30%+ network declining
        features['network_effect'] = 20
    
    # Calculate probability
    total_risk_points = sum(features.values())
    
    if total_risk_points > 60:
        prediction = {
            'probability_of_decline': 0.75,
            'confidence': 'HIGH',
            'timeframe': '6-12 months',
            'recommendation': '‚ö†Ô∏è HIGH RISK: Multiple warning signals detected',
            'key_indicators': list(features.keys())
        }
    elif total_risk_points > 30:
        prediction = {
            'probability_of_decline': 0.45,
            'confidence': 'MEDIUM',
            'timeframe': '12-18 months',
            'recommendation': 'Monitor closely: Some concerning trends',
            'key_indicators': list(features.keys())
        }
    else:
        prediction = {
            'probability_of_decline': 0.15,
            'confidence': 'LOW',
            'timeframe': 'N/A',
            'recommendation': '‚úì Quality stable: No significant concerns',
            'key_indicators': []
        }
    
    return prediction
```

---

<a name="roadmap"></a>
# üìÖ ROADMAP –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø

## Week 1: Foundation APIs
**–¶–µ–ª—å**: –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ API

### –ó–∞–¥–∞—á–∏:
- [ ] **Day 1-2**: CQC API Setup
  - –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å partnerCode
  - –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ 5 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
  - –°–æ—Ö—Ä–∞–Ω–∏—Ç—å 100+ –¥–æ–º–æ–≤ –≤ South East –≤ test DB
  - –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å completeness –¥–∞–Ω–Ω—ã—Ö

- [ ] **Day 3-4**: FSA FHRS API
  - –ò–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
  - Match FSA establishments —Å CQC locations (–ø–æ –≥–µ–æ–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º)
  - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å correlation FSA rating ‚Üî CQC rating
  - –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ DB

- [ ] **Day 5**: Companies House API
  - Setup authentication
  - Fetch company data –¥–ª—è top 20 providers –≤ —Ä–µ–≥–∏–æ–Ω–µ
  - Build financial stability scores
  - Identify any red flags

### Deliverable Week 1:
‚úÖ Database —Å 100+ –¥–æ–º–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∞—è CQC + FSA + Companies House data
‚úÖ Validation report: data completeness, quality issues

---

## Week 2: Commercial APIs
**–¶–µ–ª—å**: –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞—Ç–Ω—ã–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏

### –ó–∞–¥–∞—á–∏:
- [ ] **Day 1-2**: Google Places API
  - Setup billing (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ $200 free credits)
  - Find place_id –¥–ª—è –≤—Å–µ—Ö 100 –¥–æ–º–æ–≤
  - Fetch reviews –¥–ª—è top 20 –¥–æ–º–æ–≤
  - Implement sentiment analysis

- [ ] **Day 3**: Perplexity API
  - Setup account –∏ credits ($10 –º–∏–Ω–∏–º—É–º)
  - Test 10 queries –¥–ª—è different care homes
  - Evaluate citation quality
  - Test multi-turn investigation

- [ ] **Day 4-5**: Autumna Scraping
  - Implement scraper —Å proxy rotation
  - Scrape 50 homes (test set)
  - Validate: prices, amenities, photos
  - Setup weekly cron job

### Deliverable Week 2:
‚úÖ Enhanced database —Å Google reviews, Perplexity insights, Autumna pricing
‚úÖ Cost analysis report (actual API costs vs projected)

---

## Week 3: Advanced Features
**–¶–µ–ª—å**: Google Places Insights –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏

### –ó–∞–¥–∞—á–∏:
- [ ] **Day 1-3**: BigQuery Places Insights
  - Setup BigQuery project
  - Subscribe to UK dataset
  - Run all 5 test queries
  - Validate behavioral metrics
  - CRITICAL: Test correlation dwell time ‚Üí CQC rating

- [ ] **Day 4-5**: Multi-API Integration
  - Implement comprehensive_profile function
  - Test risk assessment algorithm
  - Build predictive quality model
  - Validate predictions against known cases

### Deliverable Week 3:
‚úÖ Places Insights integration working
‚úÖ Multi-source data fusion pipeline
‚úÖ Predictive model prototype

---

## Week 4: Production Readiness
**–¶–µ–ª—å**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

### –ó–∞–¥–∞—á–∏:
- [ ] **Day 1**: Performance Optimization
  - Implement caching (Redis)
  - Parallel API calls where possible
  - Optimize database queries

- [ ] **Day 2**: Error Handling & Resilience
  - Retry logic –¥–ª—è –≤—Å–µ—Ö APIs
  - Fallback mechanisms
  - Rate limit handling

- [ ] **Day 3**: Monitoring & Alerting
  - API health checks
  - Cost monitoring dashboards
  - Data freshness alerts

- [ ] **Day 4**: Documentation
  - API integration docs
  - Data dictionary
  - Troubleshooting guide

- [ ] **Day 5**: Security & Compliance
  - API key rotation
  - GDPR compliance check
  - Data retention policies

### Deliverable Week 4:
‚úÖ Production-ready data pipeline
‚úÖ Complete documentation
‚úÖ Monitoring dashboards

---

## üéØ Success Metrics

### Technical KPIs:
- **API Availability**: >99.5% uptime –¥–ª—è –≤—Å–µ—Ö critical APIs
- **Data Freshness**: 90%+ homes updated within last 7 days
- **Data Quality**: <5% missing critical fields (CQC rating, FSA rating)
- **Response Time**: <2 sec –¥–ª—è single home query
- **Cost Efficiency**: <¬£300/month –¥–ª—è 1000 homes South East

### Business KPIs:
- **Coverage**: 95%+ care homes –≤ South East –∏–º–µ—é—Ç –ø—Ä–æ—Ñ–∏–ª–∏
- **Uniqueness**: 100% –¥–æ–º–æ–≤ –∏–º–µ—é—Ç —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π insight (–Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã–π —É –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤)
- **Predictive Accuracy**: 70%+ accuracy –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ CQC rating changes
- **User Value**: Demonstrable case studies –≥–¥–µ multi-source data –ø–æ–º–æ–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é

---

## üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê

### Checklist –ø–µ—Ä–µ–¥ Production:
- [ ] –í—Å–µ APIs –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- [ ] Rate limits –∏ costs –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã
- [ ] Error handling –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ edge cases
- [ ] Security: API keys –≤ environment variables, –Ω–µ –≤ –∫–æ–¥–µ
- [ ] GDPR: Privacy policy –æ–±–Ω–æ–≤–ª–µ–Ω–∞, consent flows –≥–æ—Ç–æ–≤—ã
- [ ] Backup: Automated daily backups –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
- [ ] Monitoring: Alerting –¥–ª—è API failures, cost overruns
- [ ] Documentation: Onboarding doc –¥–ª—è –Ω–æ–≤—ã—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤
- [ ] Legal: Terms of use –¥–ª—è —Å–∫—Ä–∞–ø–∏–Ω–≥–∞ (Autumna) reviewed by lawyer

---

## üí° –ö–õ–Æ–ß–ï–í–´–ï –ò–ù–°–ê–ô–¢–´ –ò –í–´–í–û–î–´

### –ß—Ç–æ –¥–µ–ª–∞–µ—Ç RightCareHome —É–Ω–∏–∫–∞–ª—å–Ω—ã–º:
1. **Google Places Insights**: –ù–ò–ö–¢–û –≤ UK care home –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç behavioral footfall data
2. **Predictive Analytics**: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∑–∞ 6-12 –º–µ—Å—è—Ü–µ–≤ –î–û CQC inspection
3. **Multi-Source Validation**: –ü–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑ 7+ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ‚Üí –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
4. **FSA Integration**: –ü–µ—Ä–≤—ã–µ, –∫—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç food hygiene –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –¥–æ–º–æ–≤ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è diabetics)

### Red Flags –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è:
- ‚ö†Ô∏è Companies House: insolvency history, dissolved status, 5+ charges
- ‚ö†Ô∏è Places Insights: visitor footfall decline >30%, dwell time <25 min
- ‚ö†Ô∏è FSA: rating <3, multiple "Improvement Required" scores
- ‚ö†Ô∏è CQC: "Inadequate" rating, enforcement actions, safeguarding alerts
- ‚ö†Ô∏è Perplexity: Recent negative news, complaints, outbreak mentions

### Competitive Moat:
**–ó–∞—â–∏—Ç–Ω–∞—è "–∫—Ä–µ–ø–æ—Å—Ç—å" –¥–∞–Ω–Ω—ã—Ö:**
1. BigQuery Places Insights: –¢—Ä–µ–±—É–µ—Ç technical expertise + GCP infrastructure
2. Multi-API fusion: Complex integration, 4-6 –Ω–µ–¥–µ–ª—å development
3. Predictive models: –¢—Ä–µ–±—É–µ—Ç—Å—è historical data (6+ months collection)
4. Domain expertise: Healthcare + Tech + Data Science combination —Ä–µ–¥–∫–∞

**–†–µ–∑—É–ª—å—Ç–∞—Ç**: 6-12 –º–µ—Å—è—Ü–µ–≤ lead time –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –ø–æ–≤—Ç–æ—Ä–∏—Ç—å

---

## üìû SUPPORT –ò –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò

### –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –ø—Ä–æ–±–ª–µ–º—ã:
1. **CQC API**: syndicationapi@cqc.org.uk
2. **FSA API**: data@food.gov.uk
3. **Companies House**: enquiries@companieshouse.gov.uk
4. **Google Cloud Support**: –ß–µ—Ä–µ–∑ Cloud Console (–µ—Å–ª–∏ Premium support)
5. **Perplexity**: help@perplexity.ai

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ Next Steps:
1. –ù–∞—á–∞—Ç—å —Å Week 1 Roadmap
2. Setup development environment (Python, PostgreSQL, Redis)
3. –°–æ–∑–¥–∞—Ç—å test GCP project –¥–ª—è BigQuery
4. Allocate budget: ¬£500 –¥–ª—è –º–µ—Å—è—Ü–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (APIs + proxies)

---

**–£–¥–∞—á–∏ —Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º! üöÄ**

*–≠—Ç–æ—Ç –ø–ª–∞–Ω —Å–æ–∑–¥–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ industry research. –ê–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ –ø–æ–¥ —Å–≤–æ–∏ –Ω—É–∂–¥—ã.*
