# Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³Ğ° ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…

## ğŸ“‹ ĞĞ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ

1. [ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°](#ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ)
2. [ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹](#Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°)
3. [Ğ¤Ğ°Ğ·Ğ° 0: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ°Ğ¹Ñ‚Ğ°](#Ñ„Ğ°Ğ·Ğ°-0)
4. [Ğ¤Ğ°Ğ·Ğ° 1: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ (Smart Discovery)](#Ñ„Ğ°Ğ·Ğ°-1)
5. [Ğ¤Ğ°Ğ·Ğ° 2: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ±Ğ¾Ñ€ (Semantic Crawl)](#Ñ„Ğ°Ğ·Ğ°-2)
6. [Ğ¤Ğ°Ğ·Ğ° 3: AI-Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ (Intelligent Extraction)](#Ñ„Ğ°Ğ·Ğ°-3)
7. [ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ](#Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹)
8. [ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²](#Ñ‚Ğ¸Ğ¿Ñ‹-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²)
9. [ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ](#Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ)

---

<a name="ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ"></a>
## 1. ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°

### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¶ĞµÑÑ‚ĞºĞ¸Ñ… ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²

**Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ (ĞĞ• Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾):**
```python
# âŒ Ğ–ĞµÑÑ‚ĞºĞ°Ñ Ğ¿Ñ€Ğ¸Ğ²ÑĞ·ĞºĞ° Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ
name = soup.find('h1', class_='care-home-title').text
phone = soup.find('a', class_='phone-link')['href']
address = soup.find('div', class_='address-block').text
```

**ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹:**
- Ğ Ğ°Ğ·Ğ½Ñ‹Ğµ ÑĞ°Ğ¹Ñ‚Ñ‹ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ ĞºĞ»Ğ°ÑÑÑ‹ CSS
- Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ° Ğ»Ğ¾Ğ¼Ğ°ĞµÑ‚ Ğ¿Ğ°Ñ€ÑĞµÑ€
- ĞšĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ°
- ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ½Ğ° Ñ‚Ñ‹ÑÑÑ‡Ğ¸ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²

### Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ (Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ²ĞµĞ·Ğ´Ğµ)

**ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿:** ĞĞµ Ğ¸Ñ‰ĞµĞ¼ Ğ¿Ğ¾ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼, Ğ° **Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚**

```python
# âœ… Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´
name = find_entity_by_semantic_meaning("care home name", page_content)
phone = find_entity_by_pattern("phone number", page_content)
address = find_entity_by_meaning("physical address with postcode", page_content)
```

**ĞŸÑ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ°:**
- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ½Ğ° Ğ»ÑĞ±Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ HTML
- Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ² Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº Ğ½Ğ¾Ğ²Ñ‹Ğ¼ ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼
- ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ½Ğ° 15,000+ Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ… UK

---

<a name="Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°"></a>
## 2. ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹

### ĞšĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ğ¤ĞĞ—Ğ 0: ĞĞĞĞ›Ğ˜Ğ— Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ«                                   â”‚
â”‚  â€¢ ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ CMS (WordPress, Drupal, Custom)             â”‚
â”‚  â€¢ Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² URL                                  â”‚
â”‚  â€¢ ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ğ¤ĞĞ—Ğ 1: Ğ˜ĞĞ¢Ğ•Ğ›Ğ›Ğ•ĞšĞ¢Ğ£ĞĞ›Ğ¬ĞĞĞ• ĞĞ‘ĞĞĞ Ğ£Ğ–Ğ•ĞĞ˜Ğ•                       â”‚
â”‚  â€¢ Firecrawl Map API Ğ´Ğ»Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹                          â”‚
â”‚  â€¢ Semantic URL classification                              â”‚
â”‚  â€¢ AI-Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ğ¤ĞĞ—Ğ 2: Ğ¡Ğ•ĞœĞĞĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ™ Ğ¡Ğ‘ĞĞ                                  â”‚
â”‚  â€¢ Firecrawl Crawl Ñ prompt                                 â”‚
â”‚  â€¢ ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°                           â”‚
â”‚  â€¢ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ markdown + html                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ğ¤ĞĞ—Ğ 3: AI-Ğ˜Ğ—Ğ’Ğ›Ğ•Ğ§Ğ•ĞĞ˜Ğ•                                      â”‚
â”‚  â€¢ Claude API Ğ´Ğ»Ñ NLU                                       â”‚
â”‚  â€¢ Named Entity Recognition                                 â”‚
â”‚  â€¢ Structured data generation                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ¢ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑÑ‚ĞµĞº

**ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹:**
- **Firecrawl v2.5** - web scraping Ñ AI
- **Claude 3.5 Sonnet** - semantic understanding
- **Pydantic** - data validation
- **BeautifulSoup** - HTML fallback parsing
- **Regex patterns** - entity extraction

**API Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸:**
```python
from firecrawl import FirecrawlApp
from anthropic import Anthropic
from pydantic import BaseModel, Field
from bs4 import BeautifulSoup
import re
from typing import List, Optional, Dict, Any
```

---

<a name="Ñ„Ğ°Ğ·Ğ°-0"></a>
## 3. Ğ¤Ğ°Ğ·Ğ° 0: ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ°Ğ¹Ñ‚Ğ°

### 3.1 ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ CMS Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹

```python
class SiteAnalyzer:
    """ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹ ÑĞ°Ğ¹Ñ‚Ğ°"""
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.cms_detected = None
        self.site_patterns = {}
        
    async def detect_cms(self, html: str, headers: dict) -> str:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ CMS Ğ¿Ğ¾ ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ğ°Ğ¼"""
        
        cms_signatures = {
            "WordPress": [
                r'wp-content',
                r'wp-includes',
                r'<meta name="generator" content="WordPress',
            ],
            "Drupal": [
                r'/sites/default/',
                r'Drupal\.settings',
                r'<meta name="Generator" content="Drupal'
            ],
            "Wix": [
                r'wix\.com',
                r'_wix',
                r'X-Wix-Request-Id'
            ],
            "Squarespace": [
                r'squarespace',
                r'static\.squarespace',
            ],
            "Webflow": [
                r'webflow',
                r'assets\.website-files\.com'
            ],
            "Custom": []
        }
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾ HTML
        for cms, patterns in cms_signatures.items():
            for pattern in patterns:
                if re.search(pattern, html, re.IGNORECASE):
                    return cms
        
        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾ HTTP Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°Ğ¼
        server = headers.get('Server', '').lower()
        x_powered = headers.get('X-Powered-By', '').lower()
        
        if 'wordpress' in x_powered:
            return "WordPress"
        elif 'drupal' in server:
            return "Drupal"
        
        return "Custom"
    
    async def analyze_site_structure(self, homepage_html: str) -> Dict:
        """ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†"""
        
        soup = BeautifulSoup(homepage_html, 'html.parser')
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… ÑÑÑ‹Ğ»Ğ¾Ğº
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ URL
            if href.startswith('/'):
                href = self.base_url + href
            elif not href.startswith('http'):
                continue
                
            # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ½ÑƒÑ‚Ñ€ĞµĞ½Ğ½Ğ¸Ğµ ÑÑÑ‹Ğ»ĞºĞ¸
            if self.base_url in href:
                links.append(href)
        
        # ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ URL Ğ¿Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼
        url_patterns = self._classify_urls(links)
        
        return {
            "total_links": len(links),
            "url_patterns": url_patterns,
            "cms": self.cms_detected
        }
    
    def _classify_urls(self, urls: List[str]) -> Dict:
        """ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ URL Ğ¿Ğ¾ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼"""
        
        patterns = {
            "listings": [],       # Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ ÑĞ¾ ÑĞ¿Ğ¸ÑĞºĞ°Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼Ğ¾Ğ²
            "detail": [],         # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ´Ğ¾Ğ¼Ğ¾Ğ²
            "regional": [],       # Ğ ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            "services": [],       # Ğ¢Ğ¸Ğ¿Ñ‹ ÑƒÑĞ»ÑƒĞ³
            "about": [],          # Ğ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸
            "news": [],           # ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸
            "contact": [],        # ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹
            "other": []
        }
        
        # Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
        keywords = {
            "listings": [
                'homes', 'directory', 'find', 'search', 'list',
                'locations', 'all-homes', 'care-homes'
            ],
            "detail": [
                # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹ URL Ñ ID Ğ¸Ğ»Ğ¸ slug
                # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ¿Ğ¾ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ğµ: /care-homes/[slug]/
            ],
            "regional": [
                'location', 'area', 'region', 'county', 'city',
                'london', 'birmingham', 'manchester'  # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ¾Ğ²
            ],
            "services": [
                'services', 'care-types', 'nursing', 'dementia',
                'residential', 'respite', 'palliative'
            ],
            "about": [
                'about', 'company', 'who-we-are', 'our-story', 'team'
            ],
            "news": [
                'news', 'blog', 'articles', 'stories', 'press'
            ],
            "contact": [
                'contact', 'enquiry', 'get-in-touch', 'book'
            ]
        }
        
        for url in urls:
            url_lower = url.lower()
            classified = False
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¿Ğ¾ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼
            for category, words in keywords.items():
                if any(word in url_lower for word in words):
                    patterns[category].append(url)
                    classified = True
                    break
            
            # Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ´Ğ»Ñ detail pages (Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ URL Ğ±ĞµĞ· ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²)
            if not classified:
                path_parts = url.replace(self.base_url, '').strip('/').split('/')
                # Ğ•ÑĞ»Ğ¸ URL Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğ¹ (3+ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ) Ğ¸ Ğ½Ğµ Ğ¿Ğ¾Ğ¿Ğ°Ğ» Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
                if len(path_parts) >= 3:
                    patterns["detail"].append(url)
                else:
                    patterns["other"].append(url)
        
        return patterns

    async def detect_pagination(self, listing_html: str) -> Optional[Dict]:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ° Ğ¿Ğ°Ğ³Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸"""
        
        soup = BeautifulSoup(listing_html, 'html.parser')
        
        pagination_patterns = [
            # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 1: ?page=N
            r'\?page=(\d+)',
            # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 2: /page/N/
            r'/page/(\d+)/',
            # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 3: ?offset=N
            r'\?offset=(\d+)',
            # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 4: #page-N
            r'#page-(\d+)'
        ]
        
        # ĞŸĞ¾Ğ¸ÑĞº ÑÑÑ‹Ğ»Ğ¾Ğº Ğ½Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        pagination_links = soup.find_all('a', href=True, text=re.compile(r'next|â€º|Â»|>', re.I))
        pagination_links += soup.find_all('a', href=re.compile(r'page=\d+|/page/\d+'))
        
        if not pagination_links:
            return None
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°
        for link in pagination_links:
            href = link['href']
            for pattern in pagination_patterns:
                if re.search(pattern, href):
                    return {
                        "type": "url_parameter" if '?' in pattern else "path_segment",
                        "pattern": pattern,
                        "example": href
                    }
        
        return None

    async def detect_infinite_scroll(self, html: str) -> bool:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ infinite scroll (JavaScript loading)"""
        
        indicators = [
            r'infinite[- ]?scroll',
            r'lazy[- ]?load',
            r'load[- ]?more',
            r'scroll[- ]?event'
        ]
        
        for indicator in indicators:
            if re.search(indicator, html, re.IGNORECASE):
                return True
        
        return False
```

### 3.2 Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ²

```python
class URLPatternRecognizer:
    """Ğ Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² URL Ğ´Ğ»Ñ Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ°"""
    
    @staticmethod
    def extract_url_template(urls: List[str]) -> Dict[str, str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² URL Ğ¸Ğ· ÑĞ¿Ğ¸ÑĞºĞ°"""
        
        templates = {}
        
        # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° URL Ğ¿Ğ¾ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğ¼ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼
        from collections import defaultdict
        pattern_groups = defaultdict(list)
        
        for url in urls:
            # Ğ£Ğ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°
            parsed = urlparse(url)
            path = parsed.path
            
            # Ğ—Ğ°Ğ¼ĞµĞ½Ğ° Ñ‡Ğ¸ÑĞµĞ» Ğ½Ğ° {id}, ÑĞ»Ğ°Ğ³Ğ¾Ğ² Ğ½Ğ° {slug}
            # /care-homes/123/ -> /care-homes/{id}/
            # /care-homes/avery-park/ -> /care-homes/{slug}/
            
            template = path
            # Ğ§Ğ¸ÑĞ»Ğ°
            template = re.sub(r'/\d+/', '/{id}/', template)
            # Ğ¡Ğ»Ğ°Ğ³Ğ¸ (lowercase + hyphens)
            template = re.sub(r'/[a-z\-]+/', '/{slug}/', template)
            
            pattern_groups[template].append(url)
        
        # Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ñ‡Ğ°ÑÑ‚Ñ‹Ñ… ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ²
        sorted_patterns = sorted(
            pattern_groups.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )
        
        for i, (template, urls) in enumerate(sorted_patterns[:5]):
            templates[f"pattern_{i+1}"] = {
                "template": template,
                "count": len(urls),
                "examples": urls[:3]
            }
        
        return templates
    
    @staticmethod
    def match_url_to_template(url: str, templates: Dict) -> Optional[str]:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ, ĞºĞ°ĞºĞ¾Ğ¼Ñƒ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñƒ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ URL"""
        
        from urllib.parse import urlparse
        path = urlparse(url).path
        
        for template_name, template_data in templates.items():
            template = template_data['template']
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ regex Ğ¸Ğ· ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ°
            regex_pattern = template.replace('{id}', r'\d+')
            regex_pattern = regex_pattern.replace('{slug}', r'[a-z0-9\-]+')
            regex_pattern = '^' + regex_pattern + '$'
            
            if re.match(regex_pattern, path):
                return template_name
        
        return None
```

### 3.3 ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†

```python
class ContentTypeClassifier:
    """AI-ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ° ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹"""
    
    def __init__(self, anthropic_api_key: str):
        self.client = Anthropic(api_key=anthropic_api_key)
    
    async def classify_page_type(self, url: str, html_snippet: str) -> str:
        """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ° ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ñ‡ĞµÑ€ĞµĞ· Claude"""
        
        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² HTML Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°
        snippet = html_snippet[:5000]
        
        prompt = f"""Analyze this webpage and classify its type.

URL: {url}

HTML snippet:
{snippet}

Classify this page into ONE of these categories:
1. care_home_listing - A page listing multiple care homes (directory, search results)
2. care_home_detail - Detailed page about a SINGLE care home facility
3. regional_page - Page about care homes in a specific region/city
4. service_page - Page describing types of care services
5. news_article - News article or blog post
6. about_page - About the company/organization
7. contact_page - Contact information
8. homepage - Main homepage
9. other - None of the above

Respond with ONLY the category name, nothing else."""

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=50,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return response.content[0].text.strip()
    
    async def classify_batch(
        self, 
        pages: List[Dict[str, str]]
    ) -> Dict[str, str]:
        """ĞŸĞ°ĞºĞµÑ‚Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†"""
        
        classifications = {}
        
        for page in pages:
            try:
                page_type = await self.classify_page_type(
                    page['url'],
                    page['html']
                )
                classifications[page['url']] = page_type
            except Exception as e:
                print(f"Classification error for {page['url']}: {e}")
                classifications[page['url']] = "other"
        
        return classifications
```

---

<a name="Ñ„Ğ°Ğ·Ğ°-1"></a>
## 4. Ğ¤Ğ°Ğ·Ğ° 1: Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ (Smart Discovery)

### 4.1 ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Map Ñ ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ¹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸ĞµĞ¹

```python
class SmartDiscovery:
    """Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ URL Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…"""
    
    def __init__(self, firecrawl_api_key: str, anthropic_api_key: str):
        self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)
        self.classifier = ContentTypeClassifier(anthropic_api_key)
        
    async def discover_site(self, base_url: str) -> Dict:
        """
        ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹ ÑĞ°Ğ¹Ñ‚Ğ°
        """
        
        # Ğ¨Ğ°Ğ³ 1: Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ Map Ğ²ÑĞµÑ… URL
        print(f"ğŸ“ Phase 1.1: Mapping site structure...")
        
        map_result = self.firecrawl.map_url(base_url, params={
            "search": None,  # ĞĞµÑ‚ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ° - Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ’Ğ¡Ğ•
            "limit": 1000
        })
        
        all_urls = map_result.get('links', [])
        print(f"   Found {len(all_urls)} total URLs")
        
        # Ğ¨Ğ°Ğ³ 2: ĞŸĞµÑ€Ğ²Ğ¸Ñ‡Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ URL Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ğ¼
        print(f"ğŸ“ Phase 1.2: URL pattern classification...")
        
        analyzer = SiteAnalyzer(base_url)
        url_patterns = analyzer._classify_urls(all_urls)
        
        # Ğ¨Ğ°Ğ³ 3: Sample Ğ¸ AI-ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
        print(f"ğŸ“ Phase 1.3: AI content classification...")
        
        # Ğ‘ĞµÑ€ĞµĞ¼ ÑÑĞ¼Ğ¿Ğ»Ñ‹ Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸
        samples_to_check = []
        for category, urls in url_patterns.items():
            # Ğ‘ĞµÑ€ĞµĞ¼ Ğ´Ğ¾ 3 URL Ğ¸Ğ· ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
            sample_urls = urls[:3]
            for url in sample_urls:
                samples_to_check.append(url)
        
        # Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ scrape Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ 5000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)
        sample_pages = []
        for url in samples_to_check[:20]:  # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ 20 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸
            try:
                result = self.firecrawl.scrape_url(url, params={
                    "formats": ["html"],
                    "onlyMainContent": True
                })
                sample_pages.append({
                    "url": url,
                    "html": result.get('html', '')[:5000]
                })
            except:
                continue
        
        # AI ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
        classifications = await self.classifier.classify_batch(sample_pages)
        
        # Ğ¨Ğ°Ğ³ 4: Ğ£Ñ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI
        print(f"ğŸ“ Phase 1.4: Refining URL patterns...")
        
        confirmed_patterns = self._refine_patterns(
            url_patterns,
            classifications
        )
        
        # Ğ¨Ğ°Ğ³ 5: ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ² URL Ğ´Ğ»Ñ detail pages
        detail_urls = confirmed_patterns.get('care_home_detail', [])
        url_templates = URLPatternRecognizer.extract_url_template(detail_urls)
        
        return {
            "base_url": base_url,
            "total_urls": len(all_urls),
            "url_patterns": confirmed_patterns,
            "url_templates": url_templates,
            "sample_classifications": classifications
        }
    
    def _refine_patterns(
        self,
        url_patterns: Dict,
        ai_classifications: Dict
    ) -> Dict:
        """Ğ£Ñ‚Ğ¾Ñ‡Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸"""
        
        refined = {}
        
        # ĞŸĞµÑ€ĞµĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ AI
        for url, ai_type in ai_classifications.items():
            if ai_type not in refined:
                refined[ai_type] = []
            
            # ĞĞ°Ğ¹Ñ‚Ğ¸ Ğ²ÑĞµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ URL Ğ² Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ°Ñ…
            for category, urls in url_patterns.items():
                if url in urls:
                    # ĞŸĞµÑ€ĞµĞ¼ĞµÑÑ‚Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸Ğµ URL Ğ² Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½ÑƒÑ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ
                    refined[ai_type].extend(urls)
                    break
        
        # Ğ£Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹
        for category in refined:
            refined[category] = list(set(refined[category]))
        
        return refined
```

### 4.2 ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ listing pages

```python
class ListingPageDetector:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† ÑĞ¾ ÑĞ¿Ğ¸ÑĞºĞ°Ğ¼Ğ¸ Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…"""
    
    @staticmethod
    def detect_listing_indicators(html: str) -> Dict[str, Any]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ¸Ğ½Ğ´Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ¾Ğ² listing page"""
        
        soup = BeautifulSoup(html, 'html.parser')
        
        indicators = {
            "has_multiple_cards": False,
            "has_grid_layout": False,
            "has_phone_numbers": False,
            "has_addresses": False,
            "card_count": 0
        }
        
        # ĞŸĞ¾Ğ¸ÑĞº Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ñ…ÑÑ Ğ±Ğ»Ğ¾ĞºĞ¾Ğ² (ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸)
        # Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°: Ğ±Ğ»Ğ¾ĞºĞ¸ Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ¹
        divs = soup.find_all(['div', 'article', 'section'])
        
        # Ğ˜Ñ‰ĞµĞ¼ div Ñ Ğ¾Ğ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğ¼Ğ¸ ĞºĞ»Ğ°ÑÑĞ°Ğ¼Ğ¸ (Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸ĞµÑÑ)
        from collections import Counter
        class_counter = Counter()
        
        for div in divs:
            classes = div.get('class', [])
            if classes:
                class_key = ' '.join(sorted(classes))
                class_counter[class_key] += 1
        
        # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ ĞºĞ»Ğ°ÑÑ, Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑÑÑ‰Ğ¸Ğ¹ÑÑ 3+ Ñ€Ğ°Ğ·Ğ°
        for class_key, count in class_counter.items():
            if count >= 3:
                indicators["has_multiple_cards"] = True
                indicators["card_count"] = count
                break
        
        # ĞŸĞ¾Ğ¸ÑĞº grid/flex layout
        if re.search(r'display:\s*grid|display:\s*flex', str(soup), re.I):
            indicators["has_grid_layout"] = True
        
        # ĞŸĞ¾Ğ¸ÑĞº Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¾Ğ²
        phone_links = soup.find_all('a', href=re.compile(r'tel:'))
        if len(phone_links) >= 3:
            indicators["has_phone_numbers"] = True
        
        # ĞŸĞ¾Ğ¸ÑĞº Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ñ… Ğ°Ğ´Ñ€ĞµÑĞ¾Ğ² (UK postcodes)
        postcodes = re.findall(
            r'[A-Z]{1,2}\d{1,2}\s?\d[A-Z]{2}',
            soup.get_text()
        )
        if len(postcodes) >= 3:
            indicators["has_addresses"] = True
        
        # ĞÑ†ĞµĞ½ĞºĞ° ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸, Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ listing page
        confidence = sum([
            indicators["has_multiple_cards"] * 0.4,
            indicators["has_grid_layout"] * 0.2,
            indicators["has_phone_numbers"] * 0.2,
            indicators["has_addresses"] * 0.2
        ])
        
        indicators["confidence"] = confidence
        indicators["is_likely_listing"] = confidence >= 0.6
        
        return indicators
```

---

<a name="Ñ„Ğ°Ğ·Ğ°-2"></a>
## 5. Ğ¤Ğ°Ğ·Ğ° 2: Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑĞ±Ğ¾Ñ€ (Semantic Crawl)

### 5.1 Intelligent Crawl Ñ AI-prompt

```python
class SemanticCrawler:
    """Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ĞºÑ€Ğ°ÑƒĞ»Ğ¸Ğ½Ğ³ Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸ĞµĞ¹ Ğº ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ"""
    
    def __init__(self, firecrawl_api_key: str):
        self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)
    
    async def crawl_care_homes(
        self,
        discovery_result: Dict,
        max_pages: int = 500
    ) -> List[Dict]:
        """
        Ğ¡ĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ crawl Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² discovery
        """
        
        base_url = discovery_result['base_url']
        url_patterns = discovery_result['url_patterns']
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ğ°Ñ€Ñ‚Ğ¾Ğ²Ñ‹Ñ… Ñ‚Ğ¾Ñ‡ĞµĞº
        start_urls = self._select_start_urls(url_patterns)
        
        print(f"ğŸ•·ï¸ Phase 2.1: Semantic crawl starting from {len(start_urls)} URLs")
        
        # ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ prompt Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ²
        crawl_prompt = self._generate_adaptive_prompt(discovery_result)
        
        print(f"ğŸ•·ï¸ Crawl prompt: {crawl_prompt}")
        
        # Crawl Ñ prompt
        crawl_params = {
            "url": start_urls[0],  # Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ ÑÑ‚Ğ°Ñ€Ñ‚Ğ¾Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡ĞºĞ°
            "prompt": crawl_prompt,
            "limit": max_pages,
            "scrapeOptions": {
                "formats": ["markdown", "html"],
                "onlyMainContent": True,
                "waitFor": 1000
            },
            "maxDiscoveryDepth": 3,
            "allowBackwardCrawling": False
        }
        
        # Ğ•ÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ‡ĞµÑ‚ĞºĞ¸Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ includePaths
        if discovery_result.get('url_templates'):
            # ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·ÑƒĞµĞ¼ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ² regex
            include_patterns = self._templates_to_regex(
                discovery_result['url_templates']
            )
            crawl_params['includePaths'] = include_patterns
        
        # Ğ—Ğ°Ğ¿ÑƒÑĞº crawl
        crawl_result = self.firecrawl.crawl_url(**crawl_params)
        
        # ĞĞ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ
        print(f"ğŸ•·ï¸ Crawling in progress...")
        crawl_id = crawl_result.get('id')
        
        final_data = await self._poll_crawl_status(crawl_id)
        
        print(f"âœ… Crawled {len(final_data)} pages")
        
        return final_data
    
    def _select_start_urls(self, url_patterns: Dict) -> List[str]:
        """Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑÑ‚Ğ°Ñ€Ñ‚Ğ¾Ğ²Ñ‹Ñ… URL Ğ´Ğ»Ñ crawl"""
        
        start_urls = []
        
        # ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 1: Homepage
        if 'homepage' in url_patterns and url_patterns['homepage']:
            start_urls.append(url_patterns['homepage'][0])
        
        # ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 2: Listing pages
        if 'care_home_listing' in url_patterns:
            start_urls.extend(url_patterns['care_home_listing'][:2])
        
        # ĞŸÑ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ 3: Regional pages (Ñ‚Ğ¾Ğ¿Ğ¾Ğ²Ñ‹Ğµ)
        if 'regional_page' in url_patterns:
            start_urls.extend(url_patterns['regional_page'][:3])
        
        return start_urls
    
    def _generate_adaptive_prompt(self, discovery_result: Dict) -> str:
        """Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ³Ğ¾ prompt Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ° ÑĞ°Ğ¹Ñ‚Ğ°"""
        
        # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ prompt
        prompt_parts = [
            "Extract content about care homes and elderly care facilities."
        ]
        
        # ĞĞ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ²
        patterns = discovery_result.get('url_patterns', {})
        
        if 'care_home_detail' in patterns:
            prompt_parts.append(
                "Focus on individual care home pages with detailed information: "
                "name, address, phone, services, facilities, staff, photos."
            )
        
        if 'care_home_listing' in patterns:
            prompt_parts.append(
                "Include directory pages listing multiple care homes."
            )
        
        if 'regional_page' in patterns:
            prompt_parts.append(
                "Include regional pages showing care homes by location."
            )
        
        # Ğ˜ÑĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ
        prompt_parts.append(
            "\nExclude: blog posts, news articles, job listings, "
            "legal pages, cookie policies."
        )
        
        return " ".join(prompt_parts)
    
    def _templates_to_regex(self, templates: Dict) -> List[str]:
        """ĞŸÑ€ĞµĞ¾Ğ±Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ URL templates Ğ² regex Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹"""
        
        regex_patterns = []
        
        for template_name, template_data in templates.items():
            template = template_data['template']
            
            # /care-homes/{slug}/ -> ^/care-homes/[a-z0-9\-]+/$
            regex = template.replace('{slug}', '[a-z0-9\\-]+')
            regex = regex.replace('{id}', '\\d+')
            regex = '^' + regex + '$'
            
            regex_patterns.append(regex)
        
        return regex_patterns
    
    async def _poll_crawl_status(
        self,
        crawl_id: str,
        max_wait: int = 600
    ) -> List[Dict]:
        """Polling ÑÑ‚Ğ°Ñ‚ÑƒÑĞ° crawl Ğ´Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ"""
        
        import asyncio
        
        elapsed = 0
        while elapsed < max_wait:
            await asyncio.sleep(10)
            elapsed += 10
            
            status = self.firecrawl.check_crawl_status(crawl_id)
            
            current_status = status.get('status')
            completed = status.get('completed', 0)
            total = status.get('total', 0)
            
            print(f"   Status: {current_status} - {completed}/{total}")
            
            if current_status == 'completed':
                return status.get('data', [])
            elif current_status == 'failed':
                raise Exception(f"Crawl failed: {status.get('error')}")
        
        raise Exception(f"Crawl timeout after {max_wait}s")
```

### 5.2 Rate Limiting & Retry Logic

```python
class RateLimitedCrawler:
    """Crawler Ñ Ğ¸Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼ rate limiting"""
    
    def __init__(self, firecrawl_api_key: str, base_delay: float = 2.0):
        self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)
        self.base_delay = base_delay
        self.delay_multiplier = 1.0
        self.last_request_time = None
    
    async def adaptive_delay(self):
        """ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²"""
        
        if self.last_request_time:
            elapsed = time.time() - self.last_request_time
            required_delay = self.base_delay * self.delay_multiplier
            
            if elapsed < required_delay:
                wait_time = required_delay - elapsed
                await asyncio.sleep(wait_time)
        
        self.last_request_time = time.time()
    
    async def scrape_with_retry(
        self,
        url: str,
        max_retries: int = 3
    ) -> Optional[Dict]:
        """Scrape Ñ exponential backoff"""
        
        for attempt in range(max_retries):
            await self.adaptive_delay()
            
            try:
                result = self.firecrawl.scrape_url(url, params={
                    "formats": ["markdown", "html"],
                    "onlyMainContent": True
                })
                
                # Ğ£ÑĞ¿ĞµÑ… - ÑĞ½Ğ¸Ğ¶Ğ°ĞµĞ¼ delay multiplier
                self.delay_multiplier = max(1.0, self.delay_multiplier * 0.9)
                
                return result
                
            except Exception as e:
                error_msg = str(e).lower()
                
                if 'rate limit' in error_msg or '429' in error_msg:
                    # Rate limit hit - ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºÑƒ
                    self.delay_multiplier *= 2.0
                    backoff = self.base_delay * (2 ** attempt) * self.delay_multiplier
                    
                    print(f"âš ï¸ Rate limit hit. Backoff: {backoff:.1f}s")
                    await asyncio.sleep(backoff)
                    
                elif 'timeout' in error_msg:
                    print(f"âš ï¸ Timeout on {url}. Retry {attempt+1}/{max_retries}")
                    await asyncio.sleep(5)
                    
                else:
                    print(f"âŒ Error: {e}")
                    if attempt == max_retries - 1:
                        return None
        
        return None
```

---

<a name="Ñ„Ğ°Ğ·Ğ°-3"></a>
## 6. Ğ¤Ğ°Ğ·Ğ° 3: AI-Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ (Intelligent Extraction)

### 6.1 Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ°

```python
from pydantic import BaseModel, Field
from typing import List, Optional, Any

class UniversalCareHomeSchema(BaseModel):
    """Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ…ĞµĞ¼Ğ° Ğ´Ğ»Ñ Ğ›Ğ®Ğ‘ĞĞ“Ğ ÑĞ°Ğ¹Ñ‚Ğ° Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…"""
    
    # TIER 1: ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ²ÑĞµĞ³Ğ´Ğ°)
    name: str = Field(
        description="Official name of the care home facility"
    )
    phone: Optional[str] = Field(
        None,
        description="Primary contact phone number"
    )
    address: Optional[str] = Field(
        None,
        description="Full address including street, city, postcode"
    )
    
    # TIER 2: Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ)
    website_url: Optional[str] = Field(
        None,
        description="URL of the care home's detail page"
    )
    email: Optional[str] = Field(
        None,
        description="Contact email address"
    )
    care_types: List[str] = Field(
        default_factory=list,
        description="Types of care provided: Residential, Nursing, Dementia, Respite, etc."
    )
    
    # TIER 3: Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ)
    description: Optional[str] = Field(
        None,
        description="General description of the care home"
    )
    facilities: List[str] = Field(
        default_factory=list,
        description="Available facilities and amenities"
    )
    staff_info: Optional[str] = Field(
        None,
        description="Information about staff qualifications"
    )
    room_types: List[str] = Field(
        default_factory=list,
        description="Types of rooms available"
    )
    capacity: Optional[int] = Field(
        None,
        description="Total bed capacity"
    )
    
    # TIER 4: ĞĞ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    pricing: Optional[str] = Field(
        None,
        description="Pricing information if available"
    )
    cqc_rating: Optional[str] = Field(
        None,
        description="CQC rating if mentioned"
    )
    awards: List[str] = Field(
        default_factory=list,
        description="Awards and accreditations"
    )
    photos: List[str] = Field(
        default_factory=list,
        description="URLs of photos"
    )
    testimonials: List[Dict[str, str]] = Field(
        default_factory=list,
        description="Customer testimonials"
    )
    
    # ĞœĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
    source_url: Optional[str] = None
    extraction_date: Optional[str] = None
    extraction_confidence: Optional[float] = Field(
        None,
        ge=0.0,
        le=1.0,
        description="Confidence score of extraction (0.0-1.0)"
    )
```

### 6.2 Semantic Entity Extractor Ñ Claude

```python
class SemanticEntityExtractor:
    """AI-Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ¸Ğ· Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ HTML/Markdown"""
    
    def __init__(self, anthropic_api_key: str):
        self.client = Anthropic(api_key=anthropic_api_key)
    
    async def extract_care_home_data(
        self,
        content: str,
        source_url: str,
        content_type: str = "markdown"
    ) -> Dict[str, Any]:
        """
        Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ¾Ğ¼Ğµ Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…
        """
        
        # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ´Ğ¾ 100k Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ² (~400k ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)
        max_chars = 400000
        if len(content) > max_chars:
            content = content[:max_chars]
        
        prompt = f"""You are extracting structured data about a care home from webpage content.

Source URL: {source_url}
Content format: {content_type}

Content:
{content}

Extract the following information and return as JSON:

{{
  "name": "Official care home name",
  "phone": "Contact phone number",
  "address": "Full address with postcode",
  "email": "Contact email",
  "website_url": "URL to care home page",
  "care_types": ["List of care types: Residential, Nursing, Dementia, etc."],
  "description": "Brief description of the care home",
  "facilities": ["List of facilities and amenities"],
  "staff_info": "Information about staff qualifications",
  "room_types": ["Types of rooms"],
  "capacity": "Number of beds if mentioned",
  "pricing": "Pricing information if available",
  "cqc_rating": "CQC rating if mentioned",
  "awards": ["Awards and accreditations"],
  "photos": ["URLs of photos found in content"],
  "testimonials": [
    {{"quote": "...", "author": "..."}}
  ]
}}

Important rules:
1. Extract ONLY information explicitly present in the content
2. Use null for missing fields, empty arrays [] for missing lists
3. For address, extract the complete UK address with postcode
4. For phone, extract UK format (prefer landline over mobile)
5. Return ONLY valid JSON, no preamble or explanation
6. If multiple care homes are on the page, extract the primary/featured one

Return JSON:"""

        try:
            response = self.client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=4000,
                temperature=0.0,  # Ğ”ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
            
            # Parse JSON response
            json_text = response.content[0].text
            # Remove markdown code blocks if present
            json_text = re.sub(r'```json\s*|\s*```', '', json_text).strip()
            
            extracted_data = json.loads(json_text)
            
            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            extracted_data['source_url'] = source_url
            extracted_data['extraction_date'] = datetime.now().isoformat()
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ confidence score
            confidence = self._calculate_confidence(extracted_data)
            extracted_data['extraction_confidence'] = confidence
            
            return extracted_data
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON parse error: {e}")
            print(f"Response: {json_text}")
            return None
        except Exception as e:
            print(f"âŒ Extraction error: {e}")
            return None
    
    def _calculate_confidence(self, data: Dict) -> float:
        """Ğ Ğ°ÑÑ‡ĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        
        # ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¾Ğ»Ñ
        critical_fields = ['name', 'phone', 'address']
        critical_score = sum([
            1.0 for field in critical_fields 
            if data.get(field) and data[field]
        ]) / len(critical_fields)
        
        # Ğ’Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ
        important_fields = ['email', 'care_types', 'description']
        important_score = sum([
            1.0 for field in important_fields 
            if data.get(field) and (
                data[field] if isinstance(data[field], str) 
                else len(data[field]) > 0
            )
        ]) / len(important_fields)
        
        # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ
        optional_fields = ['facilities', 'staff_info', 'room_types', 'capacity']
        optional_score = sum([
            1.0 for field in optional_fields 
            if data.get(field) and (
                data[field] if not isinstance(data[field], list)
                else len(data[field]) > 0
            )
        ]) / len(optional_fields)
        
        # Weighted average
        confidence = (
            critical_score * 0.6 +
            important_score * 0.3 +
            optional_score * 0.1
        )
        
        return round(confidence, 2)
    
    async def extract_batch(
        self,
        pages: List[Dict],
        max_concurrent: int = 5
    ) -> List[Dict]:
        """ĞŸĞ°ĞºĞµÑ‚Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ¸ĞµĞ¼ ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸"""
        
        import asyncio
        from asyncio import Semaphore
        
        semaphore = Semaphore(max_concurrent)
        
        async def extract_one(page):
            async with semaphore:
                return await self.extract_care_home_data(
                    content=page.get('markdown', page.get('html', '')),
                    source_url=page.get('url', ''),
                    content_type='markdown' if 'markdown' in page else 'html'
                )
        
        tasks = [extract_one(page) for page in pages]
        results = await asyncio.gather(*tasks)
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ None
        return [r for r in results if r is not None]
```

### 6.3 Fallback: Regex-based Entity Extraction

```python
class RegexEntityExtractor:
    """Fallback Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· regex (ĞºĞ¾Ğ³Ğ´Ğ° AI Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½)"""
    
    @staticmethod
    def extract_phone_numbers(text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ UK Ñ‚ĞµĞ»ĞµÑ„Ğ¾Ğ½Ğ¾Ğ²"""
        
        patterns = [
            # Landline: 01234 567890
            r'\b0\d{4}\s?\d{6}\b',
            # Landline: 020 1234 5678
            r'\b0\d{2,3}\s?\d{4}\s?\d{4}\b',
            # Mobile: 07123 456789
            r'\b07\d{3}\s?\d{6}\b',
            # With +44: +44 20 1234 5678
            r'\+44\s?\d{2,3}\s?\d{4}\s?\d{4}',
            # With brackets: (01234) 567890
            r'\(0\d{2,4}\)\s?\d{6,7}'
        ]
        
        phones = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            phones.extend(matches)
        
        # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
        cleaned = []
        for phone in phones:
            # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹ Ğ¸ ÑĞºĞ¾Ğ±ĞºĞ¸
            clean = re.sub(r'[\s\(\)]', '', phone)
            if clean not in cleaned:
                cleaned.append(clean)
        
        return cleaned
    
    @staticmethod
    def extract_email(text: str) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ email"""
        
        pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        matches = re.findall(pattern, text)
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ñ… false positives
        blacklist = ['example.com', 'domain.com', 'email.com']
        valid = [m for m in matches if not any(b in m for b in blacklist)]
        
        return valid[0] if valid else None
    
    @staticmethod
    def extract_uk_postcode(text: str) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ UK postcode"""
        
        # UK postcode pattern
        pattern = r'\b[A-Z]{1,2}\d{1,2}\s?\d[A-Z]{2}\b'
        matches = re.findall(pattern, text)
        
        return matches[0] if matches else None
    
    @staticmethod
    def extract_address(text: str, postcode: str) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ñ€ĞµÑĞ° Ğ¿Ğ¾ postcode"""
        
        if not postcode:
            return None
        
        # Ğ˜Ñ‰ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¿ĞµÑ€ĞµĞ´ postcode (Ğ´Ğ¾ 200 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)
        pattern = rf'(.{{0,200}}{re.escape(postcode)})'
        match = re.search(pattern, text, re.IGNORECASE)
        
        if match:
            address_text = match.group(1)
            # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ°
            address_text = re.sub(r'\s+', ' ', address_text).strip()
            return address_text
        
        return None
    
    @staticmethod
    def extract_care_types(text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑƒÑ…Ğ¾Ğ´Ğ°"""
        
        care_keywords = {
            "Residential Care": ["residential care", "residential home"],
            "Nursing Care": ["nursing care", "nursing home"],
            "Dementia Care": ["dementia care", "dementia specialist", "alzheimer"],
            "Respite Care": ["respite care", "short stay", "temporary care"],
            "Palliative Care": ["palliative care", "end of life", "hospice"],
            "Independent Living": ["independent living", "retirement living"],
            "Nursing Dementia": ["nursing dementia", "advanced dementia care"]
        }
        
        found_types = []
        text_lower = text.lower()
        
        for care_type, keywords in care_keywords.items():
            if any(kw in text_lower for kw in keywords):
                found_types.append(care_type)
        
        return found_types
    
    def extract_all(self, text: str) -> Dict:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… ÑÑƒÑ‰Ğ½Ğ¾ÑÑ‚ĞµĞ¹"""
        
        phones = self.extract_phone_numbers(text)
        email = self.extract_email(text)
        postcode = self.extract_uk_postcode(text)
        address = self.extract_address(text, postcode) if postcode else None
        care_types = self.extract_care_types(text)
        
        return {
            "phone": phones[0] if phones else None,
            "email": email,
            "address": address,
            "care_types": care_types,
            "extraction_method": "regex"
        }
```

---

<a name="Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹"></a>
## 7. ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ

### 7.1 Smart Name Extraction

```python
class NameExtractor:
    """Ğ˜Ğ½Ñ‚ĞµĞ»Ğ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾Ğ¼Ğ° Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…"""
    
    @staticmethod
    def extract_from_html(soup: BeautifulSoup, url: str) -> Optional[str]:
        """ĞœĞ½Ğ¾Ğ³Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ¾Ğµ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ"""
        
        # Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 1: H1 tag (Ğ½Ğ°Ğ¸Ğ²Ñ‹ÑÑˆĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚)
        h1 = soup.find('h1')
        if h1:
            name = h1.get_text(strip=True)
            # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ°: "Welcome to Avery Park Care Home" -> "Avery Park"
            name = re.sub(r'^(Welcome to|About)\s+', '', name, flags=re.I)
            name = re.sub(r'\s+(Care Home|Nursing Home|Residential Home).*$', '', name, flags=re.I)
            if len(name) > 3:
                return name
        
        # Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 2: Title tag
        title = soup.find('title')
        if title:
            name = title.get_text(strip=True)
            # "Avery Park | Care Homes UK" -> "Avery Park"
            name = name.split('|')[0].split('-')[0].strip()
            name = re.sub(r'\s+(Care Home|Nursing Home).*$', '', name, flags=re.I)
            if len(name) > 3:
                return name
        
        # Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 3: Open Graph meta
        og_title = soup.find('meta', property='og:title')
        if og_title and og_title.get('content'):
            return og_title['content']
        
        # Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ 4: URL slug
        # /care-homes/avery-park/ -> "Avery Park"
        slug = url.rstrip('/').split('/')[-1]
        if slug and slug not in ['care-homes', 'homes', 'directory']:
            # Convert kebab-case to Title Case
            name = slug.replace('-', ' ').title()
            return name
        
        return None
```

### 7.2 Smart Address Parser

```python
class AddressParser:
    """ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ UK Ğ°Ğ´Ñ€ĞµÑĞ¾Ğ² Ğ»ÑĞ±Ğ¾Ğ³Ğ¾ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°"""
    
    @staticmethod
    def parse_uk_address(text: str) -> Dict[str, Optional[str]]:
        """
        Ğ Ğ°Ğ·Ğ±Ğ¾Ñ€ UK Ğ°Ğ´Ñ€ĞµÑĞ° Ğ½Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹
        
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚:
        {
            "street": "123 Main Street",
            "city": "Birmingham",
            "county": "West Midlands",
            "postcode": "B18 4BJ",
            "full_address": "..."
        }
        """
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ postcode
        postcode_match = re.search(
            r'\b([A-Z]{1,2}\d{1,2}\s?\d[A-Z]{2})\b',
            text
        )
        postcode = postcode_match.group(1) if postcode_match else None
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
        components = {
            "street": None,
            "city": None,
            "county": None,
            "postcode": postcode,
            "full_address": text.strip()
        }
        
        if not postcode:
            return components
        
        # Ğ Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ğµ Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸ Ñ‡ĞµÑ€ĞµĞ· Ğ·Ğ°Ğ¿ÑÑ‚ÑƒÑ
        parts = [p.strip() for p in text.split(',')]
        
        if len(parts) >= 3:
            # ĞŸÑ€ĞµĞ´Ğ¿Ğ¾Ğ»Ğ°Ğ³Ğ°ĞµĞ¼: Street, City, County, Postcode
            components["street"] = parts[0]
            components["city"] = parts[1]
            
            # County Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ² Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ postcode
            last_part = parts[-1]
            if postcode in last_part:
                county_part = last_part.replace(postcode, '').strip()
                if county_part:
                    components["county"] = county_part
            elif len(parts) > 3:
                components["county"] = parts[2]
        
        elif len(parts) == 2:
            # Street, City Postcode
            components["street"] = parts[0]
            city_postcode = parts[1]
            components["city"] = city_postcode.replace(postcode, '').strip()
        
        else:
            # ĞĞ´Ğ¸Ğ½ Ğ±Ğ»Ğ¾Ğº - Ğ¿Ñ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ñ€Ğ°Ğ·Ğ±Ğ¸Ñ‚ÑŒ ÑƒĞ¼Ğ½ĞµĞµ
            # "123 Main St, Birmingham B18 4BJ"
            text_without_postcode = text.replace(postcode, '').strip(' ,')
            parts = [p.strip() for p in text_without_postcode.split(',')]
            
            if parts:
                components["street"] = parts[0]
                if len(parts) > 1:
                    components["city"] = parts[-1]
        
        return components
```

### 7.3 Smart Facility Detector

```python
class FacilityDetector:
    """ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ² Ğ¸ ÑƒÑĞ»ÑƒĞ³ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
    
    FACILITY_PATTERNS = {
        "Garden": ["garden", "outdoor space", "landscaped grounds", "patio", "terrace"],
        "Cinema Room": ["cinema", "movie room", "film screening", "theatre room"],
        "Hair Salon": ["hair salon", "hairdresser", "barber", "hair care"],
        "Library": ["library", "reading room", "book collection"],
        "CafÃ©": ["cafÃ©", "coffee shop", "bistro", "refreshment"],
        "Gym": ["gym", "fitness", "exercise room", "workout"],
        "Swimming Pool": ["pool", "swimming", "hydrotherapy pool"],
        "Activities Room": ["activities room", "recreation", "hobby room"],
        "WiFi": ["wi-fi", "wifi", "internet access", "wireless"],
        "Parking": ["parking", "car park", "visitor parking"],
        "Lift": ["lift", "elevator", "accessible"],
        "Minibus": ["minibus", "transport", "trips", "outings"],
        "Chapel": ["chapel", "prayer room", "spiritual"],
        "Restaurant": ["restaurant", "dining room", "meal service"],
        "24-hour Care": ["24 hour", "24/7", "round the clock", "24-hour nursing"]
    }
    
    @classmethod
    def detect_facilities(cls, text: str) -> List[str]:
        """ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°"""
        
        text_lower = text.lower()
        detected = []
        
        for facility, keywords in cls.FACILITY_PATTERNS.items():
            if any(kw in text_lower for kw in keywords):
                detected.append(facility)
        
        return detected
    
    @classmethod
    def detect_from_images(cls, soup: BeautifulSoup) -> List[str]:
        """ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ ÑƒĞ´Ğ¾Ğ±ÑÑ‚Ğ² Ğ¿Ğ¾ alt Ñ‚ĞµĞºÑÑ‚Ğ°Ğ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹"""
        
        detected = []
        images = soup.find_all('img', alt=True)
        
        for img in images:
            alt = img['alt'].lower()
            
            for facility, keywords in cls.FACILITY_PATTERNS.items():
                if any(kw in alt for kw in keywords):
                    if facility not in detected:
                        detected.append(facility)
        
        return detected
```

---

<a name="Ñ‚Ğ¸Ğ¿Ñ‹-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²"></a>
## 8. ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ² ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²

### 8.1 WordPress ÑĞ°Ğ¹Ñ‚Ñ‹ (70% Ñ€Ñ‹Ğ½ĞºĞ°)

**Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:**
- URL ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°: `/care-homes/{slug}/`
- CMS ÑĞ¸Ğ³Ğ½Ğ°Ñ‚ÑƒÑ€Ñ‹: `wp-content`, `wp-includes`
- ĞŸĞ»Ğ°Ğ³Ğ¸Ğ½Ñ‹: Contact Form 7, Yoast SEO
- Ğ¡Ñ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ Ñ‚ĞµĞ¼Ñ‹

**Ğ¡Ğ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹:**

```python
class WordPressExtractor:
    """Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ WordPress"""
    
    @staticmethod
    def detect_wordpress(html: str) -> bool:
        indicators = ['wp-content', 'wp-includes', 'wordpress']
        return any(ind in html.lower() for ind in indicators)
    
    @staticmethod
    def extract_featured_image(soup: BeautifulSoup) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ featured image WordPress"""
        
        # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 1: wp-post-image class
        img = soup.find('img', class_=re.compile(r'wp-post-image'))
        if img and img.get('src'):
            return img['src']
        
        # ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½ 2: wp-content uploads
        img = soup.find('img', src=re.compile(r'/wp-content/uploads/'))
        if img:
            return img['src']
        
        return None
    
    @staticmethod
    def extract_custom_fields(soup: BeautifulSoup) -> Dict:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ WordPress custom fields"""
        
        # Advanced Custom Fields (ACF) Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ data-attributes
        custom_data = {}
        
        elements_with_data = soup.find_all(attrs={"data-acf": True})
        for el in elements_with_data:
            for attr, value in el.attrs.items():
                if attr.startswith('data-'):
                    field_name = attr.replace('data-', '')
                    custom_data[field_name] = value
        
        return custom_data
```

### 8.2 Wix ÑĞ°Ğ¹Ñ‚Ñ‹ (10% Ñ€Ñ‹Ğ½ĞºĞ°)

**Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:**
- Heavy JavaScript
- Dynamic loading
- Nested iframes
- API endpoints

**Ğ¡Ğ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹:**

```python
class WixExtractor:
    """Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Wix"""
    
    @staticmethod
    def detect_wix(html: str) -> bool:
        return 'wix.com' in html.lower() or '_wix' in html.lower()
    
    @staticmethod
    async def scrape_wix_site(url: str, firecrawl: FirecrawlApp) -> Dict:
        """Scraping Wix Ñ JavaScript rendering"""
        
        # Wix Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ waitFor Ğ´Ğ»Ñ JavaScript
        result = firecrawl.scrape_url(url, params={
            "formats": ["markdown"],
            "waitFor": 3000,  # Ğ–Ğ´ĞµĞ¼ 3 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ´Ğ»Ñ JS
            "onlyMainContent": True
        })
        
        return result
```

### 8.3 Custom-built ÑĞ°Ğ¹Ñ‚Ñ‹ (15% Ñ€Ñ‹Ğ½ĞºĞ°)

**Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:**
- Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°
- ĞĞµÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğµ URL
- Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ CMS

**Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´:**

```python
class CustomSiteExtractor:
    """Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ Ğ»ÑĞ±Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²"""
    
    def __init__(self, anthropic_client):
        self.claude = anthropic_client
    
    async def extract_with_ai(self, html: str, url: str) -> Dict:
        """ĞŸĞ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ AI-driven Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ"""
        
        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Claude Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
        prompt = f"""Analyze this webpage and extract care home information.

URL: {url}

HTML content (truncated):
{html[:50000]}

Extract:
1. Care home name
2. Contact information (phone, email, address)
3. Services provided
4. Facilities available
5. Any other relevant information

Return as JSON."""

        response = self.claude.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=4000,
            messages=[{"role": "user", "content": prompt}]
        )
        
        json_text = response.content[0].text
        # Parse JSON
        data = json.loads(re.sub(r'```json\s*|\s*```', '', json_text))
        
        return data
```

### 8.4 Static HTML ÑĞ°Ğ¹Ñ‚Ñ‹ (5% Ñ€Ñ‹Ğ½ĞºĞ°)

**Ğ¥Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸:**
- ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ HTML Ğ±ĞµĞ· CMS
- ĞœĞ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ JavaScript
- Ğ¡Ñ‚Ğ°Ñ€Ñ‹Ğµ ÑĞ°Ğ¹Ñ‚Ñ‹

**ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ñ‹:**

```python
class StaticHTMLExtractor:
    """Ğ­ĞºÑÑ‚Ñ€Ğ°ĞºÑ‚Ğ¾Ñ€ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ñ‹Ñ… HTML ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²"""
    
    @staticmethod
    def extract_with_beautifulsoup(html: str) -> Dict:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· BeautifulSoup"""
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ¸
        data = {}
        
        # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ - Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ H1
        h1 = soup.find('h1')
        if h1:
            data['name'] = h1.get_text(strip=True)
        
        # Ğ¢ĞµĞ»ĞµÑ„Ğ¾Ğ½ - ÑÑÑ‹Ğ»ĞºĞ° tel:
        phone_link = soup.find('a', href=re.compile(r'^tel:'))
        if phone_link:
            data['phone'] = phone_link['href'].replace('tel:', '')
        
        # Email - ÑÑÑ‹Ğ»ĞºĞ° mailto:
        email_link = soup.find('a', href=re.compile(r'^mailto:'))
        if email_link:
            data['email'] = email_link['href'].replace('mailto:', '')
        
        # ĞĞ´Ñ€ĞµÑ - Ğ¸Ñ‰ĞµĞ¼ postcode
        text = soup.get_text()
        postcode = RegexEntityExtractor.extract_uk_postcode(text)
        if postcode:
            data['postcode'] = postcode
            data['address'] = RegexEntityExtractor.extract_address(text, postcode)
        
        return data
```

---

<a name="Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"></a>
## 9. ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹

### 9.1 Main Orchestrator Class

```python
class UniversalCareHomeScraper:
    """
    Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºÑ€Ğ°Ğ¿ĞµÑ€ Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…
    Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ›Ğ®Ğ‘Ğ«ĞœĞ˜ ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼Ğ¸
    """
    
    def __init__(
        self,
        firecrawl_api_key: str,
        anthropic_api_key: str,
        rate_limit_delay: float = 2.0
    ):
        self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)
        self.anthropic = Anthropic(api_key=anthropic_api_key)
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
        self.site_analyzer = SiteAnalyzer("")
        self.classifier = ContentTypeClassifier(anthropic_api_key)
        self.discovery = SmartDiscovery(firecrawl_api_key, anthropic_api_key)
        self.semantic_crawler = SemanticCrawler(firecrawl_api_key)
        self.entity_extractor = SemanticEntityExtractor(anthropic_api_key)
        self.rate_limiter = RateLimitedCrawler(firecrawl_api_key, rate_limit_delay)
        
        # Fallback extractors
        self.regex_extractor = RegexEntityExtractor()
        self.name_extractor = NameExtractor()
        self.facility_detector = FacilityDetector()
    
    async def scrape_site(
        self,
        base_url: str,
        max_pages: int = 500
    ) -> Dict[str, Any]:
        """
        ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ» Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾ Ğ´Ğ¾Ğ¼Ğ°Ñ… Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…
        
        Returns:
            {
                "site_info": {...},
                "care_homes": [ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ¾Ğ¼Ğ¾Ğ² Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ…],
                "statistics": {...}
            }
        """
        
        print(f"\n{'='*70}")
        print(f"ğŸ¥ UNIVERSAL CARE HOME SCRAPER")
        print(f"ğŸŒ Target: {base_url}")
        print(f"{'='*70}\n")
        
        start_time = time.time()
        
        # ==================== Ğ¤ĞĞ—Ğ 0: ĞĞĞĞ›Ğ˜Ğ— ====================
        print(f"ğŸ“Š PHASE 0: SITE ANALYSIS")
        print(f"{'â”€'*70}")
        
        # ĞŸĞµÑ€Ğ²Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ scrape homepage
        homepage = await self.rate_limiter.scrape_with_retry(base_url)
        if not homepage:
            raise Exception("Failed to access homepage")
        
        # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ CMS
        cms = await self.site_analyzer.detect_cms(
            homepage['html'],
            homepage.get('headers', {})
        )
        print(f"   âœ“ CMS detected: {cms}")
        
        # ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
        self.site_analyzer.base_url = base_url
        structure = await self.site_analyzer.analyze_site_structure(
            homepage['html']
        )
        print(f"   âœ“ Found {structure['total_links']} internal links")
        
        # ==================== Ğ¤ĞĞ—Ğ 1: ĞĞ‘ĞĞĞ Ğ£Ğ–Ğ•ĞĞ˜Ğ• ====================
        print(f"\nğŸ“ PHASE 1: INTELLIGENT DISCOVERY")
        print(f"{'â”€'*70}")
        
        discovery_result = await self.discovery.discover_site(base_url)
        
        print(f"   âœ“ Discovered URL patterns:")
        for pattern_type, urls in discovery_result['url_patterns'].items():
            if urls:
                print(f"      - {pattern_type}: {len(urls)} URLs")
        
        # ==================== Ğ¤ĞĞ—Ğ 2: ĞšĞ ĞĞ£Ğ›Ğ˜ĞĞ“ ====================
        print(f"\nğŸ•·ï¸  PHASE 2: SEMANTIC CRAWLING")
        print(f"{'â”€'*70}")
        
        crawled_pages = await self.semantic_crawler.crawl_care_homes(
            discovery_result,
            max_pages=max_pages
        )
        
        print(f"   âœ“ Crawled {len(crawled_pages)} pages")
        
        # ==================== Ğ¤ĞĞ—Ğ 3: Ğ˜Ğ—Ğ’Ğ›Ğ•Ğ§Ğ•ĞĞ˜Ğ• ====================
        print(f"\nğŸ” PHASE 3: AI EXTRACTION")
        print(f"{'â”€'*70}")
        
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ detail pages
        detail_pages = self._filter_detail_pages(
            crawled_pages,
            discovery_result
        )
        
        print(f"   âœ“ Identified {len(detail_pages)} care home detail pages")
        
        # Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‡ĞµÑ€ĞµĞ· AI
        print(f"   â³ Extracting structured data...")
        
        extracted_homes = await self.entity_extractor.extract_batch(
            detail_pages,
            max_concurrent=5
        )
        
        # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°
        validated_homes = self._validate_and_clean(extracted_homes)
        
        print(f"   âœ“ Successfully extracted {len(validated_homes)} care homes")
        
        # ==================== Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ====================
        elapsed = time.time() - start_time
        
        result = {
            "site_info": {
                "base_url": base_url,
                "cms": cms,
                "total_links": structure['total_links'],
                "discovery": discovery_result
            },
            "care_homes": validated_homes,
            "statistics": {
                "pages_crawled": len(crawled_pages),
                "detail_pages_found": len(detail_pages),
                "homes_extracted": len(validated_homes),
                "success_rate": len(validated_homes) / len(detail_pages) if detail_pages else 0,
                "elapsed_time_seconds": round(elapsed, 2)
            }
        }
        
        print(f"\n{'='*70}")
        print(f"âœ… EXTRACTION COMPLETE")
        print(f"{'='*70}")
        print(f"   Pages crawled: {result['statistics']['pages_crawled']}")
        print(f"   Care homes found: {result['statistics']['homes_extracted']}")
        print(f"   Success rate: {result['statistics']['success_rate']*100:.1f}%")
        print(f"   Time elapsed: {result['statistics']['elapsed_time_seconds']}s")
        print(f"{'='*70}\n")
        
        return result
    
    def _filter_detail_pages(
        self,
        pages: List[Dict],
        discovery: Dict
    ) -> List[Dict]:
        """Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ detail pages"""
        
        detail_urls = discovery['url_patterns'].get('care_home_detail', [])
        url_templates = discovery.get('url_templates', {})
        
        filtered = []
        
        for page in pages:
            url = page.get('url', '')
            
            # ĞœĞµÑ‚Ğ¾Ğ´ 1: ĞŸÑ€ÑĞ¼Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ğ½Ñ‹Ğ¼Ğ¸ detail URLs
            if url in detail_urls:
                filtered.append(page)
                continue
            
            # ĞœĞµÑ‚Ğ¾Ğ´ 2: Ğ¡Ğ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ñ ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½Ğ¾Ğ¼
            template_match = URLPatternRecognizer.match_url_to_template(
                url,
                url_templates
            )
            if template_match:
                filtered.append(page)
                continue
            
            # ĞœĞµÑ‚Ğ¾Ğ´ 3: Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ° - Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ URL Ñ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ğ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ñ‹Ğ¼
            # Ğ•ÑĞ»Ğ¸ Ğ² content ĞµÑÑ‚ÑŒ phone number + address = Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ detail page
            content = page.get('markdown', '')
            has_phone = bool(self.regex_extractor.extract_phone_numbers(content))
            has_postcode = bool(self.regex_extractor.extract_uk_postcode(content))
            
            if has_phone and has_postcode:
                filtered.append(page)
        
        return filtered
    
    def _validate_and_clean(self, homes: List[Dict]) -> List[Dict]:
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""
        
        validated = []
        
        for home in homes:
            # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ
            if not home.get('name'):
                continue
            
            # Confidence threshold
            confidence = home.get('extraction_confidence', 0)
            if confidence < 0.3:  # Ğ¡Ğ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ½Ğ¸Ğ·ĞºĞ°Ñ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ
                continue
            
            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°
            # Ğ¢ĞµĞ»ĞµÑ„Ğ¾Ğ½ - Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
            if home.get('phone'):
                phone = re.sub(r'[\s\(\)\-]', '', home['phone'])
                home['phone'] = phone
            
            # Email - lowercase
            if home.get('email'):
                home['email'] = home['email'].lower()
            
            # URL - ensure full URL
            if home.get('website_url') and not home['website_url'].startswith('http'):
                home['website_url'] = f"https://{home['website_url']}"
            
            validated.append(home)
        
        return validated
    
    async def export_results(
        self,
        result: Dict,
        output_dir: str = "output"
    ):
        """Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ°Ñ…"""
        
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        base_name = result['site_info']['base_url'].replace('https://', '').replace('/', '_')
        
        # JSON
        json_path = f"{output_dir}/{base_name}_complete.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        print(f"âœ“ Saved JSON: {json_path}")
        
        # CSV (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹)
        import pandas as pd
        
        df = pd.DataFrame(result['care_homes'])
        csv_path = f"{output_dir}/{base_name}_care_homes.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"âœ“ Saved CSV: {csv_path}")
        
        # Excel Ñ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
        excel_path = f"{output_dir}/{base_name}_complete.xlsx"
        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
            # Care homes sheet
            df.to_excel(writer, sheet_name='Care Homes', index=False)
            
            # Statistics sheet
            stats_df = pd.DataFrame([result['statistics']])
            stats_df.to_excel(writer, sheet_name='Statistics', index=False)
        
        print(f"âœ“ Saved Excel: {excel_path}")
```

### 9.2 Usage Example

```python
async def main():
    """ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞºÑ€Ğ°Ğ¿ĞµÑ€Ğ°"""
    
    # API Keys
    FIRECRAWL_KEY = "fc-YOUR-KEY"
    ANTHROPIC_KEY = "sk-ant-YOUR-KEY"
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
    scraper = UniversalCareHomeScraper(
        firecrawl_api_key=FIRECRAWL_KEY,
        anthropic_api_key=ANTHROPIC_KEY,
        rate_limit_delay=2.0  # 2 ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
    )
    
    # Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸
    websites = [
        "https://www.averyhealthcare.co.uk/",
        "https://www.brighterkind.com/",
        "https://www.runwoodhomes.co.uk/",
        "https://www.cartercare.co.uk/",
        # ... Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ 15,000+ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² UK
    ]
    
    # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ°
    all_results = []
    
    for website in websites:
        try:
            print(f"\n{'#'*70}")
            print(f"Processing: {website}")
            print(f"{'#'*70}")
            
            result = await scraper.scrape_site(
                base_url=website,
                max_pages=200  # Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ½Ğ° ÑĞ°Ğ¹Ñ‚
            )
            
            # Ğ­ĞºÑĞ¿Ğ¾Ñ€Ñ‚
            await scraper.export_results(result)
            
            all_results.append(result)
            
            # ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ ÑĞ°Ğ¹Ñ‚Ğ°Ğ¼Ğ¸
            await asyncio.sleep(10)
            
        except Exception as e:
            print(f"âŒ Error processing {website}: {e}")
            continue
    
    # ĞĞ³Ñ€ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹
    total_homes = sum(len(r['care_homes']) for r in all_results)
    print(f"\n{'='*70}")
    print(f"ğŸ‰ SCRAPING COMPLETE")
    print(f"{'='*70}")
    print(f"   Sites processed: {len(all_results)}")
    print(f"   Total care homes: {total_homes}")
    print(f"{'='*70}\n")

# Ğ—Ğ°Ğ¿ÑƒÑĞº
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

---

## 10. ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ñ€ĞµĞ¸Ğ¼ÑƒÑ‰ĞµÑÑ‚Ğ²Ğ° ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ°

### âœ… ĞĞ´Ğ°Ğ¿Ñ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ
- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ°Ñ†Ğ¸Ñ Ğº Ğ»ÑĞ±Ğ¾Ğ¹ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğµ ÑĞ°Ğ¹Ñ‚Ğ°
- ĞĞµ Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²
- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ WordPress, Wix, Custom CMS

### âœ… Robustness
- Ğ£ÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ² Ğº Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ¸Ğ·Ğ°Ğ¹Ğ½Ğ°
- Multiple fallback strategies
- Graceful degradation (AI â†’ Regex â†’ Manual)

### âœ… ĞœĞ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€ÑƒĞµĞ¼Ğ¾ÑÑ‚ÑŒ
- ĞĞ´Ğ¸Ğ½ ĞºĞ¾Ğ´ Ğ´Ğ»Ñ 15,000+ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²
- ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°
- Ğ­Ñ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ API ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ğ²

### âœ… ĞšĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- AI Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
- Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ
- Confidence scoring Ğ´Ğ»Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸

### âœ… Maintainability
- ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ
- Ğ¡Ğ°Ğ¼Ğ¾Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰Ğ°ÑÑÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ°
- Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³

---

## 11. Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ²Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ñ

### Ğ­Ñ‚Ğ°Ğ¿ 1: Proof of Concept (1-2 Ğ½ĞµĞ´ĞµĞ»Ğ¸)
- Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° 10-20 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ°Ñ…
- Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ
- ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ prompts

### Ğ­Ñ‚Ğ°Ğ¿ 2: Pilot (2-4 Ğ½ĞµĞ´ĞµĞ»Ğ¸)
- ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° 100-500 ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²
- Ğ¡Ğ±Ğ¾Ñ€ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ÑÑ‚Ğ¸
- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° rate limiting

### Ğ­Ñ‚Ğ°Ğ¿ 3: Production (4-8 Ğ½ĞµĞ´ĞµĞ»ÑŒ)
- ĞŸĞ¾Ğ»Ğ½Ğ¾Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ²ĞµÑ€Ñ‚Ñ‹Ğ²Ğ°Ğ½Ğ¸Ğµ
- 15,000+ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² UK
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸ Ğ°Ğ»ĞµÑ€Ñ‚Ñ‹

### Ğ­Ñ‚Ğ°Ğ¿ 4: Maintenance (ongoing)
- Ğ•Ğ¶ĞµĞ½ĞµĞ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ re-crawls
- ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğ¹ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²

---

## 12. Ğ¡Ñ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¸ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ

### ĞÑ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚ (Ğ´Ğ»Ñ 15,000 ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²)

**Firecrawl API:**
- Map: 15,000 ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ã— 1 credit = 15,000 credits
- Crawl: ~200 ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†/ÑĞ°Ğ¹Ñ‚ Ã— 15,000 = 3,000,000 pages
- ĞŸĞ¾ $0.001/page = $3,000

**Anthropic Claude API:**
- Extraction: 50,000 detail pages Ã— $0.015/request = $750
- Classification: 150,000 pages Ã— $0.003/request = $450
- Total: ~$1,200

**Ğ˜Ğ¢ĞĞ“Ğ: ~$4,200 Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¹ Ğ±Ğ°Ğ·Ñ‹ UK**
**ĞĞ° Ğ¾Ğ´Ğ¸Ğ½ Ğ´Ğ¾Ğ¼: $0.28**

### Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ñ‹
- Discovery: 1 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ğ°/ÑĞ°Ğ¹Ñ‚
- Crawl: 5-10 Ğ¼Ğ¸Ğ½ÑƒÑ‚/ÑĞ°Ğ¹Ñ‚
- Extraction: 2 Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñ‹/ÑĞ°Ğ¹Ñ‚
- **Total: 15-20 Ğ¼Ğ¸Ğ½ÑƒÑ‚/ÑĞ°Ğ¹Ñ‚**

**Ğ”Ğ»Ñ 15,000 ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²: ~3,750 Ñ‡Ğ°ÑĞ¾Ğ² (156 Ğ´Ğ½ĞµĞ¹ Ğ¿Ñ€Ğ¸ 24/7)**

Ğ¡ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ (10 concurrent): **~15-20 Ğ´Ğ½ĞµĞ¹**

---

## Ğ—Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ

Ğ­Ñ‚Ğ° ÑƒĞ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ°Ñ‚ÑŒ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾ Ğ´Ğ¾Ğ¼Ğ°Ñ… Ğ¿Ñ€ĞµÑÑ‚Ğ°Ñ€ĞµĞ»Ñ‹Ñ… **Ñ Ğ»ÑĞ±Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²**, Ğ½ĞµĞ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ Ğ¾Ñ‚ Ğ¸Ñ… ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹, CMS Ğ¸Ğ»Ğ¸ Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¹. Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ° Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğº ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼Ñƒ ÑĞ°Ğ¹Ñ‚Ñƒ, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ AI-Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ñ Ğ¸ ÑĞ²Ñ€Ğ¸ÑÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ¾Ğ².

**Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾ Ğº production deployment Ğ´Ğ»Ñ 15,000+ UK care homes! ğŸš€**